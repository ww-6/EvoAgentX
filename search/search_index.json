{"config":{"lang":["en","zh"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"EvoAgentX","text":"<p> An automated framework for evaluating and evolving agentic workflows. </p> <p> </p>"},{"location":"#-introduction","title":"\ud83d\ude80 Introduction","text":"<p>EvoAgentX is an open-source framework designed to automate the generation, execution, evaluation and optimization of agentic workflows. By leveraging large language models (LLMs), EvoAgentX enables developers and researchers to prototype, test, and deploy multi-agent systems that grow in complexity and capability over time. </p>"},{"location":"#-key-features","title":"\u2728 Key Features","text":"<ul> <li>Easy Agent and Workflow Customization: Easily create customized agents and workflows using natural language prompts. EvoAgentX makes it easy to turn your high-level ideas to working systems. </li> <li>Automatic Workflow Generation &amp; Execution: Automatically generate and execute agentic workflows from simple goal descriptions, reducing manual workload in multi-agent system design. </li> <li>WorkFlow Optimization: Integrates existing workflow optimization techniques that iteratively refine workflows for improved performance. </li> <li>Benchmarking &amp; Evaluation: Includes built-in benchmarks and standardized evaluation metrics to measure workflow. effectiveness across different tasks and agent configurations </li> <li>Workflow Execution Toolkit: Offers a suite of tools essential for executing complex workflows, such as search components and the Model Context Protocol (MCP). </li> </ul>"},{"location":"#-how-it-works","title":"\ud83d\udd0d How It Works","text":"<p>EvoAgentX uses a modular architecture with the following core components:</p> <ol> <li>Workflow Generator: Creates agentic workflows based on your goals</li> <li>Agent Manager: Handles agent creation, configuration, and deployment</li> <li>Workflow Executor: Runs workflows efficiently with proper agent communication</li> <li>Evaluators: Provides performance metrics and improvement suggestions</li> <li>Optimizers: Evolves workflows to enhance performance over time</li> </ol>"},{"location":"#-community","title":"\ud83d\udc65 Community","text":"<ul> <li>Discord: Join our Discord Channel for discussions and support</li> <li>GitHub: Contribute to the project on GitHub</li> <li>Email: Contact us at evoagentx.ai@gmail.com</li> <li>WeChat: Connect with us on WeChat for updates and support.</li> </ul>"},{"location":"#-contributing","title":"\ud83e\udd1d Contributing","text":"<p>We welcome contributions from the community! Please refer to our Contributing Guidelines for more details.</p>"},{"location":"installation/","title":"Installation Guide for EvoAgentX","text":"<p>This guide will walk you through the process of installing EvoAgentX on your system, setting up the required dependencies, and configuring the framework for your projects.</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing EvoAgentX, make sure you have the following prerequisites:</p> <ul> <li>Python 3.10 or higher</li> <li>pip (Python package installer)</li> <li>Git (for cloning the repository)</li> <li>Conda (recommended for environment management, but optional)</li> </ul>"},{"location":"installation/#installation-methods","title":"Installation Methods","text":"<p>There are several ways to install EvoAgentX. Choose the method that best suits your needs.</p>"},{"location":"installation/#method-1-using-pip-recommended","title":"Method 1: Using pip (Recommended)","text":"<p>The simplest way to install EvoAgentX is using pip:</p> <pre><code>pip install git+https://github.com/EvoAgentX/EvoAgentX.git\n</code></pre>"},{"location":"installation/#method-2-from-source-for-development","title":"Method 2: From Source (For Development)","text":"<p>If you want to contribute to EvoAgentX or need the latest development version, you can install it directly from the source:</p> <pre><code># Clone the repository\ngit clone https://github.com/EvoAgentX/EvoAgentX/\n\n# Navigate to the project directory\ncd EvoAgentX\n\n# Install the package in development mode\npip install -e .\n</code></pre>"},{"location":"installation/#method-3-using-conda-environment-recommended-for-isolation","title":"Method 3: Using Conda Environment (Recommended for Isolation)","text":"<p>If you prefer to use Conda for managing your Python environments, follow these steps:</p> <pre><code># Create a new conda environment\nconda create -n evoagentx python=3.10\n\n# Activate the environment\nconda activate evoagentx\n\n# Install the package\npip install -r requirements.txt\n# OR install in development mode\npip install -e .\n</code></pre>"},{"location":"installation/#verifying-installation","title":"Verifying Installation","text":"<p>To verify that EvoAgentX has been installed correctly, run the following Python code:</p> <pre><code>import evoagentx\n\n# Print the version\nprint(evoagentx.__version__)\n</code></pre> <p>You should see the current version of EvoAgentX printed to the console.</p>"},{"location":"quickstart/","title":"EvoAgentX Quickstart Guide","text":"<p>This quickstart guide will walk you through the essential steps to set up and start using EvoAgentX. In this tutorial, you'll learn how to:</p> <ol> <li>Configure your API keys to access LLMs </li> <li>Automatically create and execute workflows </li> </ol>"},{"location":"quickstart/#installation","title":"Installation","text":"<p><pre><code>pip install git+https://github.com/EvoAgentX/EvoAgentX.git\n</code></pre> Please refere to Installation Guide for more details about the installation. </p>"},{"location":"quickstart/#api-key--llm-setup","title":"API Key &amp; LLM Setup","text":"<p>The first step to execute a workflow in EvoAgentX is configuring your API keys to access LLMs. There are two recommended methods to configure your API keys:</p>"},{"location":"quickstart/#method-1-set-environment-variables-in-the-terminal","title":"Method 1: Set Environment Variables in the Terminal","text":"<p>This method sets the API key directly in your system environment.</p> <p>For Linux/macOS:  <pre><code>export OPENAI_API_KEY=&lt;your-openai-api-key&gt;\n</code></pre></p> <p>For Windows Command Prompt:  <pre><code>set OPENAI_API_KEY=&lt;your-openai-api-key&gt;\n</code></pre></p> <p>For Windows PowerShell: <pre><code>$env:OPENAI_API_KEY=\"&lt;your-openai-api-key&gt;\" # \" is required \n</code></pre></p> <p>Once set, you can access the key in your Python code with: <pre><code>import os\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n</code></pre></p>"},{"location":"quickstart/#method-2-use-a-env-file","title":"Method 2: Use a <code>.env</code> File","text":"<p>You can also store your API key in a <code>.env</code> file inside the root folder of your project.</p> <p>Create a file named <code>.env</code> with the following content: <pre><code>OPENAI_API_KEY=&lt;your-openai-api-key&gt;\n</code></pre></p> <p>Then, in your Python code, you can load the environment settings using <code>python-dotenv</code>: <pre><code>from dotenv import load_dotenv \nimport os \n\nload_dotenv() # Loads environment variables from .env file\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n</code></pre> \ud83d\udd10 Tip: Never commit your <code>.env</code> file to public platform (e.g., GitHub). Add it to <code>.gitignore</code>.</p>"},{"location":"quickstart/#configure-and-use-the-llm-in-evoagentx","title":"Configure and Use the LLM in EvoAgentX","text":"<p>Once your API key is configured, you can initialize and use the LLM as follows:</p> <pre><code>from evoagentx.models import OpenAILLMConfig, OpenAILLM\n\n# Load the API key from environment\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\n# Define LLM configuration\nopenai_config = OpenAILLMConfig(\n    model=\"gpt-4o-mini\",       # Specify the model name\n    openai_key=OPENAI_API_KEY, # Pass the key directly\n    stream=True,               # Enable streaming response\n    output_response=True       # Print response to stdout\n)\n\n# Initialize the language model\nllm = OpenAILLM(config=openai_config)\n\n# Generate a response from the LLM\nresponse = llm.generate(prompt=\"What is Agentic Workflow?\")\n</code></pre> <p>You can find more details about supported LLM types and their parameters in the LLM module guide.</p>"},{"location":"quickstart/#automatic-workflow-generation-and-execution","title":"Automatic WorkFlow Generation and Execution","text":"<p>Once your API key and language model are configured, you can automatically generate and execute agentic workflows in EvoAgentX. This section walks you through the core steps: generating a workflow from a goal, instantiating agents, and running the workflow to get results.</p> <p>First, let's import the necessary modules:</p> <pre><code>from evoagentx.workflow import WorkFlowGenerator, WorkFlowGraph, WorkFlow\nfrom evoagentx.agents import AgentManager\n</code></pre>"},{"location":"quickstart/#step-1-generate-workflow-and-agents","title":"Step 1: Generate WorkFlow and Agents","text":"<p>Use the <code>WorkFlowGenerator</code> to automatically create a workflow based on a natural language goal: <pre><code>goal = \"Generate html code for the Tetris game that can be played in the browser.\"\nwf_generator = WorkFlowGenerator(llm=llm)\nworkflow_graph: WorkFlowGraph = wf_generator.generate_workflow(goal=goal)\n</code></pre> <code>WorkFlowGraph</code> is a data structure that stores the overall workflow plan \u2014 including task nodes and their relationships \u2014 but does not yet include executable agents.</p> <p>You can optionally visualize or save the generated workflow: <pre><code># Visualize the workflow structure (optional)\nworkflow_graph.display()\n\n# Save the workflow to a JSON file (optional)\nworkflow_graph.save_module(\"/path/to/save/workflow_demo.json\")\n</code></pre> We provide an example generated workflow here. You can reload the saved workflow: <pre><code>workflow_graph = WorkFlowGraph.from_file(\"/path/to/save/workflow_demo.json\")\n</code></pre></p>"},{"location":"quickstart/#step-2-create-and-manage-executable-agents","title":"Step 2: Create and Manage Executable Agents","text":"<p>Use <code>AgentManager</code> to instantiate and manage agents based on the workflow graph: <pre><code>agent_manager = AgentManager()\nagent_manager.add_agents_from_workflow(workflow_graph, llm_config=openai_config)\n</code></pre></p>"},{"location":"quickstart/#step-3-execute-the-workflow","title":"Step 3: Execute the Workflow","text":"<p>Once agents are ready, you can create a <code>WorkFlow</code> instance and run it: <pre><code>workflow = WorkFlow(graph=workflow_graph, agent_manager=agent_manager, llm=llm)\noutput = workflow.execute()\nprint(output)\n</code></pre></p> <p>For a complete working example, check out the full workflow demo.</p>"},{"location":"api/actions/","title":"\ud83c\udfaf Actions","text":""},{"location":"api/actions/#evoagentx.actions","title":"evoagentx.actions","text":""},{"location":"api/actions/#evoagentx.actions.Action","title":"Action","text":"<pre><code>Action(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModule</code></p> <p>Base class for all actions in the EvoAgentX framework.</p> <p>Actions represent discrete operations that can be performed by agents. They define inputs, outputs, and execution behavior, and can optionally use tools to accomplish their tasks.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Unique identifier for the action.</p> <code>description</code> <code>str</code> <p>Human-readable description of what the action does.</p> <code>prompt</code> <code>Optional[str]</code> <p>Optional prompt template for this action.</p> <code>tools</code> <code>Optional[List[Tool]]</code> <p>Optional list of tools that can be used by this action.</p> <code>inputs_format</code> <code>Optional[Type[ActionInput]]</code> <p>Optional class defining the expected input structure.</p> <code>outputs_format</code> <code>Optional[Type[Parser]]</code> <p>Optional class defining the expected output structure.</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/actions/#evoagentx.actions.Action.init_module","title":"init_module","text":"<pre><code>init_module()\n</code></pre> <p>Initialize the action module.</p> <p>This method is called after the action is instantiated. Subclasses can override this to perform custom initialization.</p> Source code in <code>evoagentx/actions/action.py</code> <pre><code>def init_module(self):\n    \"\"\"Initialize the action module.\n\n    This method is called after the action is instantiated.\n    Subclasses can override this to perform custom initialization.\n    \"\"\"\n    pass \n</code></pre>"},{"location":"api/actions/#evoagentx.actions.Action.to_dict","title":"to_dict","text":"<pre><code>to_dict(exclude_none: bool = True, ignore: List[str] = [], **kwargs) -&gt; dict\n</code></pre> <p>Convert the action to a dictionary for saving.</p> Source code in <code>evoagentx/actions/action.py</code> <pre><code>def to_dict(self, exclude_none: bool = True, ignore: List[str] = [], **kwargs) -&gt; dict:\n    \"\"\"\n    Convert the action to a dictionary for saving.  \n    \"\"\"\n    data = super().to_dict(exclude_none=exclude_none, ignore=ignore, **kwargs)\n    if self.inputs_format:\n        data[\"inputs_format\"] = self.inputs_format.__name__ \n    if self.outputs_format:\n        data[\"outputs_format\"] = self.outputs_format.__name__ \n    # TODO: customize serialization for the tools \n    return data \n</code></pre>"},{"location":"api/actions/#evoagentx.actions.Action.execute","title":"execute","text":"<pre><code>execute(llm: Optional[BaseLLM] = None, inputs: Optional[dict] = None, sys_msg: Optional[str] = None, return_prompt: bool = False, **kwargs) -&gt; Optional[Union[Parser, Tuple[Parser, str]]]\n</code></pre> <p>Execute the action to produce a result.</p> <p>This is the main entry point for executing an action. Subclasses must implement this method to define the action's behavior.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>Optional[BaseLLM]</code> <p>The LLM used to execute the action.</p> <code>None</code> <code>inputs</code> <code>Optional[dict]</code> <p>Input data for the action execution. The input data should be a dictionary that matches the input format of the provided prompt.  For example, if the prompt contains a variable <code>{input_var}</code>, the <code>inputs</code> dictionary should have a key <code>input_var</code>, otherwise the variable will be set to empty string. </p> <code>None</code> <code>sys_msg</code> <code>Optional[str]</code> <p>Optional system message for the LLM.</p> <code>None</code> <code>return_prompt</code> <code>bool</code> <p>Whether to return the complete prompt passed to the LLM.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for the execution.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[Union[Parser, Tuple[Parser, str]]]</code> <p>If <code>return_prompt</code> is False, the method returns a Parser object containing the structured result of the action.</p> <code>Optional[Union[Parser, Tuple[Parser, str]]]</code> <p>If <code>return_prompt</code> is True, the method returns a tuple containing the Parser object and the complete prompt passed to the LLM.</p> Source code in <code>evoagentx/actions/action.py</code> <pre><code>def execute(self, llm: Optional[BaseLLM] = None, inputs: Optional[dict] = None, sys_msg: Optional[str]=None, return_prompt: bool = False, **kwargs) -&gt; Optional[Union[Parser, Tuple[Parser, str]]]:\n    \"\"\"Execute the action to produce a result.\n\n    This is the main entry point for executing an action. Subclasses must\n    implement this method to define the action's behavior.\n\n    Args:\n        llm (Optional[BaseLLM]): The LLM used to execute the action.\n        inputs (Optional[dict]): Input data for the action execution. The input data should be a dictionary that matches the input format of the provided prompt. \n            For example, if the prompt contains a variable `{input_var}`, the `inputs` dictionary should have a key `input_var`, otherwise the variable will be set to empty string. \n        sys_msg (Optional[str]): Optional system message for the LLM.\n        return_prompt (bool): Whether to return the complete prompt passed to the LLM.\n        **kwargs (Any): Additional keyword arguments for the execution.\n\n    Returns:\n        If `return_prompt` is False, the method returns a Parser object containing the structured result of the action.\n        If `return_prompt` is True, the method returns a tuple containing the Parser object and the complete prompt passed to the LLM.\n    \"\"\"\n    raise NotImplementedError(f\"`execute` function of {type(self).__name__} is not implemented!\")\n</code></pre>"},{"location":"api/actions/#evoagentx.actions.Action.async_execute","title":"async_execute  <code>async</code>","text":"<pre><code>async_execute(llm: Optional[BaseLLM] = None, inputs: Optional[dict] = None, sys_msg: Optional[str] = None, return_prompt: bool = False, **kwargs) -&gt; Optional[Union[Parser, Tuple[Parser, str]]]\n</code></pre> <p>Asynchronous execution of the action.</p> <p>This method is the asynchronous counterpart of the <code>execute</code> method. It allows the action to be executed asynchronously using an LLM.</p> Source code in <code>evoagentx/actions/action.py</code> <pre><code>async def async_execute(self, llm: Optional[BaseLLM] = None, inputs: Optional[dict] = None, sys_msg: Optional[str]=None, return_prompt: bool = False, **kwargs) -&gt; Optional[Union[Parser, Tuple[Parser, str]]]:\n    \"\"\"\n    Asynchronous execution of the action.\n\n    This method is the asynchronous counterpart of the `execute` method.\n    It allows the action to be executed asynchronously using an LLM.\n    \"\"\"\n    raise NotImplementedError(f\"`async_execute` function of {type(self).__name__} is not implemented!\")\n</code></pre>"},{"location":"api/actions/#evoagentx.actions.ActionInput","title":"ActionInput","text":"<pre><code>ActionInput(**kwargs)\n</code></pre> <p>               Bases: <code>LLMOutputParser</code></p> <p>Input specification and parsing for actions.</p> <p>This class defines the input requirements for actions and provides methods to generate structured input specifications. It inherits from LLMOutputParser  to allow parsing of LLM outputs into structured inputs for actions.</p> Notes <p>Parameters in ActionInput should be defined in Pydantic Field format. For optional variables, use format:  var: Optional[int] = Field(default=None, description=\"xxx\") Remember to add <code>default=None</code> for optional parameters.</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/actions/#evoagentx.actions.ActionInput.get_input_specification","title":"get_input_specification  <code>classmethod</code>","text":"<pre><code>get_input_specification(ignore_fields: List[str] = []) -&gt; str\n</code></pre> <p>Generate a JSON specification of the input requirements.</p> <p>Examines the class fields and produces a structured specification of the input parameters, including their types, descriptions, and whether they are required.</p> <p>Parameters:</p> Name Type Description Default <code>ignore_fields</code> <code>List[str]</code> <p>List of field names to exclude from the specification.</p> <code>[]</code> <p>Returns:</p> Type Description <code>str</code> <p>A JSON string containing the input specification, or an empty string</p> <code>str</code> <p>if no fields are defined or all are ignored.</p> Source code in <code>evoagentx/actions/action.py</code> <pre><code>@classmethod\ndef get_input_specification(cls, ignore_fields: List[str] = []) -&gt; str:\n    \"\"\"Generate a JSON specification of the input requirements.\n\n    Examines the class fields and produces a structured specification of\n    the input parameters, including their types, descriptions, and whether\n    they are required.\n\n    Args:\n        ignore_fields (List[str]): List of field names to exclude from the specification.\n\n    Returns:\n        A JSON string containing the input specification, or an empty string\n        if no fields are defined or all are ignored.\n    \"\"\"\n    fields_info = {}\n    attrs = cls.get_attrs()\n    for field_name, field_info in cls.model_fields.items():\n        if field_name in ignore_fields:\n            continue\n        if field_name not in attrs:\n            continue\n        field_type = get_type_name(field_info.annotation)\n        field_desc = field_info.description if field_info.description is not None else None\n        # field_required = field_info.is_required()\n        field_default = str(field_info.default) if field_info.default is not PydanticUndefined else None\n        field_required = True if field_default is None else False\n        description = field_type + \", \"\n        if field_desc is not None:\n            description += (field_desc.strip() + \", \") \n        description += (\"required\" if field_required else \"optional\")\n        if field_default is not None:\n            description += (\", Default value: \" + field_default)\n        fields_info[field_name] = description\n\n    if len(fields_info) == 0:\n        return \"\" \n    fields_info_str = json.dumps(fields_info, indent=4)\n    return fields_info_str\n</code></pre>"},{"location":"api/actions/#evoagentx.actions.ActionInput.get_required_input_names","title":"get_required_input_names  <code>classmethod</code>","text":"<pre><code>get_required_input_names() -&gt; List[str]\n</code></pre> <p>Get a list of all required input parameter names.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: Names of all parameters that are required (don't have default values).</p> Source code in <code>evoagentx/actions/action.py</code> <pre><code>@classmethod\ndef get_required_input_names(cls) -&gt; List[str]:\n    \"\"\"Get a list of all required input parameter names.\n\n    Returns:\n        List[str]: Names of all parameters that are required (don't have default values).\n    \"\"\"\n    required_fields = []\n    attrs = cls.get_attrs()\n    for field_name, field_info in cls.model_fields.items():\n        if field_name not in attrs:\n            continue\n        field_default = field_info.default\n        # A field is required if it doesn't have a default value\n        if field_default is PydanticUndefined:\n            required_fields.append(field_name)\n    return required_fields\n</code></pre>"},{"location":"api/actions/#evoagentx.actions.ActionOutput","title":"ActionOutput","text":"<pre><code>ActionOutput(**kwargs)\n</code></pre> <p>               Bases: <code>LLMOutputParser</code></p> <p>Output representation for actions.</p> <p>This class handles the structured output of actions, providing methods to convert the output to structured data. It inherits from LLMOutputParser to support parsing of LLM outputs into structured action results.</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/actions/#evoagentx.actions.ActionOutput.to_str","title":"to_str","text":"<pre><code>to_str() -&gt; str\n</code></pre> <p>Convert the output to a formatted JSON string.</p> <p>Returns:</p> Type Description <code>str</code> <p>A pretty-printed JSON string representation of the structured data.</p> Source code in <code>evoagentx/actions/action.py</code> <pre><code>def to_str(self) -&gt; str:\n    \"\"\"Convert the output to a formatted JSON string.\n\n    Returns:\n        A pretty-printed JSON string representation of the structured data.\n    \"\"\"\n    return json.dumps(self.get_structured_data(), indent=4)\n</code></pre>"},{"location":"api/actions/#evoagentx.actions.CodeExtraction","title":"CodeExtraction","text":"<pre><code>CodeExtraction(**kwargs)\n</code></pre> <p>               Bases: <code>Action</code></p> <p>An action that extracts and organizes code blocks from text.</p> <p>This action uses an LLM to analyze text containing code blocks, extract them, suggest appropriate filenames, and save them to a specified directory. It can also identify which file is likely the main entry point based on heuristics.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the action.</p> <code>description</code> <code>str</code> <p>A description of what the action does.</p> <code>prompt</code> <code>Optional[str]</code> <p>The prompt template used by the action.</p> <code>inputs_format</code> <code>Optional[Type[ActionInput]]</code> <p>The expected format of inputs to this action.</p> <code>outputs_format</code> <code>Optional[Type[Parser]]</code> <p>The format of the action's output.</p> Source code in <code>evoagentx/actions/code_extraction.py</code> <pre><code>def __init__(self, **kwargs):\n\n    name = kwargs.pop(\"name\") if \"name\" in kwargs else CODE_EXTRACTION[\"name\"]\n    description = kwargs.pop(\"description\") if \"description\" in kwargs else CODE_EXTRACTION[\"description\"]\n    prompt = kwargs.pop(\"prompt\") if \"prompt\" in kwargs else CODE_EXTRACTION[\"prompt\"]\n    # inputs_format = kwargs.pop(\"inputs_format\") if \"inputs_format\" in kwargs else CodeExtractionInput\n    # outputs_format = kwargs.pop(\"outputs_format\") if \"outputs_format\" in kwargs else CodeExtractionOutput\n    inputs_format = kwargs.pop(\"inputs_format\", None) or CodeExtractionInput\n    outputs_format = kwargs.pop(\"outputs_format\", None) or CodeExtractionOutput\n    super().__init__(name=name, description=description, prompt=prompt, inputs_format=inputs_format, outputs_format=outputs_format, **kwargs)\n</code></pre>"},{"location":"api/actions/#evoagentx.actions.CodeExtraction.identify_main_file","title":"identify_main_file","text":"<pre><code>identify_main_file(saved_files: Dict[str, str]) -&gt; Optional[str]\n</code></pre> <p>Identify the main file from the saved files based on content and file type.</p> <p>This method uses a combination of common filename conventions and content analysis to determine which file is likely the main entry point of a project.</p> <p>Parameters:</p> Name Type Description Default <code>saved_files</code> <code>Dict[str, str]</code> <p>Dictionary mapping filenames to their full paths</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Path to the main file if found, None otherwise</p> Source code in <code>evoagentx/actions/code_extraction.py</code> <pre><code>def identify_main_file(self, saved_files: Dict[str, str]) -&gt; Optional[str]:\n    \"\"\"Identify the main file from the saved files based on content and file type.\n\n    This method uses a combination of common filename conventions and content\n    analysis to determine which file is likely the main entry point of a project.\n\n    Args:\n        saved_files: Dictionary mapping filenames to their full paths\n\n    Returns:\n        Path to the main file if found, None otherwise\n\n    \"\"\"\n    # Priority lookup for common main files by language\n    main_file_priorities = [\n        # HTML files\n        \"index.html\",\n        # Python files\n        \"main.py\", \n        \"app.py\",\n        # JavaScript files\n        \"index.js\",\n        \"main.js\",\n        \"app.js\",\n        # Java files\n        \"Main.java\",\n        # C/C++ files\n        \"main.cpp\", \n        \"main.c\",\n        # Go files\n        \"main.go\",\n        # Other common entry points\n        \"index.php\",\n        \"Program.cs\"\n    ]\n\n    # First check priority list\n    for main_file in main_file_priorities:\n        if main_file in saved_files:\n            return saved_files[main_file]\n\n    # If no priority file found, use heuristics based on file extensions\n\n    # If we have HTML files, use the first one\n    html_files = {k: v for k, v in saved_files.items() if k.endswith('.html')}\n    if html_files:\n        return next(iter(html_files.values()))\n\n    # Check for Python files with \"__main__\" section\n    py_files = {k: v for k, v in saved_files.items() if k.endswith('.py')}\n    if py_files:\n        for filename, path in py_files.items():\n            with open(path, 'r', encoding='utf-8') as f:\n                content = f.read()\n                if \"if __name__ == '__main__'\" in content or 'if __name__ == \"__main__\"' in content:\n                    return path\n        # If no main found, return the first Python file\n        if py_files:\n            return next(iter(py_files.values()))\n\n    # If we have Java files, look for one with a main method\n    java_files = {k: v for k, v in saved_files.items() if k.endswith('.java')}\n    if java_files:\n        for filename, path in java_files.items():\n            with open(path, 'r', encoding='utf-8') as f:\n                content = f.read()\n                if \"public static void main\" in content:\n                    return path\n        # If no main found, return the first Java file\n        if java_files:\n            return next(iter(java_files.values()))\n\n    # For JavaScript applications\n    js_files = {k: v for k, v in saved_files.items() if k.endswith('.js')}\n    if js_files:\n        return next(iter(js_files.values()))\n\n    # If all else fails, return the first file\n    if saved_files:\n        return next(iter(saved_files.values()))\n\n    # No files found\n    return None\n</code></pre>"},{"location":"api/actions/#evoagentx.actions.CodeExtraction.save_code_blocks","title":"save_code_blocks","text":"<pre><code>save_code_blocks(code_blocks: List[Dict], target_directory: str) -&gt; Dict[str, str]\n</code></pre> <p>Save code blocks to files in the target directory.</p> <p>Creates the target directory if it doesn't exist and saves each code block to a file with an appropriate name, handling filename conflicts.</p> <p>Parameters:</p> Name Type Description Default <code>code_blocks</code> <code>List[Dict]</code> <p>List of dictionaries containing code block information</p> required <code>target_directory</code> <code>str</code> <p>Directory path where files should be saved</p> required <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dictionary mapping filenames to their full paths</p> Source code in <code>evoagentx/actions/code_extraction.py</code> <pre><code>def save_code_blocks(self, code_blocks: List[Dict], target_directory: str) -&gt; Dict[str, str]:\n    \"\"\"Save code blocks to files in the target directory.\n\n    Creates the target directory if it doesn't exist and saves each code block\n    to a file with an appropriate name, handling filename conflicts.\n\n    Args:\n        code_blocks: List of dictionaries containing code block information\n        target_directory: Directory path where files should be saved\n\n    Returns:\n        Dictionary mapping filenames to their full paths\n    \"\"\"\n    os.makedirs(target_directory, exist_ok=True)\n    saved_files = {}\n\n    for block in code_blocks:\n        filename = block.get(\"filename\", \"unknown.txt\")\n        content = block.get(\"content\", \"\")\n\n        # Skip empty blocks\n        if not content.strip():\n            continue\n\n        # Handle filename conflicts\n        base_filename = filename\n        counter = 1\n        while filename in saved_files:\n            name_parts = base_filename.split('.')\n            if len(name_parts) &gt; 1:\n                filename = f\"{'.'.join(name_parts[:-1])}_{counter}.{name_parts[-1]}\"\n            else:\n                filename = f\"{base_filename}_{counter}\"\n            counter += 1\n\n        # Save to file\n        file_path = os.path.join(target_directory, filename)\n        with open(file_path, 'w', encoding='utf-8') as f:\n            f.write(content)\n\n        # Add to map\n        saved_files[filename] = file_path\n\n    return saved_files\n</code></pre>"},{"location":"api/actions/#evoagentx.actions.CodeExtraction.execute","title":"execute","text":"<pre><code>execute(llm: Optional[BaseLLM] = None, inputs: Optional[dict] = None, sys_msg: Optional[str] = None, return_prompt: bool = False, **kwargs) -&gt; CodeExtractionOutput\n</code></pre> <p>Execute the CodeExtraction action.</p> <p>Extracts code blocks from the provided text using the specified LLM, saves them to the target directory, and identifies the main file.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>Optional[BaseLLM]</code> <p>The LLM to use for code extraction</p> <code>None</code> <code>inputs</code> <code>Optional[dict]</code> <p>Dictionary containing: - code_string: The string with code blocks to extract - target_directory: Where to save the files - project_name: Optional project folder name</p> <code>None</code> <code>sys_msg</code> <code>Optional[str]</code> <p>Optional system message override for the LLM</p> <code>None</code> <code>return_prompt</code> <code>bool</code> <p>Whether to return the prompt along with the result</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>CodeExtractionOutput</code> <p>CodeExtractionOutput with extracted file information</p> Source code in <code>evoagentx/actions/code_extraction.py</code> <pre><code>def execute(self, llm: Optional[BaseLLM] = None, inputs: Optional[dict] = None, sys_msg: Optional[str]=None, return_prompt: bool = False, **kwargs) -&gt; CodeExtractionOutput:\n    \"\"\"Execute the CodeExtraction action.\n\n    Extracts code blocks from the provided text using the specified LLM,\n    saves them to the target directory, and identifies the main file.\n\n    Args:\n        llm: The LLM to use for code extraction\n        inputs: Dictionary containing:\n            - code_string: The string with code blocks to extract\n            - target_directory: Where to save the files\n            - project_name: Optional project folder name\n        sys_msg: Optional system message override for the LLM\n        return_prompt: Whether to return the prompt along with the result\n        **kwargs (Any): Additional keyword arguments\n\n    Returns:\n        CodeExtractionOutput with extracted file information\n    \"\"\"\n    if not llm:\n        error_msg = \"CodeExtraction action requires an LLM.\"\n        return CodeExtractionOutput(extracted_files={}, error=error_msg)\n\n    if not inputs:\n        error_msg = \"CodeExtraction action received invalid `inputs`: None or empty.\"\n        return CodeExtractionOutput(extracted_files={}, error=error_msg)\n\n    code_string = inputs.get(\"code_string\", \"\")\n    target_directory = inputs.get(\"target_directory\", \"\")\n    project_name = inputs.get(\"project_name\", None)\n\n    if not code_string:\n        error_msg = \"No code string provided.\"\n        return CodeExtractionOutput(extracted_files={}, error=error_msg)\n\n    if not target_directory:\n        error_msg = \"No target directory provided.\"\n        return CodeExtractionOutput(extracted_files={}, error=error_msg)\n\n    # Create project folder if name is provided\n    if project_name:\n        project_dir = os.path.join(target_directory, project_name)\n    else:\n        project_dir = target_directory\n\n    try:\n        # Use LLM to extract code blocks and suggest filenames\n        prompt_params = {\"code_string\": code_string}\n        system_message = CODE_EXTRACTION[\"system_prompt\"] if sys_msg is None else sys_msg\n\n        llm_response: CodeBlockList = llm.generate(\n            prompt=self.prompt.format(**prompt_params),\n            system_message=system_message,\n            parser=CodeBlockList,\n            parse_mode=\"json\"\n        )\n        code_blocks = llm_response.get_structured_data().get(\"code_blocks\", [])\n\n        # Save code blocks to files\n        saved_files = self.save_code_blocks(code_blocks, project_dir)\n\n        # Identify main file\n        main_file = self.identify_main_file(saved_files)\n\n        result = CodeExtractionOutput(\n            extracted_files=saved_files,\n            main_file=main_file\n        )\n\n        if return_prompt:\n            return result, self.prompt.format(**prompt_params)\n\n        return result\n\n    except Exception as e:\n        error_msg = f\"Error extracting code: {str(e)}\"\n        return CodeExtractionOutput(extracted_files={}, error=error_msg)\n</code></pre>"},{"location":"api/agents/","title":"\ud83e\udd16 Agents","text":""},{"location":"api/agents/#evoagentx.agents","title":"evoagentx.agents","text":""},{"location":"api/agents/#evoagentx.agents.Agent","title":"Agent","text":"<pre><code>Agent(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModule</code></p> <p>Base class for all agents. </p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Unique identifier for the agent</p> <code>description</code> <code>str</code> <p>Human-readable description of the agent's purpose</p> <code>llm_config</code> <code>Optional[LLMConfig]</code> <p>Configuration for the language model. If provided, a new LLM instance will be created.  Otherwise, the existing LLM instance specified in the <code>llm</code> field will be used.   </p> <code>llm</code> <code>Optional[BaseLLM]</code> <p>Language model instance. If provided, the existing LLM instance will be used. </p> <code>agent_id</code> <code>Optional[str]</code> <p>Unique ID for the agent, auto-generated if not provided</p> <code>system_prompt</code> <code>Optional[str]</code> <p>System prompt for the Agent.</p> <code>actions</code> <code>List[Action]</code> <p>List of available actions</p> <code>n</code> <code>Optional[int]</code> <p>Number of latest messages used to provide context for action execution. It uses all the messages in short term memory by default. </p> <code>is_human</code> <code>bool</code> <p>Whether this agent represents a human user</p> <code>version</code> <code>int</code> <p>Version number of the agent, default is 0.</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/agents/#evoagentx.agents.Agent.__call__","title":"__call__","text":"<pre><code>__call__(*args: Any, **kwargs: Any) -&gt; Union[dict, Coroutine[Any, Any, dict]]\n</code></pre> <p>Make the operator callable and automatically choose between sync and async execution.</p> Source code in <code>evoagentx/agents/agent.py</code> <pre><code>def __call__(self, *args: Any, **kwargs: Any) -&gt; Union[dict, Coroutine[Any, Any, dict]]:\n    \"\"\"Make the operator callable and automatically choose between sync and async execution.\"\"\"\n    try:\n        # Safe way to check if we're inside an async environment\n        asyncio.get_running_loop()\n        return self.async_execute(*args, **kwargs)\n    except RuntimeError:\n        # No running loop \u2014 likely in sync context or worker thread\n        return self.execute(*args, **kwargs)\n</code></pre>"},{"location":"api/agents/#evoagentx.agents.Agent.async_execute","title":"async_execute  <code>async</code>","text":"<pre><code>async_execute(action_name: str, msgs: Optional[List[Message]] = None, action_input_data: Optional[dict] = None, return_msg_type: Optional[MessageType] = UNKNOWN, return_action_input_data: Optional[bool] = False, **kwargs) -&gt; Union[Message, Tuple[Message, dict]]\n</code></pre> <p>Execute an action asynchronously with the given context and return results.</p> <p>This is the async version of the execute method, allowing it to perform actions based on the current conversation context.</p> <p>Parameters:</p> Name Type Description Default <code>action_name</code> <code>str</code> <p>The name of the action to execute</p> required <code>msgs</code> <code>Optional[List[Message]]</code> <p>Optional list of messages providing context for the action</p> <code>None</code> <code>action_input_data</code> <code>Optional[dict]</code> <p>Optional pre-extracted input data for the action</p> <code>None</code> <code>return_msg_type</code> <code>Optional[MessageType]</code> <p>Message type for the return message</p> <code>UNKNOWN</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters, may include workflow information</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Message</code> <code>Union[Message, Tuple[Message, dict]]</code> <p>A message containing the execution results</p> Source code in <code>evoagentx/agents/agent.py</code> <pre><code>async def async_execute(\n    self, \n    action_name: str, \n    msgs: Optional[List[Message]] = None, \n    action_input_data: Optional[dict] = None, \n    return_msg_type: Optional[MessageType] = MessageType.UNKNOWN,\n    return_action_input_data: Optional[bool] = False, \n    **kwargs\n) -&gt; Union[Message, Tuple[Message, dict]]:\n    \"\"\"Execute an action asynchronously with the given context and return results.\n\n    This is the async version of the execute method, allowing it to perform actions\n    based on the current conversation context.\n\n    Args:\n        action_name: The name of the action to execute\n        msgs: Optional list of messages providing context for the action\n        action_input_data: Optional pre-extracted input data for the action\n        return_msg_type: Message type for the return message\n        **kwargs (Any): Additional parameters, may include workflow information\n\n    Returns:\n        Message: A message containing the execution results\n    \"\"\"\n    action, action_input_data = self._prepare_execution(\n        action_name=action_name,\n        msgs=msgs,\n        action_input_data=action_input_data,\n        **kwargs\n    )\n\n    # execute action asynchronously\n    async_execute_source = inspect.getsource(action.async_execute)\n    if \"NotImplementedError\" in async_execute_source:\n        # if the async_execute method is not implemented, use the execute method instead\n        execution_results = action.execute(\n            llm=self.llm, \n            inputs=action_input_data, \n            sys_msg=self.system_prompt,\n            return_prompt=True,\n            **kwargs\n        )\n    else:\n        execution_results = await action.async_execute(\n            llm=self.llm, \n            inputs=action_input_data, \n            sys_msg=self.system_prompt,\n            return_prompt=True,\n            **kwargs\n    )\n    action_output, prompt = execution_results\n\n    message = self._create_output_message(\n        action_output=action_output,\n        prompt=prompt,\n        action_name=action_name,\n        return_msg_type=return_msg_type,\n        **kwargs\n    )\n    if return_action_input_data:\n        return message, action_input_data\n    return message\n</code></pre>"},{"location":"api/agents/#evoagentx.agents.Agent.execute","title":"execute","text":"<pre><code>execute(action_name: str, msgs: Optional[List[Message]] = None, action_input_data: Optional[dict] = None, return_msg_type: Optional[MessageType] = UNKNOWN, return_action_input_data: Optional[bool] = False, **kwargs) -&gt; Union[Message, Tuple[Message, dict]]\n</code></pre> <p>Execute an action with the given context and return results.</p> <p>This is the core method for agent functionality, allowing it to perform actions based on the current conversation context.</p> <p>Parameters:</p> Name Type Description Default <code>action_name</code> <code>str</code> <p>The name of the action to execute</p> required <code>msgs</code> <code>Optional[List[Message]]</code> <p>Optional list of messages providing context for the action</p> <code>None</code> <code>action_input_data</code> <code>Optional[dict]</code> <p>Optional pre-extracted input data for the action</p> <code>None</code> <code>return_msg_type</code> <code>Optional[MessageType]</code> <p>Message type for the return message</p> <code>UNKNOWN</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters, may include workflow information</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Message</code> <code>Union[Message, Tuple[Message, dict]]</code> <p>A message containing the execution results</p> Source code in <code>evoagentx/agents/agent.py</code> <pre><code>def execute(\n    self, \n    action_name: str, \n    msgs: Optional[List[Message]] = None, \n    action_input_data: Optional[dict] = None, \n    return_msg_type: Optional[MessageType] = MessageType.UNKNOWN,\n    return_action_input_data: Optional[bool] = False, \n    **kwargs\n) -&gt; Union[Message, Tuple[Message, dict]]:\n    \"\"\"Execute an action with the given context and return results.\n\n    This is the core method for agent functionality, allowing it to perform actions\n    based on the current conversation context.\n\n    Args:\n        action_name: The name of the action to execute\n        msgs: Optional list of messages providing context for the action\n        action_input_data: Optional pre-extracted input data for the action\n        return_msg_type: Message type for the return message\n        **kwargs (Any): Additional parameters, may include workflow information\n\n    Returns:\n        Message: A message containing the execution results\n    \"\"\"\n    action, action_input_data = self._prepare_execution(\n        action_name=action_name,\n        msgs=msgs,\n        action_input_data=action_input_data,\n        **kwargs\n    )\n\n    # execute action\n    execution_results = action.execute(\n        llm=self.llm, \n        inputs=action_input_data, \n        sys_msg=self.system_prompt,\n        return_prompt=True,\n        **kwargs\n    )\n    action_output, prompt = execution_results\n\n    message = self._create_output_message(\n        action_output=action_output,\n        prompt=prompt,\n        action_name=action_name,\n        return_msg_type=return_msg_type,\n        **kwargs\n    )\n    if return_action_input_data:\n        return message, action_input_data\n    return message\n</code></pre>"},{"location":"api/agents/#evoagentx.agents.Agent.init_llm","title":"init_llm","text":"<pre><code>init_llm()\n</code></pre> <p>Initialize the language model for the agent.</p> Source code in <code>evoagentx/agents/agent.py</code> <pre><code>def init_llm(self):\n    \"\"\"\n    Initialize the language model for the agent.\n    \"\"\"\n    assert self.llm_config or self.llm, \"must provide either 'llm_config' or 'llm' when is_human=False\"\n    if self.llm_config and not self.llm:\n        llm_cls = MODEL_REGISTRY.get_model(self.llm_config.llm_type)\n        self.llm = llm_cls(config=self.llm_config)\n    if self.llm:\n        self.llm_config = self.llm.config\n</code></pre>"},{"location":"api/agents/#evoagentx.agents.Agent.init_long_term_memory","title":"init_long_term_memory","text":"<pre><code>init_long_term_memory()\n</code></pre> <p>Initialize long-term memory components.</p> Source code in <code>evoagentx/agents/agent.py</code> <pre><code>def init_long_term_memory(self):\n    \"\"\"\n    Initialize long-term memory components.\n    \"\"\"\n    assert self.storage_handler is not None, \"must provide ``storage_handler`` when use_long_term_memory=True\"\n    # TODO revise the initialisation of long_term_memory and long_term_memory_manager\n    if not self.long_term_memory:\n        self.long_term_memory = LongTermMemory()\n    if not self.long_term_memory_manager:\n        self.long_term_memory_manager = MemoryManager(\n            storage_handler=self.storage_handler,\n            memory=self.long_term_memory\n        )\n</code></pre>"},{"location":"api/agents/#evoagentx.agents.Agent.init_context_extractor","title":"init_context_extractor","text":"<pre><code>init_context_extractor()\n</code></pre> <p>Initialize the context extraction action.</p> Source code in <code>evoagentx/agents/agent.py</code> <pre><code>def init_context_extractor(self):\n    \"\"\"\n    Initialize the context extraction action.\n    \"\"\"\n    cext_action = ContextExtraction()\n    self.cext_action_name = cext_action.name\n    self.add_action(cext_action)\n</code></pre>"},{"location":"api/agents/#evoagentx.agents.Agent.add_action","title":"add_action","text":"<pre><code>add_action(action: Type[Action])\n</code></pre> <p>Add a new action to the agent's available actions.</p> <p>Parameters:</p> Name Type Description Default <code>action</code> <code>Type[Action]</code> <p>The action instance to add</p> required Source code in <code>evoagentx/agents/agent.py</code> <pre><code>def add_action(self, action: Type[Action]):\n    \"\"\"\n    Add a new action to the agent's available actions.\n\n    Args:\n        action: The action instance to add\n    \"\"\"\n    action_name  = action.name\n    if action_name in self._action_map:\n        return\n    self.actions.append(action)\n    self._action_map[action_name] = action\n</code></pre>"},{"location":"api/agents/#evoagentx.agents.Agent.check_action_name","title":"check_action_name","text":"<pre><code>check_action_name(action_name: str)\n</code></pre> <p>Check if an action name is valid for this agent.</p> <p>Parameters:</p> Name Type Description Default <code>action_name</code> <code>str</code> <p>Name of the action to check</p> required Source code in <code>evoagentx/agents/agent.py</code> <pre><code>def check_action_name(self, action_name: str):\n    \"\"\"\n    Check if an action name is valid for this agent.\n\n    Args:\n        action_name: Name of the action to check\n    \"\"\"\n    if action_name not in self._action_map:\n        raise KeyError(f\"'{action_name}' is an invalid action for {self.name}! Available action names: {list(self._action_map.keys())}\")\n</code></pre>"},{"location":"api/agents/#evoagentx.agents.Agent.get_action","title":"get_action","text":"<pre><code>get_action(action_name: str) -&gt; Action\n</code></pre> <p>Retrieves the Action instance associated with the given name.</p> <p>Parameters:</p> Name Type Description Default <code>action_name</code> <code>str</code> <p>Name of the action to retrieve</p> required <p>Returns:</p> Type Description <code>Action</code> <p>The Action instance with the specified name</p> Source code in <code>evoagentx/agents/agent.py</code> <pre><code>def get_action(self, action_name: str) -&gt; Action:\n    \"\"\"\n    Retrieves the Action instance associated with the given name.\n\n    Args:\n        action_name: Name of the action to retrieve\n\n    Returns:\n        The Action instance with the specified name\n    \"\"\"\n    self.check_action_name(action_name=action_name)\n    return self._action_map[action_name]\n</code></pre>"},{"location":"api/agents/#evoagentx.agents.Agent.get_action_name","title":"get_action_name","text":"<pre><code>get_action_name(action_cls: Type[Action]) -&gt; str\n</code></pre> <p>Searches through the agent's actions to find one matching the specified type.</p> <p>Parameters:</p> Name Type Description Default <code>action_cls</code> <code>Type[Action]</code> <p>The Action class type to search for</p> required <p>Returns:</p> Type Description <code>str</code> <p>The name of the matching action</p> Source code in <code>evoagentx/agents/agent.py</code> <pre><code>def get_action_name(self, action_cls: Type[Action]) -&gt; str:\n    \"\"\"\n    Searches through the agent's actions to find one matching the specified type.\n\n    Args:\n        action_cls: The Action class type to search for\n\n    Returns:\n        The name of the matching action\n    \"\"\"\n    for name, action in self._action_map.items():\n        if isinstance(action, action_cls):\n            return name\n    raise ValueError(f\"Couldn't find an action that matches Type '{action_cls.__name__}'\")\n</code></pre>"},{"location":"api/agents/#evoagentx.agents.Agent.get_action_inputs","title":"get_action_inputs","text":"<pre><code>get_action_inputs(action: Action) -&gt; Union[dict, None]\n</code></pre> <p>Uses the context extraction action to determine appropriate inputs for the specified action based on the conversation history.</p> <p>Parameters:</p> Name Type Description Default <code>action</code> <code>Action</code> <p>The action for which to extract inputs</p> required <p>Returns:</p> Type Description <code>Union[dict, None]</code> <p>Dictionary of extracted input data, or None if extraction fails</p> Source code in <code>evoagentx/agents/agent.py</code> <pre><code>def get_action_inputs(self, action: Action) -&gt; Union[dict, None]:\n    \"\"\"\n    Uses the context extraction action to determine appropriate inputs\n    for the specified action based on the conversation history.\n\n    Args:\n        action: The action for which to extract inputs\n\n    Returns:\n        Dictionary of extracted input data, or None if extraction fails\n    \"\"\"\n    # return the input data of an action.\n    context = self.short_term_memory.get(n=self.n)\n    cext_action = self.get_action(self.cext_action_name)\n    action_inputs = cext_action.execute(llm=self.llm, action=action, context=context)\n    return action_inputs\n</code></pre>"},{"location":"api/agents/#evoagentx.agents.Agent.get_all_actions","title":"get_all_actions","text":"<pre><code>get_all_actions() -&gt; List[Action]\n</code></pre> <p>Get all actions except the context extraction action.</p> <p>Returns:</p> Type Description <code>List[Action]</code> <p>List of Action instances available for execution</p> Source code in <code>evoagentx/agents/agent.py</code> <pre><code>def get_all_actions(self) -&gt; List[Action]:\n    \"\"\"Get all actions except the context extraction action.\n\n    Returns:\n        List of Action instances available for execution\n    \"\"\"\n    actions = [action for action in self.actions if action.name != self.cext_action_name]\n    return actions\n</code></pre>"},{"location":"api/agents/#evoagentx.agents.Agent.get_agent_profile","title":"get_agent_profile","text":"<pre><code>get_agent_profile(action_names: List[str] = None) -&gt; str\n</code></pre> <p>Generate a human-readable profile of the agent and its capabilities.</p> <p>Parameters:</p> Name Type Description Default <code>action_names</code> <code>List[str]</code> <p>Optional list of action names to include in the profile.           If None, all actions are included.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>A formatted string containing the agent profile</p> Source code in <code>evoagentx/agents/agent.py</code> <pre><code>def get_agent_profile(self, action_names: List[str] = None) -&gt; str:\n    \"\"\"Generate a human-readable profile of the agent and its capabilities.\n\n    Args:\n        action_names: Optional list of action names to include in the profile.\n                      If None, all actions are included.\n\n    Returns:\n        A formatted string containing the agent profile\n    \"\"\"\n    all_actions = self.get_all_actions()\n    if action_names is None:\n        # if `action_names` is None, return description of all actions \n        action_descriptions = \"\\n\".join([f\"  - {action.name}: {action.description}\" for action in all_actions])\n    else: \n        # otherwise, only return description of actions that matches `action_names`\n        action_descriptions = \"\\n\".join([f\"  - {action.name}: {action.description}\" for action in all_actions if action.name in action_names])\n    profile = f\"Agent Name: {self.name}\\nDescription: {self.description}\\nAvailable Actions:\\n{action_descriptions}\"\n    return profile\n</code></pre>"},{"location":"api/agents/#evoagentx.agents.Agent.clear_short_term_memory","title":"clear_short_term_memory","text":"<pre><code>clear_short_term_memory()\n</code></pre> <p>Remove all content from the agent's short-term memory.</p> Source code in <code>evoagentx/agents/agent.py</code> <pre><code>def clear_short_term_memory(self):\n    \"\"\"\n    Remove all content from the agent's short-term memory.\n    \"\"\"\n    pass \n</code></pre>"},{"location":"api/agents/#evoagentx.agents.Agent.get_prompts","title":"get_prompts","text":"<pre><code>get_prompts() -&gt; dict\n</code></pre> <p>Get all the prompts of the agent.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary with keys in the format 'agent_name::action_name' and values containing the system_prompt and action prompt.</p> Source code in <code>evoagentx/agents/agent.py</code> <pre><code>def get_prompts(self) -&gt; dict:\n    \"\"\"\n    Get all the prompts of the agent.\n\n    Returns:\n        dict: A dictionary with keys in the format 'agent_name::action_name' and values\n            containing the system_prompt and action prompt.\n    \"\"\"\n    prompts = {}\n    for action in self.get_all_actions():\n        prompts[action.name] = {\n            \"system_prompt\": self.system_prompt, \n            \"prompt\": action.prompt\n        }\n    return prompts\n</code></pre>"},{"location":"api/agents/#evoagentx.agents.Agent.set_prompt","title":"set_prompt","text":"<pre><code>set_prompt(action_name: str, prompt: str, system_prompt: Optional[str] = None) -&gt; bool\n</code></pre> <p>Set the prompt for a specific action of this agent.</p> <p>Parameters:</p> Name Type Description Default <code>action_name</code> <code>str</code> <p>Name of the action whose prompt should be updated</p> required <code>prompt</code> <code>str</code> <p>New prompt text to set for the action</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>Optional new system prompt to set for the agent</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the prompt was successfully updated, False otherwise</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the action_name does not exist for this agent</p> Source code in <code>evoagentx/agents/agent.py</code> <pre><code>def set_prompt(self, action_name: str, prompt: str, system_prompt: Optional[str] = None) -&gt; bool:\n    \"\"\"\n    Set the prompt for a specific action of this agent.\n\n    Args:\n        action_name: Name of the action whose prompt should be updated\n        prompt: New prompt text to set for the action\n        system_prompt: Optional new system prompt to set for the agent\n\n    Returns:\n        bool: True if the prompt was successfully updated, False otherwise\n\n    Raises:\n        KeyError: If the action_name does not exist for this agent\n    \"\"\"\n    try:\n        action = self.get_action(action_name)\n        action.prompt = prompt\n\n        if system_prompt is not None:\n            self.system_prompt = system_prompt\n\n        return True\n    except KeyError:\n        raise KeyError(f\"Action '{action_name}' not found in agent '{self.name}'\")\n</code></pre>"},{"location":"api/agents/#evoagentx.agents.Agent.set_prompts","title":"set_prompts","text":"<pre><code>set_prompts(prompts: dict) -&gt; bool\n</code></pre> <p>Set the prompts for all actions of this agent.</p> <p>Parameters:</p> Name Type Description Default <code>prompts</code> <code>dict</code> <p>A dictionary with keys in the format 'action_name' and values containing the system_prompt and action prompt.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the prompts were successfully updated, False otherwise</p> Source code in <code>evoagentx/agents/agent.py</code> <pre><code>def set_prompts(self, prompts: dict) -&gt; bool:\n    \"\"\"\n    Set the prompts for all actions of this agent.\n\n    Args:\n        prompts: A dictionary with keys in the format 'action_name' and values\n            containing the system_prompt and action prompt.\n\n    Returns:\n        bool: True if the prompts were successfully updated, False otherwise\n    \"\"\"\n    for action_name, prompt_data in prompts.items():\n        # self.set_prompt(action_name, prompt_data[\"prompt\"], prompt_data[\"system_prompt\"])\n        if not isinstance(prompt_data, dict):\n            raise ValueError(f\"Invalid prompt data for action '{action_name}'. Expected a dictionary with 'prompt' and 'system_prompt' (optional) keys.\")\n        if \"prompt\" not in prompt_data:\n            raise ValueError(f\"Missing 'prompt' key in prompt data for action '{action_name}'.\")\n        self.set_prompt(action_name, prompt_data[\"prompt\"], prompt_data.get(\"system_prompt\", None))\n    return True\n</code></pre>"},{"location":"api/agents/#evoagentx.agents.Agent.save_module","title":"save_module","text":"<pre><code>save_module(path: str, ignore: List[str] = [], **kwargs) -&gt; str\n</code></pre> <p>Save the agent to persistent storage.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path where the agent should be saved</p> required <code>ignore</code> <code>List[str]</code> <p>List of field names to exclude from serialization</p> <code>[]</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters for the save operation</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The path where the agent was saved</p> Source code in <code>evoagentx/agents/agent.py</code> <pre><code>def save_module(self, path: str, ignore: List[str] = [], **kwargs)-&gt; str:\n    \"\"\"Save the agent to persistent storage.\n\n    Args:\n        path: Path where the agent should be saved\n        ignore: List of field names to exclude from serialization\n        **kwargs (Any): Additional parameters for the save operation\n\n    Returns:\n        The path where the agent was saved\n    \"\"\"\n    ignore_fields = self._save_ignore_fields + ignore\n    super().save_module(path=path, ignore=ignore_fields, **kwargs)\n</code></pre>"},{"location":"api/agents/#evoagentx.agents.Agent.load_module","title":"load_module  <code>classmethod</code>","text":"<pre><code>load_module(path: str, llm_config: LLMConfig = None, **kwargs) -&gt; Agent\n</code></pre> <p>load the agent from local storage. Must provide <code>llm_config</code> when loading the agent from local storage. </p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path of the file</p> required <code>llm_config</code> <code>LLMConfig</code> <p>The LLMConfig instance</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Agent</code> <code>Agent</code> <p>The loaded agent instance</p> Source code in <code>evoagentx/agents/agent.py</code> <pre><code>@classmethod\ndef load_module(cls, path: str, llm_config: LLMConfig = None, **kwargs) -&gt; \"Agent\":\n    \"\"\"\n    load the agent from local storage. Must provide `llm_config` when loading the agent from local storage. \n\n    Args:\n        path: The path of the file\n        llm_config: The LLMConfig instance\n\n    Returns:\n        Agent: The loaded agent instance\n    \"\"\"\n    assert llm_config is not None, \"must provide `llm_config` when using `load_module` or `from_file` to load the agent from local storage\"\n    agent = super().load_module(path=path, **kwargs)\n    agent[\"llm_config\"] = llm_config.to_dict()\n    return agent \n</code></pre>"},{"location":"api/agents/#evoagentx.agents.CustomizeAgent","title":"CustomizeAgent","text":"<pre><code>CustomizeAgent(name: str, description: str, prompt: str, llm_config: Optional[LLMConfig] = None, inputs: Optional[List[dict]] = None, outputs: Optional[List[dict]] = None, system_prompt: Optional[str] = None, output_parser: Optional[Type[ActionOutput]] = None, parse_mode: Optional[str] = 'title', parse_func: Optional[Callable] = None, title_format: Optional[str] = None, **kwargs)\n</code></pre> <p>               Bases: <code>Agent</code></p> <p>CustomizeAgent provides a flexible framework for creating specialized LLM-powered agents without  writing custom code. It enables the creation of agents with well-defined inputs and outputs,  custom prompt templates, and configurable parsing strategies. </p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the agent.</p> <code>description</code> <code>str</code> <p>A description of the agent's purpose and capabilities.</p> <code>prompt</code> <code>str</code> <p>The prompt template that will be used for the agent's primary action. Should contain placeholders in the format <code>{input_name}</code> for each input parameter.</p> <code>llm_config</code> <code>LLMConfig</code> <p>Configuration for the language model.</p> <code>inputs</code> <code>List[dict]</code> <p>List of input specifications, where each dict (e.g., <code>{\"name\": str, \"type\": str, \"description\": str, [\"required\": bool]}</code>) contains: - name (str): Name of the input parameter - type (str): Type of the input - description (str): Description of what the input represents - required (bool, optional): Whether this input is required (default: True)</p> <code>outputs</code> <code>List[dict]</code> <p>List of output specifications, where each dict (e.g., <code>{\"name\": str, \"type\": str, \"description\": str, [\"required\": bool]}</code>) contains: - name (str): Name of the output field - type (str): Type of the output - description (str): Description of what the output represents - required (bool, optional): Whether this output is required (default: True)</p> <code>system_prompt</code> <code>str</code> <p>The system prompt for the LLM. Defaults to DEFAULT_SYSTEM_PROMPT.</p> <code>output_parser</code> <code>Type[ActionOutput]</code> <p>A custom class for parsing the LLM's output. Must be a subclass of ActionOutput.</p> <code>parse_mode</code> <code>str</code> <p>Mode for parsing LLM output. Options are: - \"title\": Parse outputs using section titles (default) - \"str\": Parse as plain text - \"json\": Parse as JSON - \"xml\": Parse as XML - \"custom\": Use a custom parsing function</p> <code>parse_func</code> <code>Callable</code> <p>Custom function for parsing LLM output when parse_mode is \"custom\". Must accept a \"content\" parameter and return a dictionary.</p> <code>title_format</code> <code>str</code> <p>Format string for title parsing mode with {title} placeholder. Default is \"## {title}\".</p> Source code in <code>evoagentx/agents/customize_agent.py</code> <pre><code>def __init__(\n    self, \n    name: str, \n    description: str, \n    prompt: str, \n    llm_config: Optional[LLMConfig] = None, \n    inputs: Optional[List[dict]] = None, \n    outputs: Optional[List[dict]] = None, \n    system_prompt: Optional[str] = None,\n    output_parser: Optional[Type[ActionOutput]] = None, \n    parse_mode: Optional[str] = \"title\", \n    parse_func: Optional[Callable] = None, \n    title_format: Optional[str] = None, \n    **kwargs\n):\n    system_prompt = system_prompt or DEFAULT_SYSTEM_PROMPT\n    inputs = inputs or [] \n    outputs = outputs or [] \n\n    if isinstance(parse_func, str):\n        if not PARSE_FUNCTION_REGISTRY.has_function(parse_func):\n            raise ValueError(f\"parse function `{parse_func}` is not registered! To instantiate a CustomizeAgent from a file, you should use decorator `@register_parse_function` to register the parse function.\")\n        parse_func = PARSE_FUNCTION_REGISTRY.get_function(parse_func)\n\n    if isinstance(output_parser, str):\n        output_parser = MODULE_REGISTRY.get_module(output_parser)\n\n    # set default title format \n    if parse_mode == \"title\" and title_format is None:\n        title_format = \"## {title}\"\n\n    # validate the data \n    self.validate_data(\n        prompt = prompt, \n        inputs = inputs, \n        outputs = outputs, \n        output_parser = output_parser, \n        parse_mode = parse_mode, \n        parse_func = parse_func, \n        title_format = title_format\n    )\n\n    customize_action = self.create_customize_action(\n        name=name, \n        desc=description, \n        prompt=prompt, \n        inputs=inputs, \n        outputs=outputs, \n        parse_mode=parse_mode, \n        parse_func=parse_func,\n        output_parser=output_parser,\n        title_format=title_format\n    )\n    super().__init__(\n        name=name, \n        description=description, \n        llm_config=llm_config, \n        system_prompt=system_prompt, \n        actions=[customize_action], \n        **kwargs\n    )\n    self._store_inputs_outputs_info(inputs, outputs)\n    self.output_parser = output_parser \n    self.parse_mode = parse_mode \n    self.parse_func = parse_func \n    self.title_format = title_format\n</code></pre>"},{"location":"api/agents/#evoagentx.agents.CustomizeAgent.customize_action_name","title":"customize_action_name  <code>property</code>","text":"<pre><code>customize_action_name: str\n</code></pre> <p>Get the name of the primary custom action for this agent.</p> <p>Returns:</p> Type Description <code>str</code> <p>The name of the primary custom action</p>"},{"location":"api/agents/#evoagentx.agents.CustomizeAgent.action","title":"action  <code>property</code>","text":"<pre><code>action: Action\n</code></pre> <p>Get the primary custom action for this agent.</p> <p>Returns:</p> Type Description <code>Action</code> <p>The primary custom action</p>"},{"location":"api/agents/#evoagentx.agents.CustomizeAgent.prompt","title":"prompt  <code>property</code>","text":"<pre><code>prompt: str\n</code></pre> <p>Get the prompt for the primary custom action.</p> <p>Returns:</p> Type Description <code>str</code> <p>The prompt for the primary custom action</p>"},{"location":"api/agents/#evoagentx.agents.CustomizeAgent.create_customize_action","title":"create_customize_action","text":"<pre><code>create_customize_action(name: str, desc: str, prompt: str, inputs: List[dict], outputs: List[dict], parse_mode: str, parse_func: Optional[Callable] = None, output_parser: Optional[ActionOutput] = None, title_format: Optional[str] = '## {title}') -&gt; Action\n</code></pre> <p>Create a custom action based on the provided specifications.</p> <p>This method dynamically generates an Action class and instance with: - Input parameters defined by the inputs specification - Output format defined by the outputs specification - Custom execution logic using the customize_action_execute function</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Base name for the action</p> required <code>desc</code> <code>str</code> <p>Description of the action</p> required <code>prompt</code> <code>str</code> <p>Prompt template for the action</p> required <code>inputs</code> <code>List[dict]</code> <p>List of input field specifications</p> required <code>outputs</code> <code>List[dict]</code> <p>List of output field specifications</p> required <code>parse_mode</code> <code>str</code> <p>Mode to use for parsing LLM output</p> required <code>parse_func</code> <code>Optional[Callable]</code> <p>Optional custom parsing function</p> <code>None</code> <code>output_parser</code> <code>Optional[ActionOutput]</code> <p>Optional custom output parser class</p> <code>None</code> <p>Returns:</p> Type Description <code>Action</code> <p>A newly created Action instance</p> Source code in <code>evoagentx/agents/customize_agent.py</code> <pre><code>def create_customize_action(\n    self, \n    name: str, \n    desc: str, \n    prompt: str, \n    inputs: List[dict], \n    outputs: List[dict], \n    parse_mode: str, \n    parse_func: Optional[Callable] = None,\n    output_parser: Optional[ActionOutput] = None,\n    title_format: Optional[str] = \"## {title}\"\n) -&gt; Action:\n    \"\"\"Create a custom action based on the provided specifications.\n\n    This method dynamically generates an Action class and instance with:\n    - Input parameters defined by the inputs specification\n    - Output format defined by the outputs specification\n    - Custom execution logic using the customize_action_execute function\n\n    Args:\n        name: Base name for the action\n        desc: Description of the action\n        prompt: Prompt template for the action\n        inputs: List of input field specifications\n        outputs: List of output field specifications\n        parse_mode: Mode to use for parsing LLM output\n        parse_func: Optional custom parsing function\n        output_parser: Optional custom output parser class\n\n    Returns:\n        A newly created Action instance\n    \"\"\"\n    assert prompt is not None, \"must provide `prompt` when creating CustomizeAgent\"\n\n    # create the action input type\n    action_input_fields = {}\n    for field in inputs:\n        required = field.get(\"required\", True)\n        if required:\n            action_input_fields[field[\"name\"]] = (str, Field(description=field[\"description\"]))\n        else:\n            action_input_fields[field[\"name\"]] = (Optional[str], Field(default=None, description=field[\"description\"]))\n\n    action_input_type = create_model(\n        self._get_unique_class_name(\n            generate_dynamic_class_name(name+\" action_input\")\n        ),\n        **action_input_fields, \n        __base__=ActionInput\n    )\n\n    # create the action output type\n    if output_parser is None:\n        action_output_fields = {}\n        for field in outputs:\n            required = field.get(\"required\", True)\n            if required:\n                action_output_fields[field[\"name\"]] = (Union[str, dict, list], Field(description=field[\"description\"]))\n            else:\n                action_output_fields[field[\"name\"]] = (Optional[Union[str, dict, list]], Field(default=None, description=field[\"description\"]))\n\n        action_output_type = create_model(\n            self._get_unique_class_name(\n                generate_dynamic_class_name(name+\" action_output\")\n            ),\n            **action_output_fields, \n            __base__=ActionOutput,\n            # get_content_data=customize_get_content_data,\n            # to_str=customize_to_str\n        )\n    else:\n        # self._check_output_parser(outputs, output_parser)\n        action_output_type = output_parser\n\n    action_cls_name = self._get_unique_class_name(\n        generate_dynamic_class_name(name+\" action\")\n    )\n    customize_action_cls = create_model(\n        action_cls_name,\n        parse_mode=(Optional[str], Field(default=\"title\", description=\"the parse mode of the action, must be one of: ['title', 'str', 'json', 'xml', 'custom']\")),\n        parse_func=(Optional[Callable], Field(default=None, exclude=True, description=\"the function to parse the LLM output. It receives the LLM output and returns a dict.\")),\n        title_format=(Optional[str], Field(default=\"## {title}\", exclude=True, description=\"the format of the title. It is used when the `parse_mode` is 'title'.\")),\n        __base__=Action, \n        execute=customize_action_execute,\n        async_execute=customize_action_async_execute,\n        prepare_action_prompt=prepare_action_prompt,\n        prepare_extraction_prompt=prepare_extraction_prompt\n    )\n\n    customize_action = customize_action_cls(\n        name = action_cls_name,\n        description=desc, \n        prompt=prompt, \n        inputs_format=action_input_type, \n        outputs_format=action_output_type,\n        parse_mode=parse_mode,\n        parse_func=parse_func,\n        title_format=title_format\n    )\n    return customize_action\n</code></pre>"},{"location":"api/agents/#evoagentx.agents.CustomizeAgent.__call__","title":"__call__","text":"<pre><code>__call__(inputs: dict = None, return_msg_type: MessageType = UNKNOWN, **kwargs) -&gt; Message\n</code></pre> <p>Call the customize action.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict</code> <p>The inputs to the customize action.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ActionOutput</code> <code>Message</code> <p>The output of the customize action.</p> Source code in <code>evoagentx/agents/customize_agent.py</code> <pre><code>def __call__(self, inputs: dict = None, return_msg_type: MessageType = MessageType.UNKNOWN, **kwargs) -&gt; Message:\n    \"\"\"\n    Call the customize action.\n\n    Args:\n        inputs (dict): The inputs to the customize action.\n        **kwargs (Any): Additional keyword arguments.\n\n    Returns:\n        ActionOutput: The output of the customize action.\n    \"\"\"\n    # return self.execute(action_name=self.customize_action_name, action_input_data=inputs, **kwargs) \n    inputs = inputs or {} \n    return super().__call__(action_name=self.customize_action_name, action_input_data=inputs, return_msg_type=return_msg_type, **kwargs)\n</code></pre>"},{"location":"api/agents/#evoagentx.agents.CustomizeAgent.get_customize_agent_info","title":"get_customize_agent_info","text":"<pre><code>get_customize_agent_info() -&gt; dict\n</code></pre> <p>Get the information of the customize agent.</p> Source code in <code>evoagentx/agents/customize_agent.py</code> <pre><code>def get_customize_agent_info(self) -&gt; dict:\n    \"\"\"\n    Get the information of the customize agent.\n    \"\"\"\n    customize_action = self.get_action(self.customize_action_name)\n    action_input_params = customize_action.inputs_format.get_attrs()\n    action_output_params = customize_action.outputs_format.get_attrs()\n\n    config = {\n        \"class_name\": \"CustomizeAgent\",\n        \"name\": self.name,\n        \"description\": self.description,\n        \"prompt\": customize_action.prompt,\n        # \"llm_config\": self.llm_config.to_dict(exclude_none=True),\n        \"inputs\": [\n            {\n                \"name\": field,\n                \"type\": self._action_input_types[field],\n                \"description\": field_info.description,\n                \"required\": self._action_input_required[field]\n            }\n            for field, field_info in customize_action.inputs_format.model_fields.items() if field in action_input_params\n        ],\n        \"outputs\": [\n            {\n                \"name\": field,\n                \"type\": self._action_output_types[field],\n                \"description\": field_info.description,\n                \"required\": self._action_output_required[field]\n            }\n            for field, field_info in customize_action.outputs_format.model_fields.items() if field in action_output_params\n        ],\n        \"system_prompt\": self.system_prompt,\n        \"output_parser\": self.output_parser.__name__ if self.output_parser is not None else None,\n        \"parse_mode\": self.parse_mode,\n        \"parse_func\": self.parse_func.__name__ if self.parse_func is not None else None,\n        \"title_format\": self.title_format \n    }\n    return config\n</code></pre>"},{"location":"api/agents/#evoagentx.agents.CustomizeAgent.save_module","title":"save_module","text":"<pre><code>save_module(path: str, ignore: List[str] = [], **kwargs) -&gt; str\n</code></pre> <p>Save the customize agent's configuration to a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>File path where the configuration should be saved</p> required <code>ignore</code> <code>List[str]</code> <p>List of keys to exclude from the saved configuration</p> <code>[]</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters for the save operation</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The path where the configuration was saved</p> Source code in <code>evoagentx/agents/customize_agent.py</code> <pre><code>def save_module(self, path: str, ignore: List[str] = [], **kwargs)-&gt; str:\n    \"\"\"Save the customize agent's configuration to a JSON file.\n\n    Args:\n        path: File path where the configuration should be saved\n        ignore: List of keys to exclude from the saved configuration\n        **kwargs (Any): Additional parameters for the save operation\n\n    Returns:\n        The path where the configuration was saved\n    \"\"\"\n    config = self.get_customize_agent_info()\n\n    for ignore_key in ignore:\n        config.pop(ignore_key, None)\n\n    # Save to JSON file\n    make_parent_folder(path)\n    with open(path, 'w', encoding='utf-8') as f:\n        json.dump(config, f, indent=4, ensure_ascii=False)\n\n    return path\n</code></pre>"},{"location":"api/agents/#evoagentx.agents.AgentManager","title":"AgentManager","text":"<pre><code>AgentManager(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModule</code></p> <p>Responsible for creating and managing all Agent objects required for workflow operation.</p> <p>Attributes:</p> Name Type Description <code>storage_handler</code> <code>StorageHandler</code> <p>Used to load and save agents from/to storage.</p> <code>agents</code> <code>List[Agent]</code> <p>A list to keep track of all managed Agent instances.</p> <code>agent_states</code> <code>Dict[str, AgentState]</code> <p>A dictionary to track the state of each Agent by name.</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/agents/#evoagentx.agents.AgentManager.size","title":"size  <code>property</code>","text":"<pre><code>size\n</code></pre> <p>Get the total number of agents managed by this manager.</p>"},{"location":"api/agents/#evoagentx.agents.AgentManager.check_agents","title":"check_agents","text":"<pre><code>check_agents()\n</code></pre> <p>Validate agent list integrity and state consistency.</p> <p>Performs thorough validation of the agent manager's internal state: 1. Checks for duplicate agent names 2. Verifies that agent states exist for all agents 3. Ensures agent list and state dictionary sizes match</p> Source code in <code>evoagentx/agents/agent_manager.py</code> <pre><code>def check_agents(self):\n    \"\"\"Validate agent list integrity and state consistency.\n\n    Performs thorough validation of the agent manager's internal state:\n    1. Checks for duplicate agent names\n    2. Verifies that agent states exist for all agents\n    3. Ensures agent list and state dictionary sizes match\n    \"\"\"\n    # check that the names of self.agents should be unique\n    duplicate_agent_names = self.find_duplicate_agents(self.agents)\n    if duplicate_agent_names:\n        raise ValueError(f\"The agents should be unique. Found duplicate agent names: {duplicate_agent_names}!\")\n    # check agent states\n    if len(self.agents) != len(self.agent_states):\n        raise ValueError(f\"The lengths of self.agents ({len(self.agents)}) and self.agent_states ({len(self.agent_states)}) are different!\")\n    missing_agents = self.find_missing_agent_states()\n    if missing_agents:\n        raise ValueError(f\"The following agents' states were not found: {missing_agents}\")\n</code></pre>"},{"location":"api/agents/#evoagentx.agents.AgentManager.has_agent","title":"has_agent","text":"<pre><code>has_agent(agent_name: str) -&gt; bool\n</code></pre> <p>Check if an agent with the given name exists in the manager.</p> <p>Parameters:</p> Name Type Description Default <code>agent_name</code> <code>str</code> <p>The name of the agent to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if an agent with the given name exists, False otherwise</p> Source code in <code>evoagentx/agents/agent_manager.py</code> <pre><code>def has_agent(self, agent_name: str) -&gt; bool:\n    \"\"\"Check if an agent with the given name exists in the manager.\n\n    Args:\n        agent_name: The name of the agent to check\n\n    Returns:\n        True if an agent with the given name exists, False otherwise\n    \"\"\"\n    all_agent_names = self.list_agents()\n    return agent_name in all_agent_names\n</code></pre>"},{"location":"api/agents/#evoagentx.agents.AgentManager.load_agent","title":"load_agent","text":"<pre><code>load_agent(agent_name: str, **kwargs) -&gt; Agent\n</code></pre> <p>Load an agent from local storage through storage_handler.</p> <p>Retrieves agent data from storage and creates an Agent instance.</p> <p>Parameters:</p> Name Type Description Default <code>agent_name</code> <code>str</code> <p>The name of the agent to load</p> required <code>**kwargs</code> <code>Any</code> <p>Additional parameters for agent creation</p> <code>{}</code> <p>Returns:</p> Type Description <code>Agent</code> <p>Agent instance with data loaded from storage</p> Source code in <code>evoagentx/agents/agent_manager.py</code> <pre><code>def load_agent(self, agent_name: str, **kwargs) -&gt; Agent:\n    \"\"\"Load an agent from local storage through storage_handler.\n\n    Retrieves agent data from storage and creates an Agent instance.\n\n    Args:\n        agent_name: The name of the agent to load\n        **kwargs (Any): Additional parameters for agent creation\n\n    Returns:\n        Agent instance with data loaded from storage\n    \"\"\"\n    if not self.storage_handler:\n        raise ValueError(\"must provide ``self.storage_handler`` to use ``load_agent``\")\n    agent_data = self.storage_handler.load_agent(agent_name=agent_name)\n    agent: Agent = self.create_customize_agent(agent_data=agent_data)\n    return agent\n</code></pre>"},{"location":"api/agents/#evoagentx.agents.AgentManager.load_all_agents","title":"load_all_agents","text":"<pre><code>load_all_agents(**kwargs)\n</code></pre> <p>Load all agents from storage and add them to the manager.</p> <p>Retrieves all available agents from storage and adds them to the managed agents collection.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional parameters passed to storage handler</p> <code>{}</code> Source code in <code>evoagentx/agents/agent_manager.py</code> <pre><code>def load_all_agents(self, **kwargs):\n    \"\"\"Load all agents from storage and add them to the manager.\n\n    Retrieves all available agents from storage and adds them to the\n    managed agents collection.\n\n    Args:\n        **kwargs (Any): Additional parameters passed to storage handler\n    \"\"\"\n    pass \n</code></pre>"},{"location":"api/agents/#evoagentx.agents.AgentManager.create_customize_agent","title":"create_customize_agent","text":"<pre><code>create_customize_agent(agent_data: dict, llm_config: Optional[Union[LLMConfig, dict]] = None, **kwargs) -&gt; CustomizeAgent\n</code></pre> <p>create a customized agent from the provided <code>agent_data</code>. </p> <p>Parameters:</p> Name Type Description Default <code>agent_data</code> <code>dict</code> <p>The data used to create an Agent instance, must contain 'name', 'description' and 'prompt' keys.</p> required <code>llm_config</code> <code>Optional[LLMConfig]</code> <p>The LLM configuration to be used for the agent.  It will be used as the default LLM for agents without a <code>llm_config</code> key.  If not provided, the <code>agent_data</code> should contain a <code>llm_config</code> key.  If provided and <code>agent_data</code> contains a <code>llm_config</code> key, the <code>llm_config</code> in <code>agent_data</code> will be used.  </p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters for agent creation</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Agent</code> <code>CustomizeAgent</code> <p>the instantiated agent instance.</p> Source code in <code>evoagentx/agents/agent_manager.py</code> <pre><code>def create_customize_agent(self, agent_data: dict, llm_config: Optional[Union[LLMConfig, dict]]=None, **kwargs) -&gt; CustomizeAgent:\n    \"\"\"\n    create a customized agent from the provided `agent_data`. \n\n    Args:\n        agent_data: The data used to create an Agent instance, must contain 'name', 'description' and 'prompt' keys.\n        llm_config (Optional[LLMConfig]): The LLM configuration to be used for the agent. \n            It will be used as the default LLM for agents without a `llm_config` key. \n            If not provided, the `agent_data` should contain a `llm_config` key. \n            If provided and `agent_data` contains a `llm_config` key, the `llm_config` in `agent_data` will be used.  \n        **kwargs (Any): Additional parameters for agent creation\n\n    Returns:\n        Agent: the instantiated agent instance.\n    \"\"\"\n    agent_data = deepcopy(agent_data)\n    agent_llm_config = agent_data.get(\"llm_config\", llm_config)\n    if not agent_data.get(\"is_human\", False) and not agent_llm_config:\n        raise ValueError(\"`agent_data` should contain a `llm_config` key or `llm_config` should be provided.\")\n\n    if agent_llm_config:\n        if isinstance(agent_llm_config, dict):\n            agent_data[\"llm_config\"] = agent_llm_config\n        elif isinstance(agent_llm_config, LLMConfig):\n            agent_data[\"llm_config\"] = agent_llm_config.to_dict()\n        else:\n            raise ValueError(f\"llm_config must be a dictionary or an instance of LLMConfig. Got {type(agent_llm_config)}.\") \n\n    return CustomizeAgent.from_dict(data=agent_data)\n</code></pre>"},{"location":"api/agents/#evoagentx.agents.AgentManager.get_agent_name","title":"get_agent_name","text":"<pre><code>get_agent_name(agent: Union[str, dict, Agent]) -&gt; str\n</code></pre> <p>Extract agent name from different agent representations.</p> <p>Handles different ways to specify an agent (string name, dictionary, or Agent instance) and extracts the agent name.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Union[str, dict, Agent]</code> <p>Agent specified as a string name, dictionary with 'name' key,   or Agent instance</p> required <p>Returns:</p> Type Description <code>str</code> <p>The extracted agent name as a string</p> Source code in <code>evoagentx/agents/agent_manager.py</code> <pre><code>def get_agent_name(self, agent: Union[str, dict, Agent]) -&gt; str:\n    \"\"\"Extract agent name from different agent representations.\n\n    Handles different ways to specify an agent (string name, dictionary, or\n    Agent instance) and extracts the agent name.\n\n    Args:\n        agent: Agent specified as a string name, dictionary with 'name' key,\n              or Agent instance\n\n    Returns:\n        The extracted agent name as a string\n    \"\"\"\n    if isinstance(agent, str):\n        agent_name = agent\n    elif isinstance(agent, dict):\n        agent_name = agent[\"name\"]\n    elif isinstance(agent, Agent):\n        agent_name = agent.name\n    else:\n        raise ValueError(f\"{type(agent)} is not a supported type for ``get_agent_name``. Supported types: [str, dict, Agent].\")\n    return agent_name\n</code></pre>"},{"location":"api/agents/#evoagentx.agents.AgentManager.add_agent","title":"add_agent","text":"<pre><code>add_agent(agent: Union[str, dict, Agent], llm_config: Optional[LLMConfig] = None, **kwargs)\n</code></pre> <p>add a single agent, ignore if the agent already exists (judged by the name of an agent).</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Union[str, dict, Agent]</code> <p>The agent to be added, specified as: - String: Agent name to load from storage - Dictionary: Agent specification to create a CustomizeAgent - Agent: Existing Agent instance to add directly</p> required <code>llm_config</code> <code>Optional[LLMConfig]</code> <p>The LLM configuration to be used for the agent. Only used when the <code>agent</code> is a dictionary, used to create a CustomizeAgent. </p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters for agent creation</p> <code>{}</code> Source code in <code>evoagentx/agents/agent_manager.py</code> <pre><code>@atomic_method\ndef add_agent(self, agent: Union[str, dict, Agent], llm_config: Optional[LLMConfig]=None, **kwargs):\n    \"\"\"\n    add a single agent, ignore if the agent already exists (judged by the name of an agent).\n\n    Args:\n        agent: The agent to be added, specified as:\n            - String: Agent name to load from storage\n            - Dictionary: Agent specification to create a CustomizeAgent\n            - Agent: Existing Agent instance to add directly\n        llm_config (Optional[LLMConfig]): The LLM configuration to be used for the agent. Only used when the `agent` is a dictionary, used to create a CustomizeAgent. \n        **kwargs (Any): Additional parameters for agent creation\n    \"\"\"\n    agent_name = self.get_agent_name(agent=agent)\n    if self.has_agent(agent_name=agent_name):\n        return\n    agent_instance = self.create_agent(agent=agent, llm_config=llm_config, **kwargs)\n    self.agents.append(agent_instance)\n    self.agent_states[agent_instance.name] = AgentState.AVAILABLE\n    if agent_instance.name not in self._state_conditions:\n        self._state_conditions[agent_instance.name] = threading.Condition()\n    self.check_agents()\n</code></pre>"},{"location":"api/agents/#evoagentx.agents.AgentManager.add_agents","title":"add_agents","text":"<pre><code>add_agents(agents: List[Union[str, dict, Agent]], llm_config: Optional[LLMConfig] = None, **kwargs)\n</code></pre> <p>add several agents by using self.add_agent().</p> Source code in <code>evoagentx/agents/agent_manager.py</code> <pre><code>def add_agents(self, agents: List[Union[str, dict, Agent]], llm_config: Optional[LLMConfig]=None, **kwargs):\n    \"\"\"\n    add several agents by using self.add_agent().\n    \"\"\"\n    for agent in agents:\n        self.add_agent(agent=agent, llm_config=llm_config, **kwargs)\n</code></pre>"},{"location":"api/agents/#evoagentx.agents.AgentManager.add_agents_from_workflow","title":"add_agents_from_workflow","text":"<pre><code>add_agents_from_workflow(workflow_graph, llm_config: Optional[LLMConfig] = None, **kwargs)\n</code></pre> <p>Initialize agents from the nodes of a given WorkFlowGraph and add these agents to self.agents. </p> <p>Parameters:</p> Name Type Description Default <code>workflow_graph</code> <code>WorkFlowGraph</code> <p>The workflow graph containing nodes with agents information.</p> required <code>llm_config</code> <code>Optional[LLMConfig]</code> <p>The LLM configuration to be used for the agents.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters passed to add_agent</p> <code>{}</code> Source code in <code>evoagentx/agents/agent_manager.py</code> <pre><code>def add_agents_from_workflow(self, workflow_graph, llm_config: Optional[LLMConfig]=None, **kwargs):\n    \"\"\"\n    Initialize agents from the nodes of a given WorkFlowGraph and add these agents to self.agents. \n\n    Args:\n        workflow_graph (WorkFlowGraph): The workflow graph containing nodes with agents information.\n        llm_config (Optional[LLMConfig]): The LLM configuration to be used for the agents.\n        **kwargs (Any): Additional parameters passed to add_agent\n    \"\"\"\n    from ..workflow.workflow_graph import WorkFlowGraph\n    if not isinstance(workflow_graph, WorkFlowGraph):\n        raise TypeError(\"workflow_graph must be an instance of WorkFlowGraph\")\n    for node in workflow_graph.nodes:\n        if node.agents:\n            for agent in node.agents:\n                self.add_agent(agent=agent, llm_config=llm_config, **kwargs)\n</code></pre>"},{"location":"api/agents/#evoagentx.agents.AgentManager.update_agents_from_workflow","title":"update_agents_from_workflow","text":"<pre><code>update_agents_from_workflow(workflow_graph, llm_config: Optional[LLMConfig] = None, **kwargs)\n</code></pre> <p>Update agents from a given WorkFlowGraph.</p> <p>Parameters:</p> Name Type Description Default <code>workflow_graph</code> <code>WorkFlowGraph</code> <p>The workflow graph containing nodes with agents information.</p> required <code>llm_config</code> <code>Optional[LLMConfig]</code> <p>The LLM configuration to be used for the agents.</p> <code>None</code> <code>**kwargs</code> <p>Additional parameters passed to update_agent</p> <code>{}</code> Source code in <code>evoagentx/agents/agent_manager.py</code> <pre><code>def update_agents_from_workflow(self, workflow_graph, llm_config: Optional[LLMConfig]=None, **kwargs):\n    \"\"\"\n    Update agents from a given WorkFlowGraph.\n\n    Args:\n        workflow_graph (WorkFlowGraph): The workflow graph containing nodes with agents information.\n        llm_config (Optional[LLMConfig]): The LLM configuration to be used for the agents.\n        **kwargs: Additional parameters passed to update_agent\n    \"\"\"\n    from ..workflow.workflow_graph import WorkFlowGraph\n    if not isinstance(workflow_graph, WorkFlowGraph):\n        raise TypeError(\"workflow_graph must be an instance of WorkFlowGraph\")\n    for node in workflow_graph.nodes:\n        if node.agents:\n            for agent in node.agents:\n                agent_name = self.get_agent_name(agent=agent)\n                if self.has_agent(agent_name=agent_name):\n                    # use the llm_config of the existing agent\n                    agent_llm_config = self.get_agent(agent_name).llm_config\n                    self.update_agent(agent=agent, llm_config=agent_llm_config, **kwargs)\n                else:\n                    self.add_agent(agent=agent, llm_config=llm_config, **kwargs)\n</code></pre>"},{"location":"api/agents/#evoagentx.agents.AgentManager.get_agent","title":"get_agent","text":"<pre><code>get_agent(agent_name: str, **kwargs) -&gt; Agent\n</code></pre> <p>Retrieve an agent by its name from managed agents.</p> <p>Searches the list of managed agents for an agent with the specified name.</p> <p>Parameters:</p> Name Type Description Default <code>agent_name</code> <code>str</code> <p>The name of the agent to retrieve</p> required <code>**kwargs</code> <code>Any</code> <p>Additional parameters (unused)</p> <code>{}</code> <p>Returns:</p> Type Description <code>Agent</code> <p>The Agent instance with the specified name</p> Source code in <code>evoagentx/agents/agent_manager.py</code> <pre><code>def get_agent(self, agent_name: str, **kwargs) -&gt; Agent:\n    \"\"\"Retrieve an agent by its name from managed agents.\n\n    Searches the list of managed agents for an agent with the specified name.\n\n    Args:\n        agent_name: The name of the agent to retrieve\n        **kwargs (Any): Additional parameters (unused)\n\n    Returns:\n        The Agent instance with the specified name\n    \"\"\"\n    for agent in self.agents:\n        if agent.name == agent_name:\n            return agent\n    raise ValueError(f\"Agent ``{agent_name}`` does not exists!\")\n</code></pre>"},{"location":"api/agents/#evoagentx.agents.AgentManager.update_agent","title":"update_agent","text":"<pre><code>update_agent(agent: Union[dict, Agent], llm_config: Optional[LLMConfig] = None, **kwargs)\n</code></pre> <p>Update an agent in the manager.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Union[dict, Agent]</code> <p>The agent to be updated, specified as: - Dictionary: Agent specification to update a CustomizeAgent - Agent: Existing Agent instance to update</p> required <code>llm_config</code> <code>Optional[LLMConfig]</code> <p>The LLM configuration to be used for the agent.</p> <code>None</code> Source code in <code>evoagentx/agents/agent_manager.py</code> <pre><code>def update_agent(self, agent: Union[dict, Agent], llm_config: Optional[LLMConfig]=None, **kwargs):\n    \"\"\"\n    Update an agent in the manager.\n\n    Args:\n        agent: The agent to be updated, specified as:\n            - Dictionary: Agent specification to update a CustomizeAgent\n            - Agent: Existing Agent instance to update\n        llm_config (Optional[LLMConfig]): The LLM configuration to be used for the agent.\n    \"\"\"\n    agent_name = self.get_agent_name(agent=agent)\n    self.remove_agent(agent_name=agent_name)\n    self.add_agent(agent=agent, llm_config=llm_config, **kwargs)\n</code></pre>"},{"location":"api/agents/#evoagentx.agents.AgentManager.remove_agent","title":"remove_agent","text":"<pre><code>remove_agent(agent_name: str, remove_from_storage: bool = False, **kwargs)\n</code></pre> <p>Remove an agent from the manager and optionally from storage.</p> <p>Parameters:</p> Name Type Description Default <code>agent_name</code> <code>str</code> <p>The name of the agent to remove</p> required <code>remove_from_storage</code> <code>bool</code> <p>If True, also remove the agent from storage</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters passed to storage_handler.remove_agent</p> <code>{}</code> Source code in <code>evoagentx/agents/agent_manager.py</code> <pre><code>@atomic_method\ndef remove_agent(self, agent_name: str, remove_from_storage: bool=False, **kwargs):\n    \"\"\"\n    Remove an agent from the manager and optionally from storage.\n\n    Args:\n        agent_name: The name of the agent to remove\n        remove_from_storage: If True, also remove the agent from storage\n        **kwargs (Any): Additional parameters passed to storage_handler.remove_agent\n    \"\"\"\n    self.agents = [agent for agent in self.agents if agent.name != agent_name]\n    self.agent_states.pop(agent_name, None)\n    self._state_conditions.pop(agent_name, None) \n    if remove_from_storage:\n        self.storage_handler.remove_agent(agent_name=agent_name, **kwargs)\n    self.check_agents()\n</code></pre>"},{"location":"api/agents/#evoagentx.agents.AgentManager.get_agent_state","title":"get_agent_state","text":"<pre><code>get_agent_state(agent_name: str) -&gt; AgentState\n</code></pre> <p>Get the state of a specific agent by its name.</p> <p>Parameters:</p> Name Type Description Default <code>agent_name</code> <code>str</code> <p>The name of the agent.</p> required <p>Returns:</p> Name Type Description <code>AgentState</code> <code>AgentState</code> <p>The current state of the agent.</p> Source code in <code>evoagentx/agents/agent_manager.py</code> <pre><code>def get_agent_state(self, agent_name: str) -&gt; AgentState:\n    \"\"\"\n    Get the state of a specific agent by its name.\n\n    Args:\n        agent_name: The name of the agent.\n\n    Returns:\n        AgentState: The current state of the agent.\n    \"\"\"\n    return self.agent_states[agent_name]\n</code></pre>"},{"location":"api/agents/#evoagentx.agents.AgentManager.set_agent_state","title":"set_agent_state","text":"<pre><code>set_agent_state(agent_name: str, new_state: AgentState) -&gt; bool\n</code></pre> <p>Changes an agent's state and notifies any threads waiting on that agent's state. Thread-safe operation for coordinating multi-threaded agent execution.</p> <p>Parameters:</p> Name Type Description Default <code>agent_name</code> <code>str</code> <p>The name of the agent</p> required <code>new_state</code> <code>AgentState</code> <p>The new state to set</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the state was updated successfully, False otherwise</p> Source code in <code>evoagentx/agents/agent_manager.py</code> <pre><code>@atomic_method\ndef set_agent_state(self, agent_name: str, new_state: AgentState) -&gt; bool:\n    \"\"\"\n    Changes an agent's state and notifies any threads waiting on that agent's state.\n    Thread-safe operation for coordinating multi-threaded agent execution.\n\n    Args:\n        agent_name: The name of the agent\n        new_state: The new state to set\n\n    Returns:\n        True if the state was updated successfully, False otherwise\n    \"\"\"\n\n    # if agent_name in self.agent_states and isinstance(new_state, AgentState):\n    #     # self.agent_states[agent_name] = new_state\n    #     with self._state_conditions[agent_name]:\n    #         self.agent_states[agent_name] = new_state\n    #         self._state_conditions[agent_name].notify_all()\n    #     self.check_agents()\n    #     return True\n    # else:\n    #     return False\n    if agent_name in self.agent_states and isinstance(new_state, AgentState):\n        if agent_name not in self._state_conditions:\n            self._state_conditions[agent_name] = threading.Condition()\n        with self._state_conditions[agent_name]:\n            self.agent_states[agent_name] = new_state\n            self._state_conditions[agent_name].notify_all()\n        return True\n    return False\n</code></pre>"},{"location":"api/agents/#evoagentx.agents.AgentManager.get_all_agent_states","title":"get_all_agent_states","text":"<pre><code>get_all_agent_states() -&gt; Dict[str, AgentState]\n</code></pre> <p>Get the states of all managed agents.</p> <p>Returns:</p> Type Description <code>Dict[str, AgentState]</code> <p>Dict[str, AgentState]: A dictionary mapping agent names to their states.</p> Source code in <code>evoagentx/agents/agent_manager.py</code> <pre><code>def get_all_agent_states(self) -&gt; Dict[str, AgentState]:\n    \"\"\"Get the states of all managed agents.\n\n    Returns:\n        Dict[str, AgentState]: A dictionary mapping agent names to their states.\n    \"\"\"\n    return self.agent_states\n</code></pre>"},{"location":"api/agents/#evoagentx.agents.AgentManager.save_all_agents","title":"save_all_agents","text":"<pre><code>save_all_agents(**kwargs)\n</code></pre> <p>Save all managed agents to persistent storage.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional parameters passed to the storage handler</p> <code>{}</code> Source code in <code>evoagentx/agents/agent_manager.py</code> <pre><code>@atomic_method\ndef save_all_agents(self, **kwargs):\n    \"\"\"Save all managed agents to persistent storage.\n\n    Args:\n        **kwargs (Any): Additional parameters passed to the storage handler\n    \"\"\"\n    pass \n</code></pre>"},{"location":"api/agents/#evoagentx.agents.AgentManager.clear_agents","title":"clear_agents","text":"<pre><code>clear_agents()\n</code></pre> <p>Remove all agents from the manager.</p> Source code in <code>evoagentx/agents/agent_manager.py</code> <pre><code>@atomic_method\ndef clear_agents(self):\n    \"\"\"\n    Remove all agents from the manager.\n    \"\"\"\n    self.agents = [] \n    self.agent_states = {}\n    self._state_conditions = {}\n    self.check_agents()\n</code></pre>"},{"location":"api/agents/#evoagentx.agents.AgentManager.wait_for_agent_available","title":"wait_for_agent_available","text":"<pre><code>wait_for_agent_available(agent_name: str, timeout: Optional[float] = None) -&gt; bool\n</code></pre> <p>Wait for an agent to be available.</p> <p>Parameters:</p> Name Type Description Default <code>agent_name</code> <code>str</code> <p>The name of the agent to wait for</p> required <code>timeout</code> <code>Optional[float]</code> <p>Maximum time to wait in seconds, or None to wait indefinitely</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the agent became available, False if timed out</p> Source code in <code>evoagentx/agents/agent_manager.py</code> <pre><code>def wait_for_agent_available(self, agent_name: str, timeout: Optional[float] = None) -&gt; bool:\n    \"\"\"Wait for an agent to be available.\n\n    Args:\n        agent_name: The name of the agent to wait for\n        timeout: Maximum time to wait in seconds, or None to wait indefinitely\n\n    Returns:\n        True if the agent became available, False if timed out\n    \"\"\"\n    if agent_name not in self._state_conditions:\n        self._state_conditions[agent_name] = threading.Condition()\n    condition = self._state_conditions[agent_name]\n\n    with condition:\n        return condition.wait_for(\n            lambda: self.agent_states.get(agent_name) == AgentState.AVAILABLE,\n            timeout=timeout\n        )\n</code></pre>"},{"location":"api/benchmark/","title":"\ud83e\uddea Benchmark","text":""},{"location":"api/benchmark/#evoagentx.benchmark","title":"evoagentx.benchmark","text":""},{"location":"api/benchmark/#evoagentx.benchmark.NQ","title":"NQ","text":"<pre><code>NQ(path: str = None, mode: str = 'all', **kwargs)\n</code></pre> <p>               Bases: <code>Benchmark</code></p> <p>Benchmark class for evaluating question answering on Natural Questions dataset.</p> <p>Natural Questions (NQ) is a dataset for open-domain question answering, containing real questions from Google Search and answers from Wikipedia. This class handles loading the dataset, evaluating answers, and computing metrics like exact match and F1 score.</p> <p>Each NQ example has the following structure: {     \"id\": str,      \"question\": str,      \"answers\": List[str] }</p> <p>The benchmark evaluates answers using exact match, F1 score, and accuracy metrics.</p> Source code in <code>evoagentx/benchmark/nq.py</code> <pre><code>def __init__(self, path: str = None, mode: str = \"all\", **kwargs):\n    path = os.path.expanduser(path or \"~/.evoagentx/data/nq\")\n    super().__init__(name=type(self).__name__, path=path, mode=mode, **kwargs)\n</code></pre>"},{"location":"api/benchmark/#evoagentx.benchmark.HotPotQA","title":"HotPotQA","text":"<pre><code>HotPotQA(path: str = None, mode: str = 'all', **kwargs)\n</code></pre> <p>               Bases: <code>Benchmark</code></p> <p>Benchmark class for evaluating multi-hop question answering on HotPotQA dataset.</p> <p>Each HotPotQA example has the following structure: {     \"_id\": str,      \"question\": str,      \"answer\": str,      \"context\": [[\"context_title\", [\"context_sentence\", \"another_sentence\"]]],     \"supporting_facts\": [[\"supporting_title\", supporting_sentence_index]],     \"type\": str,     \"level\": str }</p> <p>The benchmark evaluates answers using exact match, F1 score, and accuracy metrics.</p> Source code in <code>evoagentx/benchmark/hotpotqa.py</code> <pre><code>def __init__(self, path: str = None, mode: str = \"all\", **kwargs):\n    path = os.path.expanduser(path or \"~/.evoagentx/data/hotpotqa\")\n    super().__init__(name=type(self).__name__, path=path, mode=mode, **kwargs)\n</code></pre>"},{"location":"api/benchmark/#evoagentx.benchmark.AFlowHotPotQA","title":"AFlowHotPotQA","text":"<pre><code>AFlowHotPotQA(path: str = None, mode: str = 'all', **kwargs)\n</code></pre> <p>               Bases: <code>HotPotQA</code></p> <p>AFlow-specific implementation of HotPotQA benchmark.</p> Source code in <code>evoagentx/benchmark/hotpotqa.py</code> <pre><code>def __init__(self, path: str = None, mode: str = \"all\", **kwargs):\n    path = os.path.expanduser(path or \"~/.evoagentx/data/hotpotqa\")\n    super().__init__(name=type(self).__name__, path=path, mode=mode, **kwargs)\n</code></pre>"},{"location":"api/benchmark/#evoagentx.benchmark.GSM8K","title":"GSM8K","text":"<pre><code>GSM8K(path: str = None, mode: str = 'all', **kwargs)\n</code></pre> <p>               Bases: <code>Benchmark</code></p> <p>Benchmark class for evaluating math reasoning on GSM8K dataset.</p> <p>GSM8K (Grade School Math 8K) is a dataset of math word problems that test a model's ability to solve grade school level math problems requiring multi-step reasoning. This class handles loading the dataset, evaluating solutions, and computing metrics based on answer accuracy.</p> <p>Each GSM8K example has the following structure: {     \"id\": \"test-1\",      \"question\": \"the question\",      \"answer\": \"the answer\" }</p> <p>The benchmark evaluates answers by extracting the final numerical value and comparing it to the ground truth answer.</p> Source code in <code>evoagentx/benchmark/gsm8k.py</code> <pre><code>def __init__(self, path: str = None, mode: str = \"all\", **kwargs):\n    path = os.path.expanduser(path or \"~/.evoagentx/data/gsm8k\")\n    super().__init__(name=type(self).__name__, path=path, mode=mode, **kwargs)\n</code></pre>"},{"location":"api/benchmark/#evoagentx.benchmark.GSM8K.extract_last_number","title":"extract_last_number","text":"<pre><code>extract_last_number(text: str) -&gt; float\n</code></pre> <p>Extract the last number from a text.</p> Source code in <code>evoagentx/benchmark/gsm8k.py</code> <pre><code>def extract_last_number(self, text: str) -&gt; float:\n    \"\"\"\n    Extract the last number from a text.\n    \"\"\"\n    matches = regex.findall(r\"[-+]?\\d+(?:,\\d{3})*(?:\\.\\d+)?|\\d+\\.\\d+\", str(text))\n    if matches:\n        last_number = matches[-1].replace(\",\", \"\").strip()\n        try:\n            last_number = float(last_number)\n            return last_number\n        except ValueError:\n            return None\n    return None\n</code></pre>"},{"location":"api/benchmark/#evoagentx.benchmark.AFlowGSM8K","title":"AFlowGSM8K","text":"<pre><code>AFlowGSM8K(path: str = None, mode: str = 'all', **kwargs)\n</code></pre> <p>               Bases: <code>GSM8K</code></p> <p>AFlow-specific implementation of GSM8K benchmark.</p> <p>This class extends the GSM8K benchmark with features specific to the AFlow framework, including loading from AFlow-formatted data files and supporting asynchronous evaluation for workflows.</p> <p>Attributes:</p> Name Type Description <code>path</code> <p>Path to the directory containing AFlow-formatted GSM8K files.</p> <code>mode</code> <p>Data loading mode (\"train\", \"dev\", \"test\", or \"all\").</p> <code>_train_data</code> <code>Optional[List[dict]]</code> <p>Training dataset loaded from AFlow format.</p> <code>_dev_data</code> <code>Optional[List[dict]]</code> <p>Development dataset loaded from AFlow format.</p> <code>_test_data</code> <code>Optional[List[dict]]</code> <p>Test dataset loaded from AFlow format.</p> Source code in <code>evoagentx/benchmark/gsm8k.py</code> <pre><code>def __init__(self, path: str = None, mode: str = \"all\", **kwargs):\n    path = os.path.expanduser(path or \"~/.evoagentx/data/aflow/gsm8k\")\n    super().__init__(path=path, mode=mode, **kwargs)\n</code></pre>"},{"location":"api/benchmark/#evoagentx.benchmark.MBPP","title":"MBPP","text":"<pre><code>MBPP(path: str = None, mode: str = 'all', timeout: int = 60, k: Union[int, list] = 1, **kwargs)\n</code></pre> <p>               Bases: <code>CodingBenchmark</code></p> <p>Benchmark class for evaluating code generation on the MBPP dataset.</p> <p>MBPP (Mostly Basic Python Programming) is a collection of Python programming    problems designed to test a model's ability to generate functionally correct    code from natural language descriptions. This class handles loading the dataset,    evaluating solutions, and computing metrics such as pass@k.</p> <p>The original MBPP format is transformed to be compatible with the HumanEval   benchmark format, allowing for consistent evaluation infrastructure.</p> <p>Each MBPP example has the following structure:   {       \"task_id\" (int): 2,        \"prompt\" (str): \"Write a function to find the shared elements from the given two lists.\",       \"code\" (str): \"def similar_elements(test_tup1, test_tup2): res = tuple(set(test_tup1) &amp; set(test_tup2)) return (res) \",        \"test_imports\": []        \"test_list\" (List[str]): ['assert set(similar_elements((3, 4, 5, 6),(5, 7, 4, 10))) == set((4, 5))', 'assert set(similar_elements((1, 2, 3, 4),(5, 4, 3, 7))) == set((3, 4))', 'assert set(similar_elements((11, 12, 14, 13),(17, 15, 14, 13))) == set((13, 14))']   }</p> <p>Attributes:       k: An integer or list of integers specifying which pass@k metrics to compute</p> Source code in <code>evoagentx/benchmark/mbpp.py</code> <pre><code>def __init__(self, path: str = None, mode: str = \"all\", timeout: int = 60, k: Union[int, list] = 1,**kwargs):\n    path = os.path.expanduser(path or \"~/.evoagentx/data/mbpp\")\n    self.k = k \n    super().__init__(name=type(self).__name__, path=path, mode=mode, timeout=timeout, **kwargs)\n</code></pre>"},{"location":"api/benchmark/#evoagentx.benchmark.MBPP.evaluate","title":"evaluate","text":"<pre><code>evaluate(prediction: Any, label: Any) -&gt; dict\n</code></pre> <p>Evaluate the solution code.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>str | List[str]</code> <p>The solution code(s).</p> required <code>label</code> <code>dict | List[dict]</code> <p>The unit test code(s).</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The evaluation metrics (pass@k).</p> Source code in <code>evoagentx/benchmark/mbpp.py</code> <pre><code>def evaluate(self, prediction: Any, label: Any) -&gt; dict:\n    \"\"\"\n    Evaluate the solution code.\n\n    Args:\n        prediction (str | List[str]): The solution code(s).\n        label (dict | List[dict]): The unit test code(s).\n\n    Returns:\n        dict: The evaluation metrics (pass@k).\n    \"\"\"\n    prediction, label = self._check_evaluation_inputs(prediction, label)\n\n    results = []\n    for solution in prediction:\n        solution_states = []\n        for label_data in label:\n            task_id = label_data[\"task_id\"]\n            prompt = self.get_example_by_id(task_id)[\"prompt\"]\n            unit_test = label_data[\"test\"]\n            entry_point = label_data[\"entry_point\"]\n            state, message = self.check_solution(\n                task_id=task_id, \n                solution=prompt + \"\\n\" + solution,\n                test=unit_test, \n                entry_point=entry_point\n            )\n            if state != self.SUCCESS:\n                break \n            solution_states.append(state)\n        results.append(len(solution_states)==len(label) and all(state==self.SUCCESS for state in solution_states))\n\n    k_list = [self.k] if isinstance(self.k, int) else self.k\n    pass_at_k = self.compute_pass_at_k(results, k_list)\n\n    return pass_at_k\n</code></pre>"},{"location":"api/benchmark/#evoagentx.benchmark.AFlowMBPP","title":"AFlowMBPP","text":"<pre><code>AFlowMBPP(path: str = None, mode: str = 'all', timeout: int = 60, k: Union[int, list] = 1, **kwargs)\n</code></pre> <p>               Bases: <code>MBPP</code></p> <p>AFlow-specific implementation of MBPP benchmark.</p> Source code in <code>evoagentx/benchmark/mbpp.py</code> <pre><code>def __init__(self, path: str = None, mode: str = \"all\", timeout: int = 60, k: Union[int, list] = 1,**kwargs):\n    path = os.path.expanduser(path or \"~/.evoagentx/data/aflow/mbpp\")\n    super().__init__(path=path, mode=mode, timeout=timeout, k=k, **kwargs)\n</code></pre>"},{"location":"api/benchmark/#evoagentx.benchmark.AFlowMBPP.evaluate","title":"evaluate","text":"<pre><code>evaluate(prediction: Any, label: Any) -&gt; dict\n</code></pre> <p>Evaluate the solution code.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>str | List[str]</code> <p>The solution code(s).</p> required <code>label</code> <code>dict | List[dict]</code> <p>The unit test code(s).</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The evaluation metrics (pass@k).</p> Source code in <code>evoagentx/benchmark/mbpp.py</code> <pre><code>def evaluate(self, prediction: Any, label: Any) -&gt; dict:\n    \"\"\"\n    Evaluate the solution code.\n\n    Args:\n        prediction (str | List[str]): The solution code(s).\n        label (dict | List[dict]): The unit test code(s).\n\n    Returns:\n        dict: The evaluation metrics (pass@k).\n    \"\"\"\n    prediction, label = self._check_evaluation_inputs(prediction, label)\n\n    results = []\n    for solution in prediction:\n        solution_states = []\n        for label_data in label:\n            task_id = label_data[\"task_id\"]\n            prompt = self.get_example_by_id(task_id)[\"prompt\"]\n            unit_test = label_data[\"test\"]\n            entry_point = label_data[\"entry_point\"]\n            state, message = self.check_solution(\n                task_id=task_id, \n                solution=prompt + \"\\n\" + solution,\n                test=unit_test, \n                entry_point=entry_point,\n                use_entrypoint_as_input=False\n            )\n            if state != self.SUCCESS:\n                break \n            solution_states.append(state)\n        results.append(len(solution_states)==len(label) and all(state==self.SUCCESS for state in solution_states))\n\n    k_list = [self.k] if isinstance(self.k, int) else self.k\n    pass_at_k = self.compute_pass_at_k(results, k_list)\n\n    return pass_at_k\n</code></pre>"},{"location":"api/benchmark/#evoagentx.benchmark.MATH","title":"MATH","text":"<pre><code>MATH(path: str = None, mode: str = 'all', **kwargs)\n</code></pre> <p>               Bases: <code>Benchmark</code></p> <p>Benchmark class for evaluating mathematical reasoning on the MATH dataset.</p> <p>MATH is a dataset of challenging competition mathematics problems, spanning various difficulty levels and subject areas. This class handles loading the dataset, extracting answers, evaluating solutions through symbolic and numerical comparisons, and computing accuracy metrics.</p> <p>The dataset includes problems across 7 subject areas (Algebra, Geometry, etc.) and 5 difficulty levels. Each problem contains LaTeX-formatted questions and solutions.</p> <p>Each MATH example has the following structure: {     \"id\": \"test-1\",      \"problem\": \"the problem\",      \"solution\": \"the solution\",     \"level\": \"Level 1\", # \"Level 1\", \"Level 2\", \"Level 3\", \"Level 4\", \"Level 5\", \"Level ?\"     \"type\": \"Algebra\", # 'Geometry', 'Algebra', 'Intermediate Algebra', 'Counting &amp; Probability', 'Precalculus', 'Number Theory', 'Prealgebra' }</p> <p>The benchmark evaluates answers using symbolic math equality checking and numerical approximation to handle equivalent mathematical expressions.</p> Source code in <code>evoagentx/benchmark/math.py</code> <pre><code>def __init__(self, path: str = None, mode: str = \"all\", **kwargs):\n    path = os.path.expanduser(path or \"~/.evoagentx/data/math\")\n    super().__init__(name=type(self).__name__, path=path, mode=mode, **kwargs)\n</code></pre>"},{"location":"api/benchmark/#evoagentx.benchmark.HumanEval","title":"HumanEval","text":"<pre><code>HumanEval(path: str = None, mode: str = 'all', timeout: int = 60, k: Union[int, list] = 1, **kwargs)\n</code></pre> <p>               Bases: <code>CodingBenchmark</code></p> <p>Benchmark class for evaluating code generation on HumanEval.</p> <pre><code>HumanEval is a collection of Python programming problems designed to test\na model's ability to generate functionally correct code from natural language\ndescriptions. This class handles loading the dataset, evaluating solutions,\nand computing metrics such as pass@k.\n\nEach HumanEval example has the following structure:\n{\n    \"task_id\": \"HumanEval/0\", \n    \"prompt\": \"from typing import List\n</code></pre> <p>def func_name(args, *kwargs) -&gt; return_type     \"function description\"</p> <p>\",          \"entry_point\": \"func_name\",         \"canonical_solution\": \"canonical solution (code)\",         \"test\": \"METADATA = {xxx}</p> <p>def check(candidate):  assert candidate(inputs) == output \"     }</p> <pre><code>Attributes:\n    k: An integer or list of integers specifying which pass@k metrics to compute\n</code></pre> Source code in <code>evoagentx/benchmark/humaneval.py</code> <pre><code>def __init__(self, path: str = None, mode: str = \"all\", timeout: int = 60, k: Union[int, list] = 1, **kwargs):\n    path = os.path.expanduser(path or \"~/.evoagentx/data/humaneval\")\n    self.k = k \n    super().__init__(name=type(self).__name__, path=path, mode=mode, timeout=timeout, **kwargs)\n</code></pre>"},{"location":"api/benchmark/#evoagentx.benchmark.HumanEval.handle_special_cases","title":"handle_special_cases","text":"<pre><code>handle_special_cases(task_id: str, solution: str, test: str) -&gt; bool\n</code></pre> <p>Handle special cases for HumanEval.</p> Source code in <code>evoagentx/benchmark/humaneval.py</code> <pre><code>def handle_special_cases(self, task_id: str, solution: str, test: str) -&gt; bool:\n    \"\"\"\n    Handle special cases for HumanEval.\n    \"\"\"\n    if task_id == \"HumanEval/50\":\n        solution = (\n            '\\n\\ndef encode_shift(s: str):\\n    \"\"\"\\n    returns encoded string by shifting every character by 5 in the alphabet.\\n    \"\"\"\\n    return \"\".join([chr(((ord(ch) + 5 - ord(\"a\")) % 26) + ord(\"a\")) for ch in s])\\n\\n\\n'\n            + solution\n        )\n        return solution, test \n\n    return super().handle_special_cases(task_id=task_id, solution=solution, test=test)\n</code></pre>"},{"location":"api/benchmark/#evoagentx.benchmark.HumanEval.evaluate","title":"evaluate","text":"<pre><code>evaluate(prediction: Any, label: Any) -&gt; dict\n</code></pre> <p>Evaluate the solution code.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>str | List[str]</code> <p>The solution code(s).</p> required <code>label</code> <code>dict | List[dict]</code> <p>The unit test code(s).</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The evaluation metrics (pass@k).</p> Source code in <code>evoagentx/benchmark/humaneval.py</code> <pre><code>def evaluate(self, prediction: Any, label: Any) -&gt; dict:\n    \"\"\"\n    Evaluate the solution code.\n\n    Args:\n        prediction (str | List[str]): The solution code(s).\n        label (dict | List[dict]): The unit test code(s).\n\n    Returns:\n        dict: The evaluation metrics (pass@k).\n    \"\"\"\n    prediction, label = self._check_evaluation_inputs(prediction, label)\n\n    results = []\n    for solution in prediction:\n        solution_states = []\n        for label_data in label:\n            task_id = label_data[\"task_id\"]\n            prompt = self.get_example_by_id(task_id)[\"prompt\"]\n            unit_test = label_data[\"test\"]\n            entry_point = label_data[\"entry_point\"]\n            state, message = self.check_solution(\n                task_id=task_id, \n                solution=prompt + solution,\n                test=unit_test, \n                entry_point=entry_point\n            )\n            if state != self.SUCCESS:\n                break \n            solution_states.append(state)\n        results.append(len(solution_states)==len(label) and all(state==self.SUCCESS for state in solution_states))\n\n    k_list = [self.k] if isinstance(self.k, int) else self.k\n    pass_at_k = self.compute_pass_at_k(results, k_list)\n\n    return pass_at_k\n</code></pre>"},{"location":"api/benchmark/#evoagentx.benchmark.AFlowHumanEval","title":"AFlowHumanEval","text":"<pre><code>AFlowHumanEval(path: str = None, mode: str = 'all', timeout: int = 60, k: Union[int, list] = 1, **kwargs)\n</code></pre> <p>               Bases: <code>HumanEval</code></p> <p>AFlow-specific implementation of HumanEval benchmark.</p> Source code in <code>evoagentx/benchmark/humaneval.py</code> <pre><code>def __init__(self, path: str = None, mode: str = \"all\", timeout: int = 60, k: Union[int, list] = 1, **kwargs):\n    path = os.path.expanduser(path or \"~/.evoagentx/data/aflow/humaneval\")\n    super().__init__(path=path, mode=mode, timeout=timeout, k=k, **kwargs)\n</code></pre>"},{"location":"api/benchmark/#evoagentx.benchmark.AFlowHumanEval.extract_test_cases_with_entry_point","title":"extract_test_cases_with_entry_point","text":"<pre><code>extract_test_cases_with_entry_point(entry_point: str)\n</code></pre> <p>Extract test cases with the given entry point.</p> Source code in <code>evoagentx/benchmark/humaneval.py</code> <pre><code>def extract_test_cases_with_entry_point(self, entry_point: str):\n    \"\"\"\n    Extract test cases with the given entry point.\n    \"\"\"\n\n    hardcoded_cases = {\n        \"find_zero\": \"\",\n        \"decode_cyclic\": \"\",\n        \"decode_shift\": \"\",\n        \"by_length\": \"\",\n        \"add\": \"\",\n        \"triangle_area\": \"\",\n        \"correct_bracketing\": \"\",\n        \"solve\": \"\",\n        \"sum_squares\": \"\",\n        \"starts_one_ends\": \"\",\n    }\n    if entry_point in hardcoded_cases:\n        return hardcoded_cases[entry_point]\n\n    for case in self._test_cases:\n        if case[\"entry_point\"] == entry_point:\n            return case[\"test\"]\n\n    return None\n</code></pre>"},{"location":"api/benchmark/#evoagentx.benchmark.LiveCodeBench","title":"LiveCodeBench","text":"<pre><code>LiveCodeBench(path: str = None, mode: str = 'all', timeout: int = 60, k: Union[int, list] = 1, num_process: int = 6, scenario: str = 'code_generation', version: str = 'release_latest', start_date: str = None, end_date: str = None, use_cot_for_execution: bool = False, **kwargs)\n</code></pre> <p>               Bases: <code>CodingBenchmark</code></p> <p>Benchmark class for evaluating LLM capabilities on real-world programming tasks.</p> <p>LiveCodeBench provides a framework for evaluating different scenarios of code-related tasks: 1. Code Generation: generating code from problem descriptions 2. Test Output Prediction: predicting test outputs given test code 3. Code Execution: generating code that executes correctly</p> <p>The benchmark supports different evaluation modes, metrics, and can be customized with various parameters like timeouts, sample dates, and processing options.</p> <p>Attributes:</p> Name Type Description <code>k</code> <p>An integer or list of integers specifying which pass@k metrics to compute</p> <code>version</code> <p>Release version of the dataset to use</p> <code>num_process</code> <p>Number of processes to use for evaluation</p> <code>start_date</code> <p>Filter problems to those after this date</p> <code>end_date</code> <p>Filter problems to those before this date</p> <code>scenario</code> <p>Type of programming task to evaluate (\"code_generation\",        \"test_output_prediction\", or \"code_execution\")</p> <code>use_cot_for_execution</code> <p>Whether to use chain-of-thought processing for code execution</p> Source code in <code>evoagentx/benchmark/livecodebench.py</code> <pre><code>def __init__(\n    self, \n    path: str = None, \n    mode: str = \"all\", \n    timeout: int = 60, \n    k: Union[int, list] = 1, \n    num_process: int = 6, \n    scenario: str = \"code_generation\", \n    version: str = \"release_latest\", \n    start_date: str = None, \n    end_date: str = None, \n    use_cot_for_execution: bool = False, \n    **kwargs\n):\n    path = os.path.expanduser(path or \"~/.evoagentx/data/livecodebench\")\n    self.k = k \n    self.version = version\n    self.num_process = num_process\n    self.start_date = start_date\n    self.end_date = end_date\n    self.scenario = scenario \n    self.use_cot_for_execution = use_cot_for_execution\n    assert scenario in VALID_SCENARIO, f\"Invalid scenario: {scenario}. Available choices: {VALID_SCENARIO}.\" \n    super().__init__(name=type(self).__name__, path=path, mode=mode, timeout=timeout, **kwargs)\n</code></pre>"},{"location":"api/benchmark/#evoagentx.benchmark.LiveCodeBench.evaluate","title":"evaluate","text":"<pre><code>evaluate(prediction: Any, label: Any) -&gt; dict\n</code></pre> <p>Evaluate the solution code.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>str | List[str]</code> <p>The solution code(s).</p> required <code>label</code> <code>dict | List[dict]</code> <p>The test cases and expected outputs. </p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The evaluation metrics (pass@k).</p> Source code in <code>evoagentx/benchmark/livecodebench.py</code> <pre><code>def evaluate(self, prediction: Any, label: Any) -&gt; dict:\n    \"\"\"\n    Evaluate the solution code.\n\n    Args:\n        prediction (str | List[str]): The solution code(s).\n        label (dict | List[dict]): The test cases and expected outputs. \n\n    Returns:\n        dict: The evaluation metrics (pass@k).\n    \"\"\"\n    prediction, label = self._check_evaluation_inputs(prediction, label)\n    k_list = [self.k] if isinstance(self.k, int) else self.k\n\n    if self.scenario == \"code_generation\":\n        solutions: List[str] = [extract_code_blocks(pred)[0] for pred in prediction]\n        metrics, results, metadatas = codegen_metrics(\n            samples_list=label, # label is already a list \n            generations_list=[solutions], # for a single example. \n            k_list=k_list, \n            num_process_evaluate=self.num_process,\n            timeout=self.timeout\n        )\n\n    elif self.scenario == \"test_output_prediction\":\n        pred_outputs = [extract_test_output_code(pred) for pred in prediction]\n        metrics, results = test_output_metrics(\n            samples=label, \n            generations=[pred_outputs], \n            k_list=k_list, \n        )\n    elif self.scenario == \"code_execution\":\n        pred_outputs = [extract_execution_code(pred, self.use_cot_for_execution) for pred in prediction]\n        metrics, results = code_execution_metrics(\n            samples=label, \n            generations=[pred_outputs], \n        )\n    else:\n        raise ValueError(f\"Invalid scenario: {self.scenario}. Available choices: {VALID_SCENARIO}.\")\n\n    pass_at_k = {f\"pass@{k}\": float(metrics[f\"pass@{k}\"]) for k in k_list}\n    return pass_at_k\n</code></pre>"},{"location":"api/core/","title":"\ud83e\udde0 Core","text":""},{"location":"api/core/#evoagentx.core","title":"evoagentx.core","text":""},{"location":"api/core/#evoagentx.core.BaseConfig","title":"BaseConfig","text":"<pre><code>BaseConfig(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModule</code></p> <p>Base configuration class that serves as parent for all configuration classes.</p> <p>A config should inherit BaseConfig and specify the attributes and their types.  Otherwise this will be an empty config.</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/core/#evoagentx.core.BaseConfig.save","title":"save","text":"<pre><code>save(path: str, **kwargs) -&gt; str\n</code></pre> <p>Save configuration to the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The file path to save the configuration</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to save_module method</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The path where the file was saved</p> Source code in <code>evoagentx/core/base_config.py</code> <pre><code>def save(self, path: str, **kwargs)-&gt; str:\n\n    \"\"\"Save configuration to the specified path.\n\n    Args:\n        path: The file path to save the configuration\n        **kwargs (Any): Additional keyword arguments passed to save_module method\n\n    Returns:\n        str: The path where the file was saved\n    \"\"\"\n    return super().save_module(path, **kwargs)\n</code></pre>"},{"location":"api/core/#evoagentx.core.BaseConfig.get_config_params","title":"get_config_params","text":"<pre><code>get_config_params() -&gt; List[str]\n</code></pre> <p>Get a list of configuration parameters.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of configuration parameter names, excluding 'class_name'</p> Source code in <code>evoagentx/core/base_config.py</code> <pre><code>def get_config_params(self) -&gt; List[str]:\n    \"\"\"Get a list of configuration parameters.\n\n    Returns:\n        List[str]: List of configuration parameter names, excluding 'class_name'\n    \"\"\"\n    config_params = list(type(self).model_fields.keys())\n    config_params.remove(\"class_name\")\n    return config_params\n</code></pre>"},{"location":"api/core/#evoagentx.core.BaseConfig.get_set_params","title":"get_set_params","text":"<pre><code>get_set_params(ignore: List[str] = []) -&gt; dict\n</code></pre> <p>Get a dictionary of explicitly set parameters.</p> <p>Parameters:</p> Name Type Description Default <code>ignore</code> <code>List[str]</code> <p>List of parameter names to ignore</p> <code>[]</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary of explicitly set parameters, excluding 'class_name' and ignored parameters</p> Source code in <code>evoagentx/core/base_config.py</code> <pre><code>def get_set_params(self, ignore: List[str] = []) -&gt; dict:\n    \"\"\"Get a dictionary of explicitly set parameters.\n\n    Args:\n        ignore: List of parameter names to ignore\n\n    Returns:\n        dict: Dictionary of explicitly set parameters, excluding 'class_name' and ignored parameters\n    \"\"\"\n    explicitly_set_fields = {field: getattr(self, field) for field in self.model_fields_set}\n    if self.kwargs:\n        explicitly_set_fields.update(self.kwargs)\n    for field in ignore:\n        explicitly_set_fields.pop(field, None)\n    explicitly_set_fields.pop(\"class_name\", None)\n    return explicitly_set_fields\n</code></pre>"},{"location":"api/core/#evoagentx.core.Message","title":"Message","text":"<pre><code>Message(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModule</code></p> <p>the base class for message. </p> <p>Attributes:</p> Name Type Description <code>content</code> <code>Any</code> <p>the content of the message, need to implement str() function. </p> <code>agent</code> <code>str</code> <p>the sender of the message, normally set as the agent name.</p> <code>action</code> <code>str</code> <p>the trigger of the message, normally set as the action name.</p> <code>prompt</code> <code>str</code> <p>the prompt used to obtain the generated text. </p> <code>next_actions</code> <code>List[str]</code> <p>the following actions. </p> <code>msg_type</code> <code>str</code> <p>the type of the message, such as \"request\", \"response\", \"command\" etc. </p> <code>wf_goal</code> <code>str</code> <p>the goal of the whole workflow. </p> <code>wf_task</code> <code>str</code> <p>the name of a task in the workflow, i.e., the <code>name</code> of a WorkFlowNode instance. </p> <code>wf_task_desc</code> <code>str</code> <p>the description of a task in the workflow, i.e., the <code>description</code> of a WorkFlowNode instance.</p> <code>message_id</code> <code>str</code> <p>the unique identifier of the message. </p> <code>timestamp</code> <code>str</code> <p>the timestame of the message.</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/core/#evoagentx.core.Message.to_dict","title":"to_dict","text":"<pre><code>to_dict(exclude_none: bool = True, ignore: List[str] = [], **kwargs) -&gt; dict\n</code></pre> <p>Convert the Message to a dictionary for saving.</p> Source code in <code>evoagentx/core/message.py</code> <pre><code>def to_dict(self, exclude_none: bool = True, ignore: List[str] = [], **kwargs) -&gt; dict:\n    \"\"\"\n    Convert the Message to a dictionary for saving. \n    \"\"\"\n    data = super().to_dict(exclude_none=exclude_none, ignore=ignore, **kwargs) \n    if self.msg_type:\n        data[\"msg_type\"] = self.msg_type.value\n    return data \n</code></pre>"},{"location":"api/core/#evoagentx.core.Message.sort_by_timestamp","title":"sort_by_timestamp  <code>classmethod</code>","text":"<pre><code>sort_by_timestamp(messages: List[Message], reverse: bool = False) -&gt; List[Message]\n</code></pre> <p>sort the messages based on the timestamp. </p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>the messages to be sorted. </p> required <code>reverse</code> <code>bool</code> <p>If True, sort the messages in descending order. Otherwise, sort the messages in ascending order.</p> <code>False</code> Source code in <code>evoagentx/core/message.py</code> <pre><code>@classmethod\ndef sort_by_timestamp(cls, messages: List['Message'], reverse: bool = False) -&gt; List['Message']:\n    \"\"\"\n    sort the messages based on the timestamp. \n\n    Args: \n        messages (List[Message]): the messages to be sorted. \n        reverse (bool): If True, sort the messages in descending order. Otherwise, sort the messages in ascending order.\n    \"\"\"\n    messages.sort(key=lambda msg: datetime.strptime(msg.timestamp, \"%Y-%m-%d %H:%M:%S\"), reverse=reverse)\n    return messages\n</code></pre>"},{"location":"api/core/#evoagentx.core.Message.sort","title":"sort  <code>classmethod</code>","text":"<pre><code>sort(messages: List[Message], key: Optional[Callable[[Message], Any]] = None, reverse: bool = False) -&gt; List[Message]\n</code></pre> <p>sort the messages using key or timestamp (by default). </p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>the messages to be sorted. </p> required <code>key</code> <code>Optional[Callable[[Message], Any]]</code> <p>the function used to sort messages. </p> <code>None</code> <code>reverse</code> <code>bool</code> <p>If True, sort the messages in descending order. Otherwise, sort the messages in ascending order.</p> <code>False</code> Source code in <code>evoagentx/core/message.py</code> <pre><code>@classmethod\ndef sort(cls, messages: List['Message'], key: Optional[Callable[['Message'], Any]] = None, reverse: bool = False) -&gt; List['Message']:\n    \"\"\"\n    sort the messages using key or timestamp (by default). \n\n    Args:\n        messages (List[Message]): the messages to be sorted. \n        key (Optional[Callable[['Message'], Any]]): the function used to sort messages. \n        reverse (bool): If True, sort the messages in descending order. Otherwise, sort the messages in ascending order.\n    \"\"\"\n    if key is None:\n        return cls.sort_by_timestamp(messages, reverse=reverse)\n    messages.sort(key=key, reverse=reverse)\n    return messages\n</code></pre>"},{"location":"api/core/#evoagentx.core.Message.merge","title":"merge  <code>classmethod</code>","text":"<pre><code>merge(messages: List[List[Message]], sort: bool = False, key: Optional[Callable[[Message], Any]] = None, reverse: bool = False) -&gt; List[Message]\n</code></pre> <p>merge different message list. </p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[List[Message]]</code> <p>the message lists to be merged. </p> required <code>sort</code> <code>bool</code> <p>whether to sort the merged messages.</p> <code>False</code> <code>key</code> <code>Optional[Callable[[Message], Any]]</code> <p>the function used to sort messages. </p> <code>None</code> <code>reverse</code> <code>bool</code> <p>If True, sort the messages in descending order. Otherwise, sort the messages in ascending order.</p> <code>False</code> Source code in <code>evoagentx/core/message.py</code> <pre><code>@classmethod\ndef merge(cls, messages: List[List['Message']], sort: bool=False, key: Optional[Callable[['Message'], Any]] = None, reverse: bool=False) -&gt; List['Message']:\n    \"\"\"\n    merge different message list. \n\n    Args:\n        messages (List[List[Message]]): the message lists to be merged. \n        sort (bool): whether to sort the merged messages.\n        key (Optional[Callable[['Message'], Any]]): the function used to sort messages. \n        reverse (bool): If True, sort the messages in descending order. Otherwise, sort the messages in ascending order.\n    \"\"\"\n    merged_messages = sum(messages, [])\n    if sort:\n        merged_messages = cls.sort(merged_messages, key=key, reverse=reverse)\n    return merged_messages\n</code></pre>"},{"location":"api/core/#evoagentx.core.Parser","title":"Parser","text":"<pre><code>Parser(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModule</code></p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/core/#evoagentx.core.Parser.parse","title":"parse  <code>classmethod</code>","text":"<pre><code>parse(content: str, **kwargs)\n</code></pre> <p>the method used to parse text into a Parser object. Use Parser.from_str to parse input by default.  Args:     content: The content to parse     **kwargs: Additional keyword arguments Returns:     Parser: The parsed Parser object</p> Source code in <code>evoagentx/core/parser.py</code> <pre><code>@classmethod\ndef parse(cls, content: str, **kwargs):\n    \"\"\"\n    the method used to parse text into a Parser object. Use Parser.from_str to parse input by default. \n    Args:\n        content: The content to parse\n        **kwargs: Additional keyword arguments\n    Returns:\n        Parser: The parsed Parser object\n    \"\"\"\n    return cls.from_str(content, **kwargs)\n</code></pre>"},{"location":"api/core/#evoagentx.core.Parser.save","title":"save","text":"<pre><code>save(path: str, **kwargs) -&gt; str\n</code></pre> <p>Save the Parser object to a file.</p> Source code in <code>evoagentx/core/parser.py</code> <pre><code>def save(self, path: str, **kwargs)-&gt; str:\n    \"\"\"\n    Save the Parser object to a file.\n    \"\"\"\n    super().save_module(path, **kwargs)\n</code></pre>"},{"location":"api/core/#evoagentx.core.BaseModule","title":"BaseModule","text":"<pre><code>BaseModule(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>Base module class that serves as the foundation for all modules in the EvoAgentX framework.</p> <p>This class provides serialization/deserialization capabilities, supports creating instances from dictionaries, JSON, or files, and exporting instances to these formats.</p> <p>Attributes:</p> Name Type Description <code>class_name</code> <code>str</code> <p>The class name, defaults to None but is automatically set during subclass initialization</p> <code>model_config</code> <p>Pydantic model configuration that controls type matching and behavior</p> <p>Initializes a BaseModule instance.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Keyword arguments used to initialize the instance</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValidationError</code> <p>When parameter validation fails</p> <code>Exception</code> <p>When other errors occur during initialization</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/core/#evoagentx.core.BaseModule.kwargs","title":"kwargs  <code>property</code>","text":"<pre><code>kwargs: dict\n</code></pre> <p>Returns the extra fields of the model.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary containing all extra keyword arguments</p>"},{"location":"api/core/#evoagentx.core.BaseModule.__init_subclass__","title":"__init_subclass__","text":"<pre><code>__init_subclass__(**kwargs)\n</code></pre> <p>Subclass initialization method that automatically sets the class_name attribute.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Type</code> <p>The subclass being initialized</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments</p> <code>{}</code> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init_subclass__(cls, **kwargs):\n    \"\"\"\n    Subclass initialization method that automatically sets the class_name attribute.\n\n    Args:\n        cls (Type): The subclass being initialized\n        **kwargs (Any): Additional keyword arguments\n    \"\"\"\n    super().__init_subclass__(**kwargs)\n    cls.class_name = cls.__name__\n</code></pre>"},{"location":"api/core/#evoagentx.core.BaseModule.init_module","title":"init_module","text":"<pre><code>init_module()\n</code></pre> <p>Module initialization method that subclasses can override to provide additional initialization logic.</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def init_module(self):\n    \"\"\"\n    Module initialization method that subclasses can override to provide additional initialization logic.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#evoagentx.core.BaseModule.__str__","title":"__str__","text":"<pre><code>__str__() -&gt; str\n</code></pre> <p>Returns a string representation of the object.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>String representation of the object</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"\n    Returns a string representation of the object.\n\n    Returns:\n        str: String representation of the object\n    \"\"\"\n    return self.to_str()\n</code></pre>"},{"location":"api/core/#evoagentx.core.BaseModule.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: Dict[str, Any], **kwargs) -&gt; BaseModule\n</code></pre> <p>Instantiate the BaseModule from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>Dictionary containing instance data</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments, can include log to control logging output</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>BaseModule</code> <code>BaseModule</code> <p>The created module instance</p> <p>Raises:</p> Type Description <code>Exception</code> <p>When errors occur during initialization</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: Dict[str, Any], **kwargs) -&gt; \"BaseModule\":\n    \"\"\"\n    Instantiate the BaseModule from a dictionary.\n\n    Args:\n        data: Dictionary containing instance data\n        **kwargs (Any): Additional keyword arguments, can include log to control logging output\n\n    Returns:\n        BaseModule: The created module instance\n\n    Raises:\n        Exception: When errors occur during initialization\n    \"\"\"\n    use_logger = kwargs.get(\"log\", True)\n    with exception_buffer() as buffer:\n        try:\n            class_name = data.get(\"class_name\", None)\n            if class_name:\n                cls = MODULE_REGISTRY.get_module(class_name)\n            module = cls._create_instance(data)\n            # module = cls.model_validate(data)\n            if len(buffer.exceptions) &gt; 0:\n                error_message = get_base_module_init_error_message(cls, data, buffer.exceptions)\n                if use_logger:\n                    logger.error(error_message)\n                raise Exception(get_error_message(buffer.exceptions))\n        finally:\n            pass\n    return module\n</code></pre>"},{"location":"api/core/#evoagentx.core.BaseModule.from_json","title":"from_json  <code>classmethod</code>","text":"<pre><code>from_json(content: str, **kwargs) -&gt; BaseModule\n</code></pre> <p>Construct the BaseModule from a JSON string.</p> <p>This method uses yaml.safe_load to parse the JSON string into a Python object, which supports more flexible parsing than standard json.loads (including handling single quotes, trailing commas, etc). The parsed data is then passed to from_dict to create the instance.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>JSON string</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments, can include <code>log</code> to control logging output</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>BaseModule</code> <code>BaseModule</code> <p>The created module instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When the input is not a valid JSON string</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>@classmethod\ndef from_json(cls, content: str, **kwargs) -&gt; \"BaseModule\":\n    \"\"\"\n    Construct the BaseModule from a JSON string.\n\n    This method uses yaml.safe_load to parse the JSON string into a Python object,\n    which supports more flexible parsing than standard json.loads (including handling\n    single quotes, trailing commas, etc). The parsed data is then passed to from_dict\n    to create the instance.\n\n    Args:\n        content: JSON string\n        **kwargs (Any): Additional keyword arguments, can include `log` to control logging output\n\n    Returns:\n        BaseModule: The created module instance\n\n    Raises:\n        ValueError: When the input is not a valid JSON string\n    \"\"\"\n    use_logger = kwargs.get(\"log\", True)\n    try:\n        data = yaml.safe_load(content)\n    except Exception:\n        error_message = f\"Can not instantiate {cls.__name__}. The input to {cls.__name__}.from_json is not a valid JSON string.\"\n        if use_logger:\n            logger.error(error_message)\n        raise ValueError(error_message)\n\n    if not isinstance(data, (list, dict)):\n        error_message = f\"Can not instantiate {cls.__name__}. The input to {cls.__name__}.from_json is not a valid JSON string.\"\n        if use_logger:\n            logger.error(error_message)\n        raise ValueError(error_message)\n\n    return cls.from_dict(data, log=use_logger)\n</code></pre>"},{"location":"api/core/#evoagentx.core.BaseModule.from_str","title":"from_str  <code>classmethod</code>","text":"<pre><code>from_str(content: str, **kwargs) -&gt; BaseModule\n</code></pre> <p>Construct the BaseModule from a string that may contain JSON.</p> <p>This method is more forgiving than <code>from_json</code> as it can extract valid JSON objects embedded within larger text. It uses <code>parse_json_from_text</code> to extract  all potential JSON strings from the input text, then tries to create an instance  from each extracted JSON string until successful.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>Text that may contain JSON strings</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments, can include <code>log</code> to control logging output</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>BaseModule</code> <code>BaseModule</code> <p>The created module instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When the input does not contain valid JSON strings or the JSON is incompatible with the class</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>@classmethod\ndef from_str(cls, content: str, **kwargs) -&gt; \"BaseModule\":\n    \"\"\"\n    Construct the BaseModule from a string that may contain JSON.\n\n    This method is more forgiving than `from_json` as it can extract valid JSON\n    objects embedded within larger text. It uses `parse_json_from_text` to extract \n    all potential JSON strings from the input text, then tries to create an instance \n    from each extracted JSON string until successful.\n\n    Args:\n        content: Text that may contain JSON strings\n        **kwargs (Any): Additional keyword arguments, can include `log` to control logging output\n\n    Returns:\n        BaseModule: The created module instance\n\n    Raises:\n        ValueError: When the input does not contain valid JSON strings or the JSON is incompatible with the class\n    \"\"\"\n    use_logger = kwargs.get(\"log\", True)\n\n    extracted_json_list = parse_json_from_text(content)\n    if len(extracted_json_list) == 0:\n        error_message = f\"The input to {cls.__name__}.from_str does not contain any valid JSON str.\"\n        if use_logger:\n            logger.error(error_message)\n        raise ValueError(error_message)\n\n    module = None\n    for json_str in extracted_json_list:\n        try:\n            module = cls.from_json(json_str, log=False)\n        except Exception:\n            continue\n        break\n\n    if module is None:\n        error_message = f\"Can not instantiate {cls.__name__}. The input to {cls.__name__}.from_str either does not contain a valide JSON str, or the JSON str is incomplete or incompatable (incorrect variables or types) with {cls.__name__}.\"\n        error_message += f\"\\nInput:\\n{content}\"\n        if use_logger:\n            logger.error(error_message)\n        raise ValueError(error_message)\n\n    return module\n</code></pre>"},{"location":"api/core/#evoagentx.core.BaseModule.load_module","title":"load_module  <code>classmethod</code>","text":"<pre><code>load_module(path: str, **kwargs) -&gt; dict\n</code></pre> <p>Load the values for a module from a file.</p> <p>By default, it opens the specified file and uses <code>yaml.safe_load</code> to parse its contents  into a Python object (typically a dictionary).</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path of the file</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The JSON object instantiated from the file</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>@classmethod \ndef load_module(cls, path: str, **kwargs) -&gt; dict:\n    \"\"\"\n    Load the values for a module from a file.\n\n    By default, it opens the specified file and uses `yaml.safe_load` to parse its contents \n    into a Python object (typically a dictionary).\n\n    Args:\n        path: The path of the file\n        **kwargs (Any): Additional keyword arguments\n\n    Returns:\n        dict: The JSON object instantiated from the file\n    \"\"\"\n    with open(path, mode=\"r\", encoding=\"utf-8\") as file:\n        content = yaml.safe_load(file.read())\n    return content\n</code></pre>"},{"location":"api/core/#evoagentx.core.BaseModule.from_file","title":"from_file  <code>classmethod</code>","text":"<pre><code>from_file(path: str, load_function: Callable = None, **kwargs) -&gt; BaseModule\n</code></pre> <p>Construct the BaseModule from a file.</p> <p>This method reads and parses a file into a data structure, then creates a module instance from that data. It first verifies that the file exists, then uses either the provided <code>load_function</code> or the default <code>load_module</code> method to read and parse the file content, and finally calls <code>from_dict</code> to create the instance.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path of the file</p> required <code>load_function</code> <code>Callable</code> <p>The function used to load the data, takes a file path as input and returns a JSON object</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments, can include <code>log</code> to control logging output</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>BaseModule</code> <code>BaseModule</code> <p>The created module instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When the file does not exist</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>@classmethod\ndef from_file(cls, path: str, load_function: Callable=None, **kwargs) -&gt; \"BaseModule\":\n    \"\"\"\n    Construct the BaseModule from a file.\n\n    This method reads and parses a file into a data structure, then creates\n    a module instance from that data. It first verifies that the file exists,\n    then uses either the provided `load_function` or the default `load_module`\n    method to read and parse the file content, and finally calls `from_dict`\n    to create the instance.\n\n    Args:\n        path: The path of the file\n        load_function: The function used to load the data, takes a file path as input and returns a JSON object\n        **kwargs (Any): Additional keyword arguments, can include `log` to control logging output\n\n    Returns:\n        BaseModule: The created module instance\n\n    Raises:\n        ValueError: When the file does not exist\n    \"\"\"\n    use_logger = kwargs.get(\"log\", True)\n    if not os.path.exists(path):\n        error_message = f\"File \\\"{path}\\\" does not exist!\"\n        if use_logger:\n            logger.error(error_message)\n        raise ValueError(error_message)\n\n    function = load_function or cls.load_module\n    content = function(path, **kwargs)\n    module = cls.from_dict(content, log=use_logger)\n\n    return module\n</code></pre>"},{"location":"api/core/#evoagentx.core.BaseModule.to_dict","title":"to_dict","text":"<pre><code>to_dict(exclude_none: bool = True, ignore: List[str] = [], **kwargs) -&gt; dict\n</code></pre> <p>Convert the BaseModule to a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>exclude_none</code> <code>bool</code> <p>Whether to exclude fields with None values</p> <code>True</code> <code>ignore</code> <code>List[str]</code> <p>List of field names to ignore</p> <code>[]</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary containing the object data</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def to_dict(self, exclude_none: bool = True, ignore: List[str] = [], **kwargs) -&gt; dict:\n    \"\"\"\n    Convert the BaseModule to a dictionary.\n\n    Args:\n        exclude_none: Whether to exclude fields with None values\n        ignore: List of field names to ignore\n        **kwargs (Any): Additional keyword arguments\n\n    Returns:\n        dict: Dictionary containing the object data\n    \"\"\"\n    data = {}\n    for field_name, _ in type(self).model_fields.items():\n        if field_name in ignore:\n            continue\n        field_value = getattr(self, field_name, None)\n        if exclude_none and field_value is None:\n            continue\n        if isinstance(field_value, BaseModule):\n            data[field_name] = field_value.to_dict(exclude_none=exclude_none, ignore=ignore)\n        elif isinstance(field_value, list):\n            data[field_name] = [\n                item.to_dict(exclude_none=exclude_none, ignore=ignore) if isinstance(item, BaseModule) else item\n                for item in field_value\n            ]\n        elif isinstance(field_value, dict):\n            data[field_name] = {\n                key: value.to_dict(exclude_none=exclude_none, ignore=ignore) if isinstance(value, BaseModule) else value\n                for key, value in field_value.items()\n            }\n        else:\n            data[field_name] = field_value\n\n    return data\n</code></pre>"},{"location":"api/core/#evoagentx.core.BaseModule.to_json","title":"to_json","text":"<pre><code>to_json(use_indent: bool = False, ignore: List[str] = [], **kwargs) -&gt; str\n</code></pre> <p>Convert the BaseModule to a JSON string.</p> <p>Parameters:</p> Name Type Description Default <code>use_indent</code> <code>bool</code> <p>Whether to use indentation</p> <code>False</code> <code>ignore</code> <code>List[str]</code> <p>List of field names to ignore</p> <code>[]</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The JSON string</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def to_json(self, use_indent: bool=False, ignore: List[str] = [], **kwargs) -&gt; str:\n    \"\"\"\n    Convert the BaseModule to a JSON string.\n\n    Args:\n        use_indent: Whether to use indentation\n        ignore: List of field names to ignore\n        **kwargs (Any): Additional keyword arguments\n\n    Returns:\n        str: The JSON string\n    \"\"\"\n    if use_indent:\n        kwargs[\"indent\"] = kwargs.get(\"indent\", 4)\n    else:\n        kwargs.pop(\"indent\", None)\n    if kwargs.get(\"default\", None) is None:\n        kwargs[\"default\"] = custom_serializer\n    data = self.to_dict(exclude_none=True)\n    for ignore_field in ignore:\n        data.pop(ignore_field, None)\n    return json.dumps(data, **kwargs)\n</code></pre>"},{"location":"api/core/#evoagentx.core.BaseModule.to_str","title":"to_str","text":"<pre><code>to_str(**kwargs) -&gt; str\n</code></pre> <p>Convert the BaseModule to a string. Use .to_json to output JSON string by default.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The string</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def to_str(self, **kwargs) -&gt; str:\n    \"\"\"\n    Convert the BaseModule to a string. Use .to_json to output JSON string by default.\n\n    Args:\n        **kwargs (Any): Additional keyword arguments\n\n    Returns:\n        str: The string\n    \"\"\"\n    return self.to_json(use_indent=False)\n</code></pre>"},{"location":"api/core/#evoagentx.core.BaseModule.save_module","title":"save_module","text":"<pre><code>save_module(path: str, ignore: List[str] = [], **kwargs) -&gt; str\n</code></pre> <p>Save the BaseModule to a file.</p> <p>This method will set non-serializable objects to None by default. If you want to save non-serializable objects, override this method. Remember to also override the <code>load_module</code> function to ensure the loaded object can be correctly parsed by <code>cls.from_dict</code>.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to save the file</p> required <code>ignore</code> <code>List[str]</code> <p>List of field names to ignore</p> <code>[]</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The path where the file is saved, same as the input path</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def save_module(self, path: str, ignore: List[str] = [], **kwargs)-&gt; str:\n    \"\"\"\n    Save the BaseModule to a file.\n\n    This method will set non-serializable objects to None by default.\n    If you want to save non-serializable objects, override this method.\n    Remember to also override the `load_module` function to ensure the loaded\n    object can be correctly parsed by `cls.from_dict`.\n\n    Args:\n        path: The path to save the file\n        ignore: List of field names to ignore\n        **kwargs (Any): Additional keyword arguments\n\n    Returns:\n        str: The path where the file is saved, same as the input path\n    \"\"\"\n    logger.info(\"Saving {} to {}\", self.__class__.__name__, path)\n    return save_json(self.to_json(use_indent=True, default=lambda x: None, ignore=ignore), path=path)\n</code></pre>"},{"location":"api/core/#evoagentx.core.BaseModule.deepcopy","title":"deepcopy","text":"<pre><code>deepcopy()\n</code></pre> <p>Deep copy the module.</p> <p>This is a tweak to the default python deepcopy that only deep copies <code>self.parameters()</code>, and for other attributes, we just do the shallow copy.</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def deepcopy(self):\n    \"\"\"Deep copy the module.\n\n    This is a tweak to the default python deepcopy that only deep copies `self.parameters()`, and for other\n    attributes, we just do the shallow copy.\n    \"\"\"\n    try:\n        # If the instance itself is copyable, we can just deep copy it.\n        # Otherwise we will have to create a new instance and copy over the attributes one by one.\n        return copy.deepcopy(self)\n    except Exception:\n        pass\n\n    # Create an empty instance.\n    new_instance = self.__class__.__new__(self.__class__)\n    # Set attribuetes of the copied instance.\n    for attr, value in self.__dict__.items():\n        if isinstance(value, BaseModule):\n            setattr(new_instance, attr, value.deepcopy())\n        else:\n            try:\n                # Try to deep copy the attribute\n                setattr(new_instance, attr, copy.deepcopy(value))\n            except Exception:\n                logging.warning(\n                    f\"Failed to deep copy attribute '{attr}' of {self.__class__.__name__}, \"\n                    \"falling back to shallow copy or reference copy.\"\n                )\n                try:\n                    # Fallback to shallow copy if deep copy fails\n                    setattr(new_instance, attr, copy.copy(value))\n                except Exception:\n                    # If even the shallow copy fails, we just copy over the reference.\n                    setattr(new_instance, attr, value)\n\n    return new_instance\n</code></pre>"},{"location":"api/core/#evoagentx.core.ParseFunctionRegistry","title":"ParseFunctionRegistry","text":"<pre><code>ParseFunctionRegistry()\n</code></pre> Source code in <code>evoagentx/core/registry.py</code> <pre><code>def __init__(self):\n    self.functions = {}\n</code></pre>"},{"location":"api/core/#evoagentx.core.ParseFunctionRegistry.register","title":"register","text":"<pre><code>register(func_name: str, func)\n</code></pre> <p>Register a function with a given name.</p> <p>Parameters:</p> Name Type Description Default <code>func_name</code> <code>str</code> <p>The name to register the function under</p> required <code>func</code> <code>Callable</code> <p>The function to register</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If a function with the same name is already registered</p> Source code in <code>evoagentx/core/registry.py</code> <pre><code>def register(self, func_name: str, func):\n    \"\"\"Register a function with a given name.\n\n    Args:\n        func_name: The name to register the function under\n        func (Callable): The function to register\n\n    Raises:\n        ValueError: If a function with the same name is already registered\n    \"\"\"\n    if func_name in self.functions:\n        raise ValueError(f\"Function name '{func_name}' is already registered!\")\n    self.functions[func_name] = func\n</code></pre>"},{"location":"api/core/#evoagentx.core.ParseFunctionRegistry.get_function","title":"get_function","text":"<pre><code>get_function(func_name: str) -&gt; callable\n</code></pre> <p>Get a registered function by name.</p> <p>Parameters:</p> Name Type Description Default <code>func_name</code> <code>str</code> <p>The name of the function to retrieve</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <code>callable</code> <p>The registered function</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If no function with the given name is registered</p> Source code in <code>evoagentx/core/registry.py</code> <pre><code>def get_function(self, func_name: str) -&gt; callable:\n    \"\"\"Get a registered function by name.\n\n    Args:\n        func_name: The name of the function to retrieve\n\n    Returns:\n        Callable: The registered function\n\n    Raises:\n        KeyError: If no function with the given name is registered\n    \"\"\"\n    if func_name not in self.functions:\n        available_funcs = list(self.functions.keys())\n        raise KeyError(f\"Function '{func_name}' not found! Available functions: {available_funcs}\")\n    return self.functions[func_name]\n</code></pre>"},{"location":"api/core/#evoagentx.core.ParseFunctionRegistry.has_function","title":"has_function","text":"<pre><code>has_function(func_name: str) -&gt; bool\n</code></pre> <p>Check if a function name is registered.</p> <p>Parameters:</p> Name Type Description Default <code>func_name</code> <code>str</code> <p>The name to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the function name is registered, False otherwise</p> Source code in <code>evoagentx/core/registry.py</code> <pre><code>def has_function(self, func_name: str) -&gt; bool:\n    \"\"\"Check if a function name is registered.\n\n    Args:\n        func_name: The name to check\n\n    Returns:\n        True if the function name is registered, False otherwise\n    \"\"\"\n    return func_name in self.functions\n</code></pre>"},{"location":"api/evaluators/","title":"\ud83e\uddd1\u200d\u2696\ufe0f Evaluators","text":""},{"location":"api/evaluators/#evoagentx.evaluators","title":"evoagentx.evaluators","text":""},{"location":"api/evaluators/#evoagentx.evaluators.Evaluator","title":"Evaluator","text":"<pre><code>Evaluator(llm: BaseLLM, num_workers: int = 1, agent_manager: Optional[AgentManager] = None, collate_func: Optional[Callable] = None, output_postprocess_func: Optional[Callable] = None, verbose: Optional[bool] = None, **kwargs)\n</code></pre> <p>A class for evaluating the performance of a workflow.</p> <p>Initialize the Evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>BaseLLM</code> <p>The LLM to use for evaluation.</p> required <code>num_workers</code> <code>int</code> <p>The number of parallel workers to use for evaluation. Default is 1. </p> <code>1</code> <code>agent_manager</code> <code>AgentManager</code> <p>The agent manager used to construct the workflow. Only used when the workflow graph is a WorkFlowGraph.</p> <code>None</code> <code>collate_func</code> <code>Callable</code> <p>A function to collate the benchmark data.  It receives a single example from the benchmark and the output (which should be a dictionary) will serve as inputs to the <code>execute</code> function of an WorkFlow (or ActionGraph) instance.  Note that the keys in the collated output should match the inputs of the workflow. The default is a lambda function that returns the example itself. </p> <code>None</code> <code>output_postprocess_func</code> <code>Callable</code> <p>A function to postprocess the output of the workflow.  It receives the output of an WorkFlow instance (str) or an ActionGraph instance (dict) as input  and the output will be passed to the <code>evaluate</code> function of the benchmark.  The default is a lambda function that returns the output itself.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to print the evaluation progress.</p> <code>None</code> Source code in <code>evoagentx/evaluators/evaluator.py</code> <pre><code>def __init__(\n    self, \n    llm: BaseLLM,\n    num_workers: int = 1, \n    agent_manager: Optional[AgentManager] = None,\n    collate_func: Optional[Callable] = None, \n    output_postprocess_func: Optional[Callable] = None, \n    verbose: Optional[bool] = None, \n    **kwargs\n):\n    \"\"\"\n    Initialize the Evaluator.\n\n    Args:\n        llm (BaseLLM): The LLM to use for evaluation.\n        num_workers (int): The number of parallel workers to use for evaluation. Default is 1. \n        agent_manager (AgentManager, optional): The agent manager used to construct the workflow. Only used when the workflow graph is a WorkFlowGraph.\n        collate_func (Callable, optional): A function to collate the benchmark data. \n            It receives a single example from the benchmark and the output (which should be a dictionary) will serve as inputs  \n            to the `execute` function of an WorkFlow (or ActionGraph) instance. \n            Note that the keys in the collated output should match the inputs of the workflow.\n            The default is a lambda function that returns the example itself. \n        output_postprocess_func (Callable, optional): A function to postprocess the output of the workflow. \n            It receives the output of an WorkFlow instance (str) or an ActionGraph instance (dict) as input \n            and the output will be passed to the `evaluate` function of the benchmark. \n            The default is a lambda function that returns the output itself.\n        verbose (bool, optional): Whether to print the evaluation progress.\n    \"\"\"\n    self.llm = llm\n    self.num_workers = num_workers\n    self.agent_manager = agent_manager\n    self._thread_agent_managers = {}\n    self.collate_func = collate_func or (lambda x: x)\n    self.output_postprocess_func = output_postprocess_func or (lambda x: x)\n    self.verbose = verbose\n    # {example_id: {\"prediction\": Any, \"label\": Any, \"metrics\": dict, \"trajectory\" (WorkFlowGraph only): List[Message]}}\n    self._evaluation_records = {}\n    self.kwargs = kwargs\n</code></pre>"},{"location":"api/evaluators/#evoagentx.evaluators.Evaluator.evaluate","title":"evaluate","text":"<pre><code>evaluate(graph: Union[WorkFlowGraph, ActionGraph], benchmark: Benchmark, eval_mode: str = 'test', indices: Optional[List[int]] = None, sample_k: Optional[int] = None, seed: Optional[int] = None, verbose: Optional[bool] = None, update_agents: Optional[bool] = False, **kwargs) -&gt; dict\n</code></pre> <p>Evaluate the performance of the workflow on the benchmark.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>WorkFlowGraph or ActionGraph</code> <p>The workflow to evaluate.</p> required <code>benchmark</code> <code>Benchmark</code> <p>The benchmark to evaluate the workflow on.</p> required <code>eval_mode</code> <code>str</code> <p>which split of the benchmark to evaluate the workflow on. Choices: [\"test\", \"dev\", \"train\"].</p> <code>'test'</code> <code>indices</code> <code>List[int]</code> <p>The indices of the data to evaluate the workflow on.</p> <code>None</code> <code>sample_k</code> <code>int</code> <p>The number of data to evaluate the workflow on. If provided, a random sample of size <code>sample_k</code> will be used.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to print the evaluation progress. If not provided, the <code>self.verbose</code> will be used.</p> <code>None</code> <code>update_agents</code> <code>bool</code> <p>Whether to update the agents in the agent manager. Only used when the workflow graph is a WorkFlowGraph.</p> <code>False</code> <p>Returns:     dict: The average metrics of the workflow evaluation.</p> Source code in <code>evoagentx/evaluators/evaluator.py</code> <pre><code>def evaluate(\n    self, \n    graph: Union[WorkFlowGraph, ActionGraph],\n    benchmark: Benchmark, \n    eval_mode: str = \"test\", \n    indices: Optional[List[int]] = None, \n    sample_k: Optional[int] = None, \n    seed: Optional[int] = None, \n    verbose: Optional[bool] = None,\n    update_agents: Optional[bool] = False,\n    **kwargs\n) -&gt; dict:\n    \"\"\"\n    Evaluate the performance of the workflow on the benchmark.\n\n    Args:\n        graph (WorkFlowGraph or ActionGraph): The workflow to evaluate.\n        benchmark (Benchmark): The benchmark to evaluate the workflow on.\n        eval_mode (str): which split of the benchmark to evaluate the workflow on. Choices: [\"test\", \"dev\", \"train\"].\n        indices (List[int], optional): The indices of the data to evaluate the workflow on.\n        sample_k (int, optional): The number of data to evaluate the workflow on. If provided, a random sample of size `sample_k` will be used.\n        verbose (bool, optional): Whether to print the evaluation progress. If not provided, the `self.verbose` will be used.\n        update_agents (bool, optional): Whether to update the agents in the agent manager. Only used when the workflow graph is a WorkFlowGraph.\n    Returns:\n        dict: The average metrics of the workflow evaluation.\n    \"\"\"\n    # clear the evaluation records\n    self._evaluation_records.clear()\n\n    # update the agents in the agent manager\n    if isinstance(graph, WorkFlowGraph) and update_agents:\n        if self.agent_manager is None:\n            raise ValueError(f\"`agent_manager` is not provided in {type(self).__name__}. Please provide an agent manager when evaluating a WorkFlowGraph.\")\n        self.agent_manager.update_agents_from_workflow(workflow_graph=graph, llm_config=self.llm.config, **kwargs)\n\n    data = self._get_eval_data(benchmark=benchmark, eval_mode=eval_mode, indices=indices, sample_k=sample_k, seed=seed)\n    results = self._evaluate_graph(graph=graph, data=data, benchmark=benchmark, verbose=verbose, **kwargs)\n    return results\n</code></pre>"},{"location":"api/evaluators/#evoagentx.evaluators.Evaluator.get_example_evaluation_record","title":"get_example_evaluation_record","text":"<pre><code>get_example_evaluation_record(benchmark: Benchmark, example: Any) -&gt; Optional[dict]\n</code></pre> <p>Get the evaluation record for a given example.</p> Source code in <code>evoagentx/evaluators/evaluator.py</code> <pre><code>def get_example_evaluation_record(self, benchmark: Benchmark, example: Any) -&gt; Optional[dict]:\n    \"\"\"\n    Get the evaluation record for a given example.\n    \"\"\"\n    example_id = benchmark.get_id(example=example)\n    return self._evaluation_records.get(example_id, None)\n</code></pre>"},{"location":"api/evaluators/#evoagentx.evaluators.Evaluator.get_evaluation_record_by_id","title":"get_evaluation_record_by_id","text":"<pre><code>get_evaluation_record_by_id(benchmark: Benchmark, example_id: str, eval_mode: str = 'test') -&gt; Optional[dict]\n</code></pre> <p>Get the evaluation record for a given example id.</p> Source code in <code>evoagentx/evaluators/evaluator.py</code> <pre><code>def get_evaluation_record_by_id(self, benchmark: Benchmark, example_id: str, eval_mode: str = \"test\") -&gt; Optional[dict]:\n    \"\"\"\n    Get the evaluation record for a given example id.\n    \"\"\"\n    example = benchmark.get_example_by_id(example_id=example_id, mode=eval_mode)\n    return self.get_example_evaluation_record(benchmark=benchmark, example=example)\n</code></pre>"},{"location":"api/evaluators/#evoagentx.evaluators.Evaluator.get_all_evaluation_records","title":"get_all_evaluation_records","text":"<pre><code>get_all_evaluation_records() -&gt; dict\n</code></pre> <p>Get all the evaluation records.</p> Source code in <code>evoagentx/evaluators/evaluator.py</code> <pre><code>def get_all_evaluation_records(self) -&gt; dict:\n    \"\"\"\n    Get all the evaluation records.\n    \"\"\"\n    return self._evaluation_records.copy()\n</code></pre>"},{"location":"api/evaluators/#evoagentx.evaluators.Evaluator.async_evaluate","title":"async_evaluate  <code>async</code>","text":"<pre><code>async_evaluate(graph: Union[WorkFlowGraph, ActionGraph], benchmark: Benchmark, eval_mode: str = 'test', indices: Optional[List[int]] = None, sample_k: Optional[int] = None, seed: Optional[int] = None, verbose: Optional[bool] = None, **kwargs) -&gt; dict\n</code></pre> <p>Asynchronously evaluate the performance of the workflow on the benchmark.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>WorkFlowGraph or ActionGraph</code> <p>The workflow to evaluate.</p> required <code>benchmark</code> <code>Benchmark</code> <p>The benchmark to evaluate the workflow on.</p> required <code>eval_mode</code> <code>str</code> <p>which split of the benchmark to evaluate the workflow on. Choices: [\"test\", \"dev\", \"train\"].</p> <code>'test'</code> <code>indices</code> <code>List[int]</code> <p>The indices of the data to evaluate the workflow on.</p> <code>None</code> <code>sample_k</code> <code>int</code> <p>The number of data to evaluate the workflow on. If provided, a random sample of size <code>sample_k</code> will be used.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to print the evaluation progress. If not provided, the <code>self.verbose</code> will be used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The average metrics of the workflow evaluation.</p> Source code in <code>evoagentx/evaluators/evaluator.py</code> <pre><code>async def async_evaluate(\n    self, \n    graph: Union[WorkFlowGraph, ActionGraph],\n    benchmark: Benchmark, \n    eval_mode: str = \"test\", \n    indices: Optional[List[int]] = None, \n    sample_k: Optional[int] = None, \n    seed: Optional[int] = None, \n    verbose: Optional[bool] = None,\n    **kwargs\n) -&gt; dict:\n    \"\"\"\n    Asynchronously evaluate the performance of the workflow on the benchmark.\n\n    Args:\n        graph (WorkFlowGraph or ActionGraph): The workflow to evaluate.\n        benchmark (Benchmark): The benchmark to evaluate the workflow on.\n        eval_mode (str): which split of the benchmark to evaluate the workflow on. Choices: [\"test\", \"dev\", \"train\"].\n        indices (List[int], optional): The indices of the data to evaluate the workflow on.\n        sample_k (int, optional): The number of data to evaluate the workflow on. If provided, a random sample of size `sample_k` will be used.\n        verbose (bool, optional): Whether to print the evaluation progress. If not provided, the `self.verbose` will be used.\n\n    Returns:\n        dict: The average metrics of the workflow evaluation.\n    \"\"\"\n    # clear the evaluation records\n    self._evaluation_records.clear()\n    data = self._get_eval_data(benchmark=benchmark, eval_mode=eval_mode, indices=indices, sample_k=sample_k, seed=seed)\n\n    if not data:\n        logger.warning(\"No data to evaluate. Return an empty dictionary.\")\n        return {}\n\n    verbose = verbose if verbose is not None else self.verbose\n\n    # Create a semaphore to limit concurrent executions\n    sem = asyncio.Semaphore(self.num_workers)\n\n    async def process_with_semaphore(example):\n        async with sem:\n            try:\n                return await self._async_evaluate_single_example(\n                    graph=graph, \n                    example=example, \n                    benchmark=benchmark, \n                    **kwargs\n                )\n            except Exception as e:\n                logger.warning(f\"Async evaluation failed for example with semaphore: {str(e)}\")\n                return None\n\n    # Create tasks for concurrent execution with semaphore\n    tasks = [process_with_semaphore(example) for example in data]\n\n    # Execute all tasks with progress bar if verbose\n    if verbose:\n        results = await tqdm_asyncio.gather(\n            *tasks,\n            desc=f\"Evaluating {benchmark.name}\",\n            total=len(data)\n        )\n    else:\n        results = await asyncio.gather(*tasks)\n\n    return self._calculate_average_score(results)\n</code></pre>"},{"location":"api/memory/","title":"\ud83d\udd87\ufe0f Memory","text":""},{"location":"api/memory/#evoagentx.memory","title":"evoagentx.memory","text":""},{"location":"api/memory/#evoagentx.memory.BaseMemory","title":"BaseMemory","text":"<pre><code>BaseMemory(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModule</code></p> <p>Base class for memory implementations in the EvoAgentX framework.</p> <p>BaseMemory provides core functionality for storing, retrieving, and  filtering messages. It maintains a chronological list of messages while  also providing indices for efficient retrieval by action or workflow goal.</p> <p>Attributes:</p> Name Type Description <code>messages</code> <code>List[Message]</code> <p>List of stored Message objects.</p> <code>memory_id</code> <code>str</code> <p>Unique identifier for this memory instance.</p> <code>timestamp</code> <code>str</code> <p>Creation timestamp of this memory instance.</p> <code>capacity</code> <code>Optional[PositiveInt]</code> <p>Maximum number of messages that can be stored, or None for unlimited.</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/memory/#evoagentx.memory.BaseMemory.size","title":"size  <code>property</code>","text":"<pre><code>size: int\n</code></pre> <p>Returns the current number of messages in memory.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of messages currently stored.</p>"},{"location":"api/memory/#evoagentx.memory.BaseMemory.init_module","title":"init_module","text":"<pre><code>init_module()\n</code></pre> <p>Initialize memory indices.</p> <p>Creates default dictionaries for indexing messages by action and workflow goal.</p> Source code in <code>evoagentx/memory/memory.py</code> <pre><code>def init_module(self):\n    \"\"\"Initialize memory indices.\n\n    Creates default dictionaries for indexing messages by action and workflow goal.\n    \"\"\"\n    self._by_action = defaultdict(list)\n    self._by_wf_goal = defaultdict(list)\n</code></pre>"},{"location":"api/memory/#evoagentx.memory.BaseMemory.clear","title":"clear","text":"<pre><code>clear()\n</code></pre> <p>Clear all messages from memory.</p> <p>Removes all messages and resets all indices.</p> Source code in <code>evoagentx/memory/memory.py</code> <pre><code>def clear(self):\n    \"\"\"Clear all messages from memory.\n\n    Removes all messages and resets all indices.\n    \"\"\"\n    self.messages.clear()\n    self._by_action.clear()\n    self._by_wf_goal.clear()\n</code></pre>"},{"location":"api/memory/#evoagentx.memory.BaseMemory.remove_message","title":"remove_message","text":"<pre><code>remove_message(message: Message)\n</code></pre> <p>Remove a single message from memory.</p> <p>Removes the specified message from the main message list and all indices. If the message is not found in memory, no action is taken.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Message</code> <p>The message to be removed. The message will be removed from     self.messages, self._by_action, and self._by_wf_goal.</p> required Source code in <code>evoagentx/memory/memory.py</code> <pre><code>def remove_message(self, message: Message):\n    \"\"\"Remove a single message from memory.\n\n    Removes the specified message from the main message list and all indices.\n    If the message is not found in memory, no action is taken.\n\n    Args:\n        message: The message to be removed. The message will be removed from \n               self.messages, self._by_action, and self._by_wf_goal.\n    \"\"\"\n    if not message:\n        return\n    if message not in self.messages:\n        return\n    safe_remove(self.messages, message)\n    if self._by_action and not message.action:\n        safe_remove(self._by_action[message.action], message)\n    if self._by_wf_goal and not message.wf_goal:\n        safe_remove(self._by_wf_goal[message.wf_goal], message)\n</code></pre>"},{"location":"api/memory/#evoagentx.memory.BaseMemory.add_message","title":"add_message","text":"<pre><code>add_message(message: Message)\n</code></pre> <p>Store a single message in memory.</p> <p>Adds the message to the main list and relevant indices if it's not already stored.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Message</code> <p>the message to be stored.</p> required Source code in <code>evoagentx/memory/memory.py</code> <pre><code>def add_message(self, message: Message):\n    \"\"\"Store a single message in memory.\n\n    Adds the message to the main list and relevant indices if it's not already stored.\n\n    Args:\n        message (Message): the message to be stored. \n    \"\"\"\n    if not message:\n        return\n    if message in self.messages:\n        return\n    self.messages.append(message)\n    if self._by_action and not message.action:\n        self._by_action[message.action].append(message)\n    if self._by_wf_goal and not message.wf_goal:\n        self._by_wf_goal[message.wf_goal].append(message)\n</code></pre>"},{"location":"api/memory/#evoagentx.memory.BaseMemory.add_messages","title":"add_messages","text":"<pre><code>add_messages(messages: Union[Message, List[Message]], **kwargs)\n</code></pre> <p>store (a) message(s) to the memory. </p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Union[Message, List[Message]]</code> <p>the input messages can be a single message or a list of message.</p> required Source code in <code>evoagentx/memory/memory.py</code> <pre><code>def add_messages(self, messages: Union[Message, List[Message]], **kwargs):\n    \"\"\"\n    store (a) message(s) to the memory. \n\n    Args:\n        messages (Union[Message, List[Message]]): the input messages can be a single message or a list of message.\n    \"\"\"\n    if not isinstance(messages, list):\n        messages = [messages]\n    for message in messages:\n        self.add_message(message)\n</code></pre>"},{"location":"api/memory/#evoagentx.memory.BaseMemory.get","title":"get","text":"<pre><code>get(n: int = None, **kwargs) -&gt; List[Message]\n</code></pre> <p>Retrieve recent messages from memory.</p> <p>Returns the most recent messages, up to the specified limit.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The maximum number of messages to return. If None, returns all messages.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters (unused in base implementation).</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Message]</code> <p>A list of Message objects, ordered from oldest to newest.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If n is negative.</p> Source code in <code>evoagentx/memory/memory.py</code> <pre><code>def get(self, n: int=None, **kwargs) -&gt; List[Message]:\n    \"\"\"Retrieve recent messages from memory.\n\n    Returns the most recent messages, up to the specified limit.\n\n    Args: \n        n: The maximum number of messages to return. If None, returns all messages.\n        **kwargs (Any): Additional parameters (unused in base implementation).\n\n    Returns:\n        A list of Message objects, ordered from oldest to newest.\n\n    Raises:\n        AssertionError: If n is negative.\n    \"\"\"\n    assert n is None or n&gt;=0, \"n must be None or a positive int\"\n    messages = self.messages if n is None else self.messages[-n:]\n    return messages\n</code></pre>"},{"location":"api/memory/#evoagentx.memory.BaseMemory.get_by_type","title":"get_by_type","text":"<pre><code>get_by_type(data: Dict[str, list], key: str, n: int = None, **kwargs) -&gt; List[Message]\n</code></pre> <p>Retrieve a list of Message objects from a given data dictionary <code>data</code> based on a specified type <code>key</code>.</p> <p>This function looks up the value associated with <code>key</code> in the <code>data</code> dictionary, which should be a list of messages. It then returns a subset of these messages according to the specified parameters. If <code>n</code> is provided, it limits the number of messages returned; otherwise, it may return the entire list. Additional keyword arguments (**kwargs) can be used to further filter or process the resulting messages.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, list]</code> <p>A dictionary where keys are type strings and values are lists of messages.</p> required <code>key</code> <code>str</code> <p>The key in <code>data</code> identifying the specific list of messages to retrieve.</p> required <code>n</code> <code>int</code> <p>The maximum number of messages to return. If not provided, all messages under the given <code>key</code> may be returned.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters for filtering or processing the messages.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Message]</code> <p>List[Message]: A list of messages corresponding to the given <code>key</code>, possibly filtered or truncated according to <code>n</code> and other provided keyword arguments.</p> Source code in <code>evoagentx/memory/memory.py</code> <pre><code>def get_by_type(self, data: Dict[str, list], key: str, n: int = None, **kwargs) -&gt; List[Message]:\n    \"\"\"\n    Retrieve a list of Message objects from a given data dictionary `data` based on a specified type `key`.\n\n    This function looks up the value associated with `key` in the `data` dictionary, which should be a list of messages. It then returns a subset of these messages according to the specified parameters.\n    If `n` is provided, it limits the number of messages returned; otherwise, it may return the entire list. Additional keyword arguments (**kwargs) can be used to further filter or process the resulting messages.\n\n    Args:\n        data (Dict[str, list]): A dictionary where keys are type strings and values are lists of messages.\n        key (str): The key in `data` identifying the specific list of messages to retrieve.\n        n (int, optional): The maximum number of messages to return. If not provided, all messages under the given `key` may be returned.\n        **kwargs (Any): Additional parameters for filtering or processing the messages.\n\n    Returns:\n        List[Message]: A list of messages corresponding to the given `key`, possibly filtered or truncated according to `n` and other provided keyword arguments.\n    \"\"\"\n    if not data or key not in data:\n        return []\n    assert n is None or n&gt;=0, \"n must be None or a positive int\"\n    messages = data[key] if n is None else data[key][-n:]\n    return messages\n</code></pre>"},{"location":"api/memory/#evoagentx.memory.BaseMemory.get_by_action","title":"get_by_action","text":"<pre><code>get_by_action(actions: Union[str, List[str]], n: int = None, **kwargs) -&gt; List[Message]\n</code></pre> <p>return messages triggered by <code>actions</code> in the memory. </p> <p>Parameters:</p> Name Type Description Default <code>actions</code> <code>Union[str, List[str]]</code> <p>A single action name or list of action names to filter by.</p> required <code>n</code> <code>int</code> <p>Maximum number of messages to return per action. If None, returns all matching messages.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters (unused in base implementation).</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Message]</code> <p>A list of Message objects, sorted by timestamp.</p> Source code in <code>evoagentx/memory/memory.py</code> <pre><code>def get_by_action(self, actions: Union[str, List[str]], n: int=None, **kwargs) -&gt; List[Message]:\n    \"\"\"\n    return messages triggered by `actions` in the memory. \n\n    Args:\n        actions: A single action name or list of action names to filter by.\n        n: Maximum number of messages to return per action. If None, returns all matching messages.\n        **kwargs (Any): Additional parameters (unused in base implementation).\n\n    Returns:\n        A list of Message objects, sorted by timestamp.\n    \"\"\"\n    if isinstance(actions, str):\n        actions = [actions]\n    messages = []\n    for action in actions:\n        messages.extend(self.get_by_type(self._by_action, key=action, n=n, **kwargs))\n    messages = Message.sort_by_timestamp(messages)\n    return messages\n</code></pre>"},{"location":"api/memory/#evoagentx.memory.BaseMemory.get_by_wf_goal","title":"get_by_wf_goal","text":"<pre><code>get_by_wf_goal(wf_goals: Union[str, List[str]], n: int = None, **kwargs) -&gt; List[Message]\n</code></pre> <p>return messages related to <code>wf_goals</code> in the memory. </p> <p>Parameters:</p> Name Type Description Default <code>wf_goals</code> <code>Union[str, List[str]]</code> <p>A single workflow goal or list of workflow goals to filter by.</p> required <code>n</code> <code>int</code> <p>Maximum number of messages to return per workflow goal. If None, returns all matching messages.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters (unused in base implementation).</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Message]</code> <p>A list of Message objects, sorted by timestamp.</p> Source code in <code>evoagentx/memory/memory.py</code> <pre><code>def get_by_wf_goal(self, wf_goals: Union[str, List[str]], n: int=None, **kwargs) -&gt; List[Message]:\n    \"\"\"\n    return messages related to `wf_goals` in the memory. \n\n    Args:\n        wf_goals: A single workflow goal or list of workflow goals to filter by.\n        n: Maximum number of messages to return per workflow goal. If None, returns all matching messages.\n        **kwargs (Any): Additional parameters (unused in base implementation).\n\n    Returns:\n        A list of Message objects, sorted by timestamp.\n    \"\"\"\n    if isinstance(wf_goals, str):\n        wf_goals = [wf_goals]\n    messages = []\n    for wf_goal in wf_goals:\n        messages.append(self.get_by_type(self._by_wf_goal, key=wf_goal, n=n, **kwargs))\n    messages = Message.sort_by_timestamp(messages)\n    return messages\n</code></pre>"},{"location":"api/memory/#evoagentx.memory.ShortTermMemory","title":"ShortTermMemory","text":"<pre><code>ShortTermMemory(**kwargs)\n</code></pre> <p>               Bases: <code>BaseMemory</code></p> <p>Short-term memory implementation.</p> <p>This class extends BaseMemory to represent a temporary, short-term memory storage. In the current implementation, it inherits all functionality from BaseMemory without modifications, but it provides a semantic distinction for different memory usage patterns in the framework.</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/memory/#evoagentx.memory.LongTermMemory","title":"LongTermMemory","text":"<pre><code>LongTermMemory(**kwargs)\n</code></pre> <p>               Bases: <code>BaseMemory</code></p> <p>Responsible for the management of raw data for long-term storage.</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/memory/#evoagentx.memory.MemoryManager","title":"MemoryManager","text":"<pre><code>MemoryManager(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModule</code></p> <p>The Memory Manager is responsible for organizing and managing LongTerm Memory's data at a higher level. It gets data from LongTermMemory, then it processes the data, store the data in LongTermMemory,  and store the LongTermMemory through StorageHandler.</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/models/","title":"\ud83e\uddec Models","text":""},{"location":"api/models/#evoagentx.models","title":"evoagentx.models","text":""},{"location":"api/models/#evoagentx.models.LLMOutputParser","title":"LLMOutputParser","text":"<pre><code>LLMOutputParser(**kwargs)\n</code></pre> <p>               Bases: <code>Parser</code></p> <p>A basic parser for LLM-generated content.</p> <p>This parser stores the raw text generated by an LLM in the <code>.content</code> attribute and provides methods to extract structured data from this text using different parsing strategies.</p> <p>Attributes:</p> Name Type Description <code>content</code> <code>str</code> <p>The raw text generated by the LLM.</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/models/#evoagentx.models.LLMOutputParser.get_attrs","title":"get_attrs  <code>classmethod</code>","text":"<pre><code>get_attrs(return_type: bool = False) -&gt; List[Union[str, tuple]]\n</code></pre> <p>Returns the attributes of the LLMOutputParser class.</p> <p>Excludes [\"class_name\", \"content\"] by default.</p> <p>Parameters:</p> Name Type Description Default <code>return_type</code> <code>bool</code> <p>Whether to return the type of the attributes along with their names.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[Union[str, tuple]]</code> <p>If <code>return_type</code> is True, returns a list of tuples where each tuple contains </p> <code>List[Union[str, tuple]]</code> <p>the attribute name and its type. Otherwise, returns a list of attribute names.</p> Source code in <code>evoagentx/models/base_model.py</code> <pre><code>@classmethod\ndef get_attrs(cls, return_type: bool = False) -&gt; List[Union[str, tuple]]:\n    \"\"\"Returns the attributes of the LLMOutputParser class.\n\n    Excludes [\"class_name\", \"content\"] by default.\n\n    Args:\n        return_type: Whether to return the type of the attributes along with their names.\n\n    Returns:\n        If `return_type` is True, returns a list of tuples where each tuple contains \n        the attribute name and its type. Otherwise, returns a list of attribute names.\n    \"\"\"\n    attrs = [] \n    exclude_attrs = [\"class_name\", \"content\"]\n    for field, field_info in cls.model_fields.items():\n        if field not in exclude_attrs:\n            if return_type:\n                field_type = get_type_name(field_info.annotation)\n                attrs.append((field, field_type))\n            else:\n                attrs.append(field)\n    return attrs\n</code></pre>"},{"location":"api/models/#evoagentx.models.LLMOutputParser.get_attr_descriptions","title":"get_attr_descriptions  <code>classmethod</code>","text":"<pre><code>get_attr_descriptions() -&gt; dict\n</code></pre> <p>Returns the attributes and their descriptions.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary mapping attribute names to their descriptions.</p> Source code in <code>evoagentx/models/base_model.py</code> <pre><code>@classmethod\ndef get_attr_descriptions(cls) -&gt; dict:\n    \"\"\"Returns the attributes and their descriptions.\n\n    Returns:\n        A dictionary mapping attribute names to their descriptions.\n    \"\"\"\n    attrs = cls.get_attrs()\n    results = {} \n    for field_name, field_info in cls.model_fields.items():\n        if field_name not in attrs:\n            continue\n        field_desc = field_info.description if field_info.description is not None else \"None\"\n        results[field_name] = field_desc\n    return results\n</code></pre>"},{"location":"api/models/#evoagentx.models.LLMOutputParser.get_content_data","title":"get_content_data  <code>classmethod</code>","text":"<pre><code>get_content_data(content: str, parse_mode: str = 'json', parse_func: Optional[Callable] = None, **kwargs) -&gt; dict\n</code></pre> <p>Parses LLM-generated content into a dictionary.</p> <p>This method takes content from an LLM response and converts it to a structured dictionary based on the specified parsing mode.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The content to parse.</p> required <code>parse_mode</code> <code>str</code> <p>The mode to parse the content. Must be one of: - 'str': Assigns the raw text content to all attributes of the parser.  - 'json': Extracts and parses JSON objects from LLM output. It will return a dictionary parsed from the first valid JSON string. - 'xml': Parses content using XML tags. It will return a dictionary parsed from the XML tags. - 'title': Parses content with Markdown-style headings. - 'custom': Uses custom parsing logic. Requires providing <code>parse_func</code> parameter as a custom parsing function.</p> <code>'json'</code> <code>parse_func</code> <code>Optional[Callable]</code> <p>The function to parse the content, only valid when parse_mode is 'custom'.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to the parsing function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>The parsed content as a dictionary.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If parse_mode is invalid or if parse_func is not provided when parse_mode is 'custom'.</p> Source code in <code>evoagentx/models/base_model.py</code> <pre><code>@classmethod\ndef get_content_data(cls, content: str, parse_mode: str = \"json\", parse_func: Optional[Callable] = None, **kwargs) -&gt; dict:\n    \"\"\"Parses LLM-generated content into a dictionary.\n\n    This method takes content from an LLM response and converts it to a structured\n    dictionary based on the specified parsing mode.\n\n    Args:\n        content: The content to parse.\n        parse_mode: The mode to parse the content. Must be one of:\n            - 'str': Assigns the raw text content to all attributes of the parser. \n            - 'json': Extracts and parses JSON objects from LLM output. It will return a dictionary parsed from the first valid JSON string.\n            - 'xml': Parses content using XML tags. It will return a dictionary parsed from the XML tags.\n            - 'title': Parses content with Markdown-style headings.\n            - 'custom': Uses custom parsing logic. Requires providing `parse_func` parameter as a custom parsing function.\n        parse_func: The function to parse the content, only valid when parse_mode is 'custom'.\n        **kwargs (Any): Additional arguments passed to the parsing function.\n\n    Returns:\n        The parsed content as a dictionary.\n\n    Raises:\n        ValueError: If parse_mode is invalid or if parse_func is not provided when parse_mode is 'custom'.\n    \"\"\"\n    attrs = cls.get_attrs()\n    if len(attrs) &lt;= 0:\n        return {} \n\n    if parse_mode == \"str\":\n        parse_func = cls._parse_str_content\n    elif parse_mode == \"json\":\n        parse_func = cls._parse_json_content\n    elif parse_mode == \"xml\":\n        parse_func = cls._parse_xml_content\n    elif parse_mode == \"title\":\n        parse_func = cls._parse_title_content\n    elif parse_mode == \"custom\":\n        if parse_func is None:\n            raise ValueError(\"`parse_func` must be provided when `parse_mode` is 'custom'.\")\n        # obtain the function inputs\n        signature = inspect.signature(parse_func)\n        if \"content\" not in signature.parameters:\n            raise ValueError(\"`parse_func` must have an input argument `content`.\")\n\n        func_args = {}\n        func_args[\"content\"] = content\n        for param_name, param in signature.parameters.items():\n            if param_name == \"content\":\n                continue  # Already set\n            if param_name in kwargs:\n                func_args[param_name] = kwargs[param_name]\n        data = parse_func(**func_args)\n        if not isinstance(data, dict):\n            raise ValueError(f\"The output of `parse_func` must be a dictionary, but found {type(data)}.\")\n        return data\n    else:\n        raise ValueError(f\"Invalid value '{parse_mode}' detected for `parse_mode`. Available choices: {PARSER_VALID_MODE}\")\n    data = parse_func(content=content, **kwargs)\n    return data\n</code></pre>"},{"location":"api/models/#evoagentx.models.LLMOutputParser.parse","title":"parse  <code>classmethod</code>","text":"<pre><code>parse(content: str, parse_mode: str = 'json', parse_func: Optional[Callable] = None, **kwargs) -&gt; LLMOutputParser\n</code></pre> <p>Parses LLM-generated text into a structured parser instance.</p> <p>This is the main method for creating parser instances from LLM output.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The text generated by the LLM.</p> required <code>parse_mode</code> <code>str</code> <p>The mode to parse the content, must be one of: - 'str': Assigns the raw text content to all attributes of the parser.  - 'json': Extracts and parses JSON objects from LLM output. Uses the first valid JSON string to create an instance of LLMOutputParser. - 'xml': Parses content using XML tags. Uses the XML tags to create an instance of LLMOutputParser. - 'title': Parses content with Markdown-style headings. Uses the Markdown-style headings to create an instance of LLMOutputParser. The default title format is \"## {title}\", you can change it by providing <code>title_format</code> parameter, which should be a string that contains <code>{title}</code> placeholder.  - 'custom': Uses custom parsing logic. Requires providing <code>parse_func</code> parameter as a custom parsing function. The <code>parse_func</code> must have a parameter named <code>content</code> and return a dictionary where the keys are the attribute names and the values are the parsed data. </p> <code>'json'</code> <code>parse_func</code> <code>Optional[Callable]</code> <p>The function to parse the content, only valid when <code>parse_mode</code> is 'custom'.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to parsing functions, such as: - <code>title_format</code> for <code>parse_mode=\"title\"</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>LLMOutputParser</code> <p>An instance of LLMOutputParser containing the parsed data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If parse_mode is invalid or if content is not a string.</p> Source code in <code>evoagentx/models/base_model.py</code> <pre><code>@classmethod\ndef parse(cls, content: str, parse_mode: str = \"json\", parse_func: Optional[Callable] = None, **kwargs) -&gt; \"LLMOutputParser\":\n    \"\"\"Parses LLM-generated text into a structured parser instance.\n\n    This is the main method for creating parser instances from LLM output.\n\n    Args:\n        content: The text generated by the LLM.\n        parse_mode: The mode to parse the content, must be one of:\n            - 'str': Assigns the raw text content to all attributes of the parser. \n            - 'json': Extracts and parses JSON objects from LLM output. Uses the first valid JSON string to create an instance of LLMOutputParser.\n            - 'xml': Parses content using XML tags. Uses the XML tags to create an instance of LLMOutputParser.\n            - 'title': Parses content with Markdown-style headings. Uses the Markdown-style headings to create an instance of LLMOutputParser. The default title format is \"## {title}\", you can change it by providing `title_format` parameter, which should be a string that contains `{title}` placeholder. \n            - 'custom': Uses custom parsing logic. Requires providing `parse_func` parameter as a custom parsing function. The `parse_func` must have a parameter named `content` and return a dictionary where the keys are the attribute names and the values are the parsed data. \n        parse_func: The function to parse the content, only valid when `parse_mode` is 'custom'.\n        **kwargs (Any): Additional arguments passed to parsing functions, such as:\n            - `title_format` for `parse_mode=\"title\"`.\n\n    Returns:\n        An instance of LLMOutputParser containing the parsed data.\n\n    Raises:\n        ValueError: If parse_mode is invalid or if content is not a string.\n    \"\"\"\n    if parse_mode not in PARSER_VALID_MODE:\n        raise ValueError(f\"'{parse_mode}' is an invalid value for `parse_mode`. Available choices: {PARSER_VALID_MODE}.\")\n    if not isinstance(content, str):\n        raise ValueError(f\"The input to {cls.__name__}.parse should be a str, but found {type(content)}.\")\n    data = cls.get_content_data(content=content, parse_mode=parse_mode, parse_func=parse_func, **kwargs)\n    data.update({\"content\": content})\n    parser = cls.from_dict(data, **kwargs)\n    # parser.content = content\n    return parser\n</code></pre>"},{"location":"api/models/#evoagentx.models.LLMOutputParser.__str__","title":"__str__","text":"<pre><code>__str__() -&gt; str\n</code></pre> <p>Returns a string representation of the parser.</p> Source code in <code>evoagentx/models/base_model.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"\n    Returns a string representation of the parser.\n    \"\"\"\n    return self.to_str()\n</code></pre>"},{"location":"api/models/#evoagentx.models.LLMOutputParser.to_str","title":"to_str","text":"<pre><code>to_str(**kwargs) -&gt; str\n</code></pre> <p>Converts the parser to a string.</p> Source code in <code>evoagentx/models/base_model.py</code> <pre><code>def to_str(self, **kwargs) -&gt; str:\n    \"\"\"\n    Converts the parser to a string.\n    \"\"\"\n    return self.content\n</code></pre>"},{"location":"api/models/#evoagentx.models.LLMOutputParser.get_structured_data","title":"get_structured_data","text":"<pre><code>get_structured_data() -&gt; dict\n</code></pre> <p>Extracts structured data from the parser.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing only the defined attributes and their values,</p> <code>dict</code> <p>excluding metadata like class_name.</p> Source code in <code>evoagentx/models/base_model.py</code> <pre><code>def get_structured_data(self) -&gt; dict:\n    \"\"\"Extracts structured data from the parser.\n\n    Returns:\n        A dictionary containing only the defined attributes and their values,\n        excluding metadata like class_name.\n    \"\"\"\n    attrs = type(self).get_attrs()\n    data = self.to_dict(ignore=[\"class_name\"])\n    # structured_data = {attr: data[attr] for attr in attrs}\n    structured_data = {key: value for key, value in data.items() if key in attrs}\n    return structured_data\n</code></pre>"},{"location":"api/models/#evoagentx.models.BaseConfig","title":"BaseConfig","text":"<pre><code>BaseConfig(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModule</code></p> <p>Base configuration class that serves as parent for all configuration classes.</p> <p>A config should inherit BaseConfig and specify the attributes and their types.  Otherwise this will be an empty config.</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/models/#evoagentx.models.BaseConfig.save","title":"save","text":"<pre><code>save(path: str, **kwargs) -&gt; str\n</code></pre> <p>Save configuration to the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The file path to save the configuration</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to save_module method</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The path where the file was saved</p> Source code in <code>evoagentx/core/base_config.py</code> <pre><code>def save(self, path: str, **kwargs)-&gt; str:\n\n    \"\"\"Save configuration to the specified path.\n\n    Args:\n        path: The file path to save the configuration\n        **kwargs (Any): Additional keyword arguments passed to save_module method\n\n    Returns:\n        str: The path where the file was saved\n    \"\"\"\n    return super().save_module(path, **kwargs)\n</code></pre>"},{"location":"api/models/#evoagentx.models.BaseConfig.get_config_params","title":"get_config_params","text":"<pre><code>get_config_params() -&gt; List[str]\n</code></pre> <p>Get a list of configuration parameters.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of configuration parameter names, excluding 'class_name'</p> Source code in <code>evoagentx/core/base_config.py</code> <pre><code>def get_config_params(self) -&gt; List[str]:\n    \"\"\"Get a list of configuration parameters.\n\n    Returns:\n        List[str]: List of configuration parameter names, excluding 'class_name'\n    \"\"\"\n    config_params = list(type(self).model_fields.keys())\n    config_params.remove(\"class_name\")\n    return config_params\n</code></pre>"},{"location":"api/models/#evoagentx.models.BaseConfig.get_set_params","title":"get_set_params","text":"<pre><code>get_set_params(ignore: List[str] = []) -&gt; dict\n</code></pre> <p>Get a dictionary of explicitly set parameters.</p> <p>Parameters:</p> Name Type Description Default <code>ignore</code> <code>List[str]</code> <p>List of parameter names to ignore</p> <code>[]</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary of explicitly set parameters, excluding 'class_name' and ignored parameters</p> Source code in <code>evoagentx/core/base_config.py</code> <pre><code>def get_set_params(self, ignore: List[str] = []) -&gt; dict:\n    \"\"\"Get a dictionary of explicitly set parameters.\n\n    Args:\n        ignore: List of parameter names to ignore\n\n    Returns:\n        dict: Dictionary of explicitly set parameters, excluding 'class_name' and ignored parameters\n    \"\"\"\n    explicitly_set_fields = {field: getattr(self, field) for field in self.model_fields_set}\n    if self.kwargs:\n        explicitly_set_fields.update(self.kwargs)\n    for field in ignore:\n        explicitly_set_fields.pop(field, None)\n    explicitly_set_fields.pop(\"class_name\", None)\n    return explicitly_set_fields\n</code></pre>"},{"location":"api/models/#evoagentx.models.LiteLLM","title":"LiteLLM","text":"<pre><code>LiteLLM(config: LLMConfig, **kwargs)\n</code></pre> <p>               Bases: <code>OpenAILLM</code></p> Source code in <code>evoagentx/models/base_model.py</code> <pre><code>def __init__(self, config: LLMConfig, **kwargs):\n    \"\"\"Initializes the LLM with configuration.\n\n    Args:\n        config: Configuration object for the LLM.\n        **kwargs (Any): Additional keyword arguments.\n    \"\"\"\n    self.config = config\n    self.kwargs = kwargs\n    self.init_model()\n</code></pre>"},{"location":"api/models/#evoagentx.models.LiteLLM.init_model","title":"init_model","text":"<pre><code>init_model()\n</code></pre> <p>Initialize the model based on the configuration.</p> Source code in <code>evoagentx/models/litellm_model.py</code> <pre><code>def init_model(self):\n    \"\"\"\n    Initialize the model based on the configuration.\n    \"\"\"\n    # Check if llm_type is correct\n    if self.config.llm_type != \"LiteLLM\":\n        raise ValueError(\"llm_type must be 'LiteLLM'\")\n\n    # Set model and extract the company name\n    self.model = self.config.model\n    company = self.model.split(\"/\")[0] if \"/\" in self.model else \"openai\"\n\n    # Set environment variables based on the company\n    if company == \"openai\":\n        if not self.config.openai_key:\n            raise ValueError(\"OpenAI API key is required for OpenAI models. You should set `openai_key` in LiteLLMConfig\")\n        os.environ[\"OPENAI_API_KEY\"] = self.config.openai_key\n    elif company == \"deepseek\":\n        if not self.config.deepseek_key:\n            raise ValueError(\"DeepSeek API key is required for DeepSeek models. You should set `deepseek_key` in LiteLLMConfig\")\n        os.environ[\"DEEPSEEK_API_KEY\"] = self.config.deepseek_key\n    elif company == \"anthropic\":\n        if not self.config.anthropic_key:\n            raise ValueError(\"Anthropic API key is required for Anthropic models. You should set `anthropic_key` in LiteLLMConfig\")\n        os.environ[\"ANTHROPIC_API_KEY\"] = self.config.anthropic_key\n    else:\n        raise ValueError(f\"Unsupported company: {company}\")\n\n    self._default_ignore_fields = [\"llm_type\", \"output_response\", \"openai_key\", \"deepseek_key\", \"anthropic_key\"] # parameters in LiteLLMConfig that are not LiteLLM models' input parameters\n</code></pre>"},{"location":"api/models/#evoagentx.models.LiteLLM.single_generate","title":"single_generate","text":"<pre><code>single_generate(messages: List[dict], **kwargs) -&gt; str\n</code></pre> <p>Generate a single response using the completion function.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[dict]</code> <p>A list of dictionaries representing the conversation history.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional parameters to be passed to the <code>completion</code> function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string containing the model's response.</p> Source code in <code>evoagentx/models/litellm_model.py</code> <pre><code>@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(5))\ndef single_generate(self, messages: List[dict], **kwargs) -&gt; str:\n\n    \"\"\"\n    Generate a single response using the completion function.\n\n    Args: \n        messages (List[dict]): A list of dictionaries representing the conversation history.\n        **kwargs (Any): Additional parameters to be passed to the `completion` function.\n\n    Returns: \n        str: A string containing the model's response.\n    \"\"\"\n    stream = kwargs[\"stream\"] if \"stream\" in kwargs else self.config.stream\n    output_response = kwargs[\"output_response\"] if \"output_response\" in kwargs else self.config.output_response\n\n    try:\n        completion_params = self.get_completion_params(**kwargs)\n        response = completion(messages=messages, **completion_params)\n        if stream:\n            output = self.get_stream_output(response, output_response=output_response)\n            cost = self._stream_cost(messages=messages, output=output)\n        else:\n            output: str = self.get_completion_output(response=response, output_response=output_response)\n            cost = self._completion_cost(response=response)\n        self._update_cost(cost=cost)\n    except Exception as e:\n        raise RuntimeError(f\"Error during single_generate: {str(e)}\")\n\n    return output\n</code></pre>"},{"location":"api/models/#evoagentx.models.LiteLLM.batch_generate","title":"batch_generate","text":"<pre><code>batch_generate(batch_messages: List[List[dict]], **kwargs) -&gt; List[str]\n</code></pre> <p>Generate responses for a batch of messages.</p> <p>Parameters:</p> Name Type Description Default <code>batch_messages</code> <code>List[List[dict]]</code> <p>A list of message lists, where each sublist represents a conversation.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional parameters to be passed to the <code>completion</code> function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of responses for each conversation.</p> Source code in <code>evoagentx/models/litellm_model.py</code> <pre><code>def batch_generate(self, batch_messages: List[List[dict]], **kwargs) -&gt; List[str]:\n    \"\"\"\n    Generate responses for a batch of messages.\n\n    Args: \n        batch_messages (List[List[dict]]): A list of message lists, where each sublist represents a conversation.\n        **kwargs (Any): Additional parameters to be passed to the `completion` function.\n\n    Returns: \n        List[str]: A list of responses for each conversation.\n    \"\"\"\n    results = []\n    for messages in batch_messages:\n        response = self.single_generate(messages, **kwargs)\n        results.append(response)\n    return results\n</code></pre>"},{"location":"api/models/#evoagentx.models.LiteLLM.single_generate_async","title":"single_generate_async  <code>async</code>","text":"<pre><code>single_generate_async(messages: List[dict], **kwargs) -&gt; str\n</code></pre> <p>Generate a single response using the async completion function.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[dict]</code> <p>A list of dictionaries representing the conversation history.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional parameters to be passed to the <code>completion</code> function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string containing the model's response.</p> Source code in <code>evoagentx/models/litellm_model.py</code> <pre><code>async def single_generate_async(self, messages: List[dict], **kwargs) -&gt; str:\n    \"\"\"\n    Generate a single response using the async completion function.\n\n    Args: \n        messages (List[dict]): A list of dictionaries representing the conversation history.\n        **kwargs (Any): Additional parameters to be passed to the `completion` function.\n\n    Returns: \n        str: A string containing the model's response.\n    \"\"\"\n    stream = kwargs[\"stream\"] if \"stream\" in kwargs else self.config.stream\n    output_response = kwargs[\"output_response\"] if \"output_response\" in kwargs else self.config.output_response\n\n    try:\n        completion_params = self.get_completion_params(**kwargs)\n        response = await acompletion(messages=messages, **completion_params)\n        if stream:\n            if hasattr(response, \"__aiter__\"):\n                output = await self.get_stream_output_async(response, output_response=output_response)\n            else:\n                output = self.get_stream_output(response, output_response=output_response)\n            cost = self._stream_cost(messages=messages, output=output)\n        else:\n            output: str = self.get_completion_output(response=response, output_response=output_response)\n            cost = self._completion_cost(response=response)\n        self._update_cost(cost=cost)\n    except Exception as e:\n        raise RuntimeError(f\"Error during single_generate_async: {str(e)}\")\n\n    return output\n</code></pre>"},{"location":"api/models/#evoagentx.models.LiteLLM.completion_cost","title":"completion_cost","text":"<pre><code>completion_cost(completion_response=None, prompt='', messages: List = [], completion='', total_time=0.0, call_type='completion', size=None, quality=None, n=None) -&gt; float\n</code></pre> <p>Calculate the cost of a given completion or other supported tasks.</p> <p>Parameters:</p> Name Type Description Default <code>completion_response</code> <code>dict</code> <p>The response received from a LiteLLM completion request.</p> <code>None</code> <code>prompt</code> <code>str</code> <p>Input prompt text.</p> <code>''</code> <code>messages</code> <code>list</code> <p>Conversation history.</p> <code>[]</code> <code>completion</code> <code>str</code> <p>Output text from the LLM.</p> <code>''</code> <code>total_time</code> <code>float</code> <p>Total time used for request.</p> <code>0.0</code> <code>call_type</code> <code>str</code> <p>Type of request (e.g., \"completion\", \"image_generation\").</p> <code>'completion'</code> <code>size</code> <code>str</code> <p>Image size for image generation.</p> <code>None</code> <code>quality</code> <code>str</code> <p>Image quality for image generation.</p> <code>None</code> <code>n</code> <code>int</code> <p>Number of generated images.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The cost in USD.</p> Source code in <code>evoagentx/models/litellm_model.py</code> <pre><code>def completion_cost(\n    self,\n    completion_response=None,\n    prompt=\"\",\n    messages: List = [],\n    completion=\"\",\n    total_time=0.0,\n    call_type=\"completion\",\n    size=None,\n    quality=None,\n    n=None,\n) -&gt; float:\n    \"\"\"\n    Calculate the cost of a given completion or other supported tasks.\n\n    Args:\n        completion_response (dict): The response received from a LiteLLM completion request.\n        prompt (str): Input prompt text.\n        messages (list): Conversation history.\n        completion (str): Output text from the LLM.\n        total_time (float): Total time used for request.\n        call_type (str): Type of request (e.g., \"completion\", \"image_generation\").\n        size (str): Image size for image generation.\n        quality (str): Image quality for image generation.\n        n (int): Number of generated images.\n\n    Returns:\n        float: The cost in USD.\n    \"\"\"\n    try:\n        # Default parameters\n        prompt_tokens = 0\n        completion_tokens = 0\n        model = self.model  # Use the class model by default\n\n        # Handle completion response\n        if completion_response:\n            prompt_tokens = completion_response.get(\"usage\", {}).get(\"prompt_tokens\", 0)\n            completion_tokens = completion_response.get(\"usage\", {}).get(\"completion_tokens\", 0)\n            model = completion_response.get(\"model\", model)\n            size = completion_response.get(\"_hidden_params\", {}).get(\"optional_params\", {}).get(\"size\", size)\n            quality = completion_response.get(\"_hidden_params\", {}).get(\"optional_params\", {}).get(\"quality\", quality)\n            n = completion_response.get(\"_hidden_params\", {}).get(\"optional_params\", {}).get(\"n\", n)\n\n        # Handle manual token counting\n        else:\n            if messages:\n                prompt_tokens = token_counter(model=model, messages=messages)\n            elif prompt:\n                prompt_tokens = token_counter(model=model, text=prompt)\n            completion_tokens = token_counter(model=model, text=completion)\n\n        # Ensure model is valid\n        if not model:\n            raise ValueError(\"Model is not defined for cost calculation.\")\n\n        # Image generation cost calculation\n        if call_type in [\"image_generation\", \"aimage_generation\"]:\n            if size and \"x\" in size and \"-x-\" not in size:\n                size = size.replace(\"x\", \"-x-\")\n            height, width = map(int, size.split(\"-x-\"))\n            return (\n                litellm.model_cost[f\"{size}/{model}\"][\"input_cost_per_pixel\"]\n                * height * width * (n or 1)\n            )\n\n        # Regular completion cost calculation\n        prompt_cost, completion_cost = cost_per_token(\n            model=model,\n            prompt_tokens=prompt_tokens,\n            completion_tokens=completion_tokens,\n            response_time_ms=total_time,\n        )\n        return prompt_cost + completion_cost\n    except Exception as e:\n        print(f\"Error calculating cost: {e}\")\n        return 0.0\n</code></pre>"},{"location":"api/models/#evoagentx.models.BaseLLM","title":"BaseLLM","text":"<pre><code>BaseLLM(config: LLMConfig, **kwargs)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for Large Language Model implementations.</p> <p>This class defines the interface that all LLM implementations must follow, providing methods for generating text, formatting messages, and parsing output.</p> <p>Attributes:</p> Name Type Description <code>config</code> <p>Configuration for the LLM.</p> <code>kwargs</code> <p>Additional keyword arguments provided during initialization.</p> <p>Initializes the LLM with configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>LLMConfig</code> <p>Configuration object for the LLM.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>evoagentx/models/base_model.py</code> <pre><code>def __init__(self, config: LLMConfig, **kwargs):\n    \"\"\"Initializes the LLM with configuration.\n\n    Args:\n        config: Configuration object for the LLM.\n        **kwargs (Any): Additional keyword arguments.\n    \"\"\"\n    self.config = config\n    self.kwargs = kwargs\n    self.init_model()\n</code></pre>"},{"location":"api/models/#evoagentx.models.BaseLLM.init_model","title":"init_model  <code>abstractmethod</code>","text":"<pre><code>init_model()\n</code></pre> <p>Initializes the underlying model.</p> <p>This method should be implemented by subclasses to set up the actual LLM.</p> Source code in <code>evoagentx/models/base_model.py</code> <pre><code>@abstractmethod\ndef init_model(self):\n    \"\"\"Initializes the underlying model.\n\n    This method should be implemented by subclasses to set up the actual LLM.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/models/#evoagentx.models.BaseLLM.__deepcopy__","title":"__deepcopy__","text":"<pre><code>__deepcopy__(memo) -&gt; BaseLLM\n</code></pre> <p>Handles deep copying of the LLM instance.</p> <p>Returns the same instance when deepcopy is called, as LLM instances often cannot be meaningfully deep-copied.</p> <p>Parameters:</p> Name Type Description Default <code>memo</code> <code>Dict[int, Any]</code> <p>Memo dictionary used by the deepcopy process.</p> required <p>Returns:</p> Type Description <code>BaseLLM</code> <p>The same LLM instance.</p> Source code in <code>evoagentx/models/base_model.py</code> <pre><code>def __deepcopy__(self, memo) -&gt; \"BaseLLM\":\n    \"\"\"Handles deep copying of the LLM instance.\n\n    Returns the same instance when deepcopy is called, as LLM instances\n    often cannot be meaningfully deep-copied.\n\n    Args:\n        memo (Dict[int, Any]): Memo dictionary used by the deepcopy process.\n\n    Returns:\n        The same LLM instance.\n    \"\"\"\n    # return the same instance when deepcopy\n    memo[id(self)] = self\n    return self\n</code></pre>"},{"location":"api/models/#evoagentx.models.BaseLLM.formulate_messages","title":"formulate_messages  <code>abstractmethod</code>","text":"<pre><code>formulate_messages(prompts: List[str], system_messages: Optional[List[str]] = None) -&gt; List[List[dict]]\n</code></pre> <p>Converts input prompts into the chat format compatible with different LLMs.</p> <p>Parameters:</p> Name Type Description Default <code>prompts</code> <code>List[str]</code> <p>A list of user prompts that need to be converted.</p> required <code>system_messages</code> <code>Optional[List[str]]</code> <p>An optional list of system messages that provide instructions or context to the model.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[List[dict]]</code> <p>A list of message lists, where each inner list contains messages in the chat format required by LLMs.</p> Source code in <code>evoagentx/models/base_model.py</code> <pre><code>@abstractmethod\ndef formulate_messages(self, prompts: List[str], system_messages: Optional[List[str]] = None) -&gt; List[List[dict]]:\n    \"\"\"Converts input prompts into the chat format compatible with different LLMs.\n\n    Args:\n        prompts: A list of user prompts that need to be converted.\n        system_messages: An optional list of system messages that provide instructions or context to the model.\n\n    Returns:\n        A list of message lists, where each inner list contains messages in the chat format required by LLMs. \n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/models/#evoagentx.models.BaseLLM.single_generate","title":"single_generate  <code>abstractmethod</code>","text":"<pre><code>single_generate(messages: List[dict], **kwargs) -&gt; str\n</code></pre> <p>Generates LLM output for a single set of messages.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[dict]</code> <p>The input messages to the LLM in chat format.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for generation settings.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The generated output text from the LLM.</p> Source code in <code>evoagentx/models/base_model.py</code> <pre><code>@abstractmethod\ndef single_generate(self, messages: List[dict], **kwargs) -&gt; str:\n    \"\"\"Generates LLM output for a single set of messages.\n\n    Args:\n        messages: The input messages to the LLM in chat format.\n        **kwargs (Any): Additional keyword arguments for generation settings.\n\n    Returns:\n        The generated output text from the LLM.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/models/#evoagentx.models.BaseLLM.batch_generate","title":"batch_generate  <code>abstractmethod</code>","text":"<pre><code>batch_generate(batch_messages: List[List[dict]], **kwargs) -&gt; List[str]\n</code></pre> <p>Generates outputs for a batch of message sets.</p> <p>Parameters:</p> Name Type Description Default <code>batch_messages</code> <code>List[List[dict]]</code> <p>A list of message lists, where each inner list contains messages for a single generation.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for generation settings.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of generated outputs from the LLM, one for each input message set.</p> Source code in <code>evoagentx/models/base_model.py</code> <pre><code>@abstractmethod\ndef batch_generate(self, batch_messages: List[List[dict]], **kwargs) -&gt; List[str]:\n    \"\"\"Generates outputs for a batch of message sets.\n\n    Args: \n        batch_messages: A list of message lists, where each inner list contains messages for a single generation.\n        **kwargs (Any): Additional keyword arguments for generation settings.\n\n    Returns:\n        A list of generated outputs from the LLM, one for each input message set.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/models/#evoagentx.models.BaseLLM.single_generate_async","title":"single_generate_async  <code>async</code>","text":"<pre><code>single_generate_async(messages: List[dict], **kwargs) -&gt; str\n</code></pre> <p>Asynchronously generates LLM output for a single set of messages.</p> <p>This default implementation wraps the synchronous method in an async executor. Subclasses should override this for true async implementation if supported.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[dict]</code> <p>The input messages to the LLM in chat format.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for generation settings.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The generated output text from the LLM.</p> Source code in <code>evoagentx/models/base_model.py</code> <pre><code>async def single_generate_async(self, messages: List[dict], **kwargs) -&gt; str:\n    \"\"\"Asynchronously generates LLM output for a single set of messages.\n\n    This default implementation wraps the synchronous method in an async executor.\n    Subclasses should override this for true async implementation if supported.\n\n    Args:\n        messages: The input messages to the LLM in chat format.\n        **kwargs (Any): Additional keyword arguments for generation settings.\n\n    Returns:\n        The generated output text from the LLM.\n    \"\"\"\n    # Default implementation for backward compatibility\n    loop = asyncio.get_event_loop()\n    result = await loop.run_in_executor(None, self.single_generate, messages, **kwargs)\n    return result\n</code></pre>"},{"location":"api/models/#evoagentx.models.BaseLLM.batch_generate_async","title":"batch_generate_async  <code>async</code>","text":"<pre><code>batch_generate_async(batch_messages: List[List[dict]], **kwargs) -&gt; List[str]\n</code></pre> <p>Asynchronously generates outputs for a batch of message sets.</p> <p>This default implementation runs each generation as a separate async task. Subclasses should override this for more efficient async batching if supported.</p> <p>Parameters:</p> Name Type Description Default <code>batch_messages</code> <code>List[List[dict]]</code> <p>A list of message lists, where each inner list contains messages for a single generation.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for generation settings.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of generated outputs from the LLM, one for each input message set.</p> Source code in <code>evoagentx/models/base_model.py</code> <pre><code>async def batch_generate_async(self, batch_messages: List[List[dict]], **kwargs) -&gt; List[str]:\n    \"\"\"Asynchronously generates outputs for a batch of message sets.\n\n    This default implementation runs each generation as a separate async task.\n    Subclasses should override this for more efficient async batching if supported.\n\n    Args: \n        batch_messages: A list of message lists, where each inner list contains messages for a single generation.\n        **kwargs (Any): Additional keyword arguments for generation settings.\n\n    Returns:\n        A list of generated outputs from the LLM, one for each input message set.\n    \"\"\"\n    # Default implementation for backward compatibility\n    tasks = [self.single_generate_async(messages, **kwargs) for messages in batch_messages]\n    return await asyncio.gather(*tasks)\n</code></pre>"},{"location":"api/models/#evoagentx.models.BaseLLM.parse_generated_text","title":"parse_generated_text","text":"<pre><code>parse_generated_text(text: str, parser: Optional[Type[LLMOutputParser]] = None, parse_mode: Optional[str] = 'json', parse_func: Optional[Callable] = None, **kwargs) -&gt; LLMOutputParser\n</code></pre> <p>Parses generated text into a structured output using a parser.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text generated by the LLM.</p> required <code>parser</code> <code>Optional[Type[LLMOutputParser]]</code> <p>An LLMOutputParser class to use for parsing. If None, the default LLMOutputParser is used.</p> <code>None</code> <code>parse_mode</code> <code>Optional[str]</code> <p>The mode to use for parsing, must be the <code>parse_mode</code> supported by the <code>parser</code>. </p> <code>'json'</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to the parser.</p> <code>{}</code> <p>Returns:</p> Type Description <code>LLMOutputParser</code> <p>An LLMOutputParser instance containing the parsed data.</p> Source code in <code>evoagentx/models/base_model.py</code> <pre><code>def parse_generated_text(self, text: str, parser: Optional[Type[LLMOutputParser]]=None, parse_mode: Optional[str] = \"json\", parse_func: Optional[Callable] = None, **kwargs) -&gt; LLMOutputParser:\n    \"\"\"Parses generated text into a structured output using a parser.\n\n    Args: \n        text: The text generated by the LLM.\n        parser: An LLMOutputParser class to use for parsing. If None, the default LLMOutputParser is used.\n        parse_mode: The mode to use for parsing, must be the `parse_mode` supported by the `parser`. \n        **kwargs (Any): Additional arguments passed to the parser.\n\n    Returns:\n        An LLMOutputParser instance containing the parsed data.\n    \"\"\"\n    if not parser:\n        parser = LLMOutputParser\n    return parser.parse(text, parse_mode=parse_mode, parse_func=parse_func)\n</code></pre>"},{"location":"api/models/#evoagentx.models.BaseLLM.parse_generated_texts","title":"parse_generated_texts","text":"<pre><code>parse_generated_texts(texts: List[str], parser: Optional[Type[LLMOutputParser]] = None, parse_mode: Optional[str] = 'json', parse_func: Optional[Callable] = None, **kwargs) -&gt; List[LLMOutputParser]\n</code></pre> <p>Parses multiple generated texts into structured outputs.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>List[str]</code> <p>A list of texts generated by the LLM.</p> required <code>parser</code> <code>Optional[Type[LLMOutputParser]]</code> <p>An LLMOutputParser class to use for parsing.</p> <code>None</code> <code>parse_mode</code> <code>Optional[str]</code> <p>The mode to use for parsing, must be the <code>parse_mode</code> supported by the <code>parser</code>. </p> <code>'json'</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to the parser.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[LLMOutputParser]</code> <p>A list of LLMOutputParser instances containing the parsed data.</p> Source code in <code>evoagentx/models/base_model.py</code> <pre><code>def parse_generated_texts(self, texts: List[str], parser: Optional[Type[LLMOutputParser]]=None, parse_mode: Optional[str] = \"json\", parse_func: Optional[Callable] = None, **kwargs) -&gt; List[LLMOutputParser]:\n    \"\"\"Parses multiple generated texts into structured outputs.\n\n    Args:\n        texts: A list of texts generated by the LLM.\n        parser: An LLMOutputParser class to use for parsing.\n        parse_mode: The mode to use for parsing, must be the `parse_mode` supported by the `parser`. \n        **kwargs (Any): Additional arguments passed to the parser.\n\n    Returns:\n        A list of LLMOutputParser instances containing the parsed data.\n    \"\"\"\n    parsed_results = [self.parse_generated_text(text=text, parser=parser, parse_mode=parse_mode, parse_func=parse_func, **kwargs) for text in texts]\n    return parsed_results\n</code></pre>"},{"location":"api/models/#evoagentx.models.BaseLLM.generate","title":"generate","text":"<pre><code>generate(prompt: Optional[Union[str, List[str]]] = None, system_message: Optional[Union[str, List[str]]] = None, messages: Optional[Union[List[dict], List[List[dict]]]] = None, parser: Optional[Type[LLMOutputParser]] = None, parse_mode: Optional[str] = 'json', parse_func: Optional[Callable] = None, **kwargs) -&gt; Union[LLMOutputParser, List[LLMOutputParser]]\n</code></pre> <p>Generates LLM output(s) and parses the result(s).</p> <p>This is the main method for generating text with the LLM. It handles both single and batch generation, and automatically parses the outputs.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>Optional[Union[str, List[str]]]</code> <p>Input prompt(s) to the LLM.</p> <code>None</code> <code>system_message</code> <code>Optional[Union[str, List[str]]]</code> <p>System message(s) for the LLM.</p> <code>None</code> <code>messages</code> <code>Optional[Union[List[dict], List[List[dict]]]]</code> <p>Chat message(s) for the LLM, already in the required format (either <code>prompt</code> or <code>messages</code> must be provided).</p> <code>None</code> <code>parser</code> <code>Optional[Type[LLMOutputParser]]</code> <p>Parser class to use for processing the output.</p> <code>None</code> <code>parse_mode</code> <code>Optional[str]</code> <p>The mode to use for parsing, must be the <code>parse_mode</code> supported by the <code>parser</code>. </p> <code>'json'</code> <code>**kwargs</code> <code>Any</code> <p>Additional generation configuration parameters.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[LLMOutputParser, List[LLMOutputParser]]</code> <p>For single generation: An LLMOutputParser instance.</p> <code>Union[LLMOutputParser, List[LLMOutputParser]]</code> <p>For batch generation: A list of LLMOutputParser instances.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input format is invalid.</p> Note <p>Either prompt or messages must be provided. If both or neither is provided, an error will be raised.</p> Source code in <code>evoagentx/models/base_model.py</code> <pre><code>def generate(\n    self,\n    prompt: Optional[Union[str, List[str]]] = None,\n    system_message: Optional[Union[str, List[str]]] = None,\n    messages: Optional[Union[List[dict],List[List[dict]]]] = None,\n    parser: Optional[Type[LLMOutputParser]] = None,\n    parse_mode: Optional[str] = \"json\", \n    parse_func: Optional[Callable] = None,\n    **kwargs\n) -&gt; Union[LLMOutputParser, List[LLMOutputParser]]:\n    \"\"\"Generates LLM output(s) and parses the result(s).\n\n    This is the main method for generating text with the LLM. It handles both\n    single and batch generation, and automatically parses the outputs.\n\n    Args:\n        prompt: Input prompt(s) to the LLM.\n        system_message: System message(s) for the LLM.\n        messages: Chat message(s) for the LLM, already in the required format (either `prompt` or `messages` must be provided).\n        parser: Parser class to use for processing the output.\n        parse_mode: The mode to use for parsing, must be the `parse_mode` supported by the `parser`. \n        **kwargs (Any): Additional generation configuration parameters.\n\n    Returns:\n        For single generation: An LLMOutputParser instance.\n        For batch generation: A list of LLMOutputParser instances.\n\n    Raises:\n        ValueError: If the input format is invalid.\n\n    Note:\n        Either prompt or messages must be provided. If both or neither is provided,\n        an error will be raised.\n    \"\"\"\n    prepared_messages, single_generate = self._prepare_messages(prompt, system_message, messages)\n    if not prepared_messages:  # Handle empty messages case\n        return []\n\n    generated_texts = self.batch_generate(batch_messages=prepared_messages, **kwargs)\n    parsed_outputs = self.parse_generated_texts(texts=generated_texts, parser=parser, parse_mode=parse_mode, parse_func=parse_func, **kwargs)\n    return parsed_outputs[0] if single_generate else parsed_outputs\n</code></pre>"},{"location":"api/models/#evoagentx.models.BaseLLM.async_generate","title":"async_generate  <code>async</code>","text":"<pre><code>async_generate(prompt: Optional[Union[str, List[str]]] = None, system_message: Optional[Union[str, List[str]]] = None, messages: Optional[Union[List[dict], List[List[dict]]]] = None, parser: Optional[Type[LLMOutputParser]] = None, parse_mode: Optional[str] = 'json', parse_func: Optional[Callable] = None, **kwargs) -&gt; Union[LLMOutputParser, List[LLMOutputParser]]\n</code></pre> <p>Asynchronously generates LLM output(s) and parses the result(s).</p> <p>This is the async version of the generate method. It works identically but performs the generation asynchronously.</p> Source code in <code>evoagentx/models/base_model.py</code> <pre><code>async def async_generate(\n    self,\n    prompt: Optional[Union[str, List[str]]] = None,\n    system_message: Optional[Union[str, List[str]]] = None,\n    messages: Optional[Union[List[dict],List[List[dict]]]] = None,\n    parser: Optional[Type[LLMOutputParser]] = None,\n    parse_mode: Optional[str] = \"json\", \n    parse_func: Optional[Callable] = None,\n    **kwargs\n) -&gt; Union[LLMOutputParser, List[LLMOutputParser]]:\n    \"\"\"Asynchronously generates LLM output(s) and parses the result(s).\n\n    This is the async version of the generate method. It works identically but\n    performs the generation asynchronously.\n    \"\"\"\n    prepared_messages, single_generate = self._prepare_messages(prompt, system_message, messages)\n    if not prepared_messages:  # Handle empty messages case\n        return []\n\n    generated_texts = await self.batch_generate_async(batch_messages=prepared_messages, **kwargs)\n    parsed_outputs = self.parse_generated_texts(texts=generated_texts, parser=parser, parse_mode=parse_mode, parse_func=parse_func, **kwargs)\n    return parsed_outputs[0] if single_generate else parsed_outputs\n</code></pre>"},{"location":"api/models/#evoagentx.models.OpenAILLM","title":"OpenAILLM","text":"<pre><code>OpenAILLM(config: LLMConfig, **kwargs)\n</code></pre> <p>               Bases: <code>BaseLLM</code></p> Source code in <code>evoagentx/models/base_model.py</code> <pre><code>def __init__(self, config: LLMConfig, **kwargs):\n    \"\"\"Initializes the LLM with configuration.\n\n    Args:\n        config: Configuration object for the LLM.\n        **kwargs (Any): Additional keyword arguments.\n    \"\"\"\n    self.config = config\n    self.kwargs = kwargs\n    self.init_model()\n</code></pre>"},{"location":"api/models/#evoagentx.models.OpenAILLM.get_stream_output","title":"get_stream_output","text":"<pre><code>get_stream_output(response: Stream, output_response: bool = True) -&gt; str\n</code></pre> <p>Process stream response and return the complete output.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>Stream</code> <p>The stream response from OpenAI</p> required <code>output_response</code> <code>bool</code> <p>Whether to print the response in real-time</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The complete output text</p> Source code in <code>evoagentx/models/openai_model.py</code> <pre><code>def get_stream_output(self, response: Stream, output_response: bool=True) -&gt; str:\n    \"\"\"\n    Process stream response and return the complete output.\n\n    Args:\n        response: The stream response from OpenAI\n        output_response: Whether to print the response in real-time\n\n    Returns:\n        str: The complete output text\n    \"\"\"\n    output = \"\"\n    for chunk in response:\n        content = chunk.choices[0].delta.content\n        if content:\n            if output_response:\n                print(content, end=\"\", flush=True)\n            output += content\n    if output_response:\n        print(\"\")\n    return output\n</code></pre>"},{"location":"api/models/#evoagentx.models.OpenAILLM.get_stream_output_async","title":"get_stream_output_async  <code>async</code>","text":"<pre><code>get_stream_output_async(response, output_response: bool = False) -&gt; str\n</code></pre> <p>Process async stream response and return the complete output.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>AsyncIterator[ChatCompletionChunk]</code> <p>The async stream response from OpenAI</p> required <code>output_response</code> <code>bool</code> <p>Whether to print the response in real-time</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The complete output text</p> Source code in <code>evoagentx/models/openai_model.py</code> <pre><code>async def get_stream_output_async(self, response, output_response: bool = False) -&gt; str:\n    \"\"\"\n    Process async stream response and return the complete output.\n\n    Args:\n        response (AsyncIterator[ChatCompletionChunk]): The async stream response from OpenAI\n        output_response (bool): Whether to print the response in real-time\n\n\n    Returns:\n        str: The complete output text\n    \"\"\"\n    output = \"\"\n    async for chunk in response:\n        content = chunk.choices[0].delta.content\n        if content:\n            if output_response:\n                print(content, end=\"\", flush=True)\n            output += content\n    if output_response:\n        print(\"\")\n    return output\n</code></pre>"},{"location":"api/models/#evoagentx.models.atomic_method","title":"atomic_method","text":"<pre><code>atomic_method(func)\n</code></pre> <p>threading safe decorator for class methods.  If there are self._lock in the instance, it will use the lock. Otherwise, use nullcontext for execution.</p> Source code in <code>evoagentx/core/decorators.py</code> <pre><code>def atomic_method(func):\n    \"\"\"\n    threading safe decorator for class methods. \n    If there are self._lock in the instance, it will use the lock. Otherwise, use nullcontext for execution.\n    \"\"\"\n    @wraps(func)\n    def wrapper(self, *args, **kwargs):\n        context = getattr(self, \"_lock\", nullcontext())\n        with context:\n            return func(self, *args, **kwargs)\n    return wrapper\n</code></pre>"},{"location":"api/optimizers/","title":"\ud83e\uddee Optimizers","text":""},{"location":"api/optimizers/#evoagentx.optimizers","title":"evoagentx.optimizers","text":""},{"location":"api/optimizers/#evoagentx.optimizers.SEWOptimizer","title":"SEWOptimizer","text":"<pre><code>SEWOptimizer(**kwargs)\n</code></pre> <p>               Bases: <code>Optimizer</code></p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/optimizers/#evoagentx.optimizers.SEWOptimizer.step","title":"step","text":"<pre><code>step(**kwargs) -&gt; Union[SequentialWorkFlowGraph, ActionGraph]\n</code></pre> <p>Take a step of optimization and return the optimized graph.</p> Source code in <code>evoagentx/optimizers/sew_optimizer.py</code> <pre><code>def step(self, **kwargs) -&gt; Union[SequentialWorkFlowGraph, ActionGraph]:\n    \"\"\"\n    Take a step of optimization and return the optimized graph.\n    \"\"\"\n    graph = self._select_graph_with_highest_score(return_metrics=False)\n    if isinstance(graph, SequentialWorkFlowGraph):\n        new_graph = self._workflow_graph_step(graph)\n    elif isinstance(graph, ActionGraph):\n        new_graph = self._action_graph_step(graph)\n    else:\n        raise ValueError(f\"Invalid graph type: {type(graph)}. The graph should be an instance of `WorkFlowGraph` or `ActionGraph`.\")\n    return new_graph\n</code></pre>"},{"location":"api/optimizers/#evoagentx.optimizers.SEWOptimizer.evaluate","title":"evaluate","text":"<pre><code>evaluate(dataset: Benchmark, eval_mode: str = 'test', graph: Optional[Union[SequentialWorkFlowGraph, ActionGraph]] = None, indices: Optional[List[int]] = None, sample_k: Optional[int] = None, **kwargs) -&gt; dict\n</code></pre> <p>Evaluate the workflow. If <code>graph</code> is provided, use the provided graph for evaluation. Otherwise, use the graph in the optimizer. </p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Benchmark</code> <p>The dataset to evaluate the workflow on.</p> required <code>eval_mode</code> <code>str</code> <p>The evaluation mode. Choices: [\"test\", \"dev\", \"train\"].</p> <code>'test'</code> <code>graph</code> <code>Union[WorkFlowGraph, ActionGraph]</code> <p>The graph to evaluate. If not provided, use the graph in the optimizer.</p> <code>None</code> <code>indices</code> <code>List[int]</code> <p>The indices of the data to evaluate the workflow on.</p> <code>None</code> <code>sample_k</code> <code>int</code> <p>The number of data to evaluate the workflow on. If provided, a random sample of size <code>sample_k</code> will be used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The metrics of the workflow evaluation.</p> Source code in <code>evoagentx/optimizers/sew_optimizer.py</code> <pre><code>def evaluate(\n    self, \n    dataset: Benchmark, \n    eval_mode: str = \"test\", \n    graph: Optional[Union[SequentialWorkFlowGraph, ActionGraph]] = None,\n    indices: Optional[List[int]] = None,\n    sample_k: Optional[int] = None,\n    **kwargs\n) -&gt; dict:\n    \"\"\"\n    Evaluate the workflow. If `graph` is provided, use the provided graph for evaluation. Otherwise, use the graph in the optimizer. \n\n    Args:\n        dataset (Benchmark): The dataset to evaluate the workflow on.\n        eval_mode (str): The evaluation mode. Choices: [\"test\", \"dev\", \"train\"].\n        graph (Union[WorkFlowGraph, ActionGraph], optional): The graph to evaluate. If not provided, use the graph in the optimizer.\n        indices (List[int], optional): The indices of the data to evaluate the workflow on.\n        sample_k (int, optional): The number of data to evaluate the workflow on. If provided, a random sample of size `sample_k` will be used.\n\n    Returns:\n        dict: The metrics of the workflow evaluation.\n    \"\"\"\n    graph = graph if graph is not None else self.graph\n    metrics_list = []\n    for i in range(self.eval_rounds):\n        eval_info = [\n            f\"[{type(graph).__name__}]\", \n            f\"Evaluation round {i+1}/{self.eval_rounds}\", \n            f\"Mode: {eval_mode}\"\n        ]\n        if indices is not None:\n            eval_info.append(f\"Indices: {len(indices)} samples\")\n        if sample_k is not None:\n            eval_info.append(f\"Sample size: {sample_k}\")\n        logger.info(\" | \".join(eval_info))\n        metrics = self.evaluator.evaluate(\n            graph=graph, \n            benchmark=dataset, \n            eval_mode=eval_mode, \n            indices=indices, \n            sample_k=sample_k,\n            **kwargs\n        )\n        metrics_list.append(metrics)\n    avg_metrics = self.evaluator._calculate_average_score(metrics_list)\n\n    return avg_metrics\n</code></pre>"},{"location":"api/optimizers/#evoagentx.optimizers.SEWOptimizer.save","title":"save","text":"<pre><code>save(path: str, ignore: List[str] = [])\n</code></pre> <p>Save the (optimized) workflow graph to a file. </p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to save the workflow graph.</p> required <code>ignore</code> <code>List[str]</code> <p>The keys to ignore when saving the workflow graph.</p> <code>[]</code> Source code in <code>evoagentx/optimizers/sew_optimizer.py</code> <pre><code>def save(self, path: str, ignore: List[str] = []):\n    \"\"\"\n    Save the (optimized) workflow graph to a file. \n\n    Args:\n        path (str): The path to save the workflow graph.\n        ignore (List[str]): The keys to ignore when saving the workflow graph.\n    \"\"\"\n    self.graph.save_module(path, ignore=ignore)\n</code></pre>"},{"location":"api/optimizers/#evoagentx.optimizers.AFlowOptimizer","title":"AFlowOptimizer","text":"<pre><code>AFlowOptimizer(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModule</code></p> <p>AFlow Optimizer for workflow optimization.</p> <p>This optimizer iteratively improves workflows through multiple rounds of optimization using large language models. It evaluates workflow performance, identifies improvement opportunities, and applies optimizations based on experience and convergence metrics.</p> <p>Attributes:</p> Name Type Description <code>question_type</code> <code>str</code> <p>Type of task to optimize for (e.g., qa, match, code)</p> <code>graph_path</code> <code>str</code> <p>Path to the workflow graph directory (must contain graph.py and prompt.py)</p> <code>optimized_path</code> <code>str</code> <p>Path to save optimized workflows (defaults to graph_path)</p> <code>initial_round</code> <code>int</code> <p>Starting round number for optimization</p> <code>optimizer_llm</code> <code>BaseLLM</code> <p>LLM used for generating optimizations</p> <code>executor_llm</code> <code>BaseLLM</code> <p>LLM used for executing the workflow</p> <code>operators</code> <code>List[str]</code> <p>List of operators available for optimization</p> <code>sample</code> <code>int</code> <p>Number of rounds to sample from for optimization</p> <code>max_rounds</code> <code>int</code> <p>Maximum number of optimization rounds to perform</p> <code>validation_rounds</code> <code>int</code> <p>Number of validation runs per optimization round</p> <code>eval_rounds</code> <code>int</code> <p>Number of evaluation runs for test mode</p> <code>check_convergence</code> <code>bool</code> <p>Whether to check for optimization convergence</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/optimizers/#evoagentx.optimizers.AFlowOptimizer.optimize","title":"optimize","text":"<pre><code>optimize(benchmark: Benchmark)\n</code></pre> <p>Run the optimization process on the workflow.</p> <p>Performs multiple rounds of optimization, evaluating each round against the benchmark and checking for convergence. Continues until convergence is detected or the maximum number of rounds is reached.</p> <p>Parameters:</p> Name Type Description Default <code>benchmark</code> <code>Benchmark</code> <p>The benchmark to evaluate the workflow against</p> required Source code in <code>evoagentx/optimizers/aflow_optimizer.py</code> <pre><code>def optimize(self, benchmark: Benchmark):\n    \"\"\"Run the optimization process on the workflow.\n\n    Performs multiple rounds of optimization, evaluating each round against\n    the benchmark and checking for convergence. Continues until convergence\n    is detected or the maximum number of rounds is reached.\n\n    Args:\n        benchmark: The benchmark to evaluate the workflow against\n    \"\"\"\n    self.benchmark = benchmark\n    for _ in range(self.max_rounds):\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n        score = loop.run_until_complete(self._execute_with_retry(self._optimize_graph))\n        self.round += 1\n        logger.info(f\"Score for round {self.round}: {score}\")\n        if self._check_convergence():\n            break\n        if self.round &gt;= self.max_rounds:\n            logger.info(f\"Max rounds reached: {self.max_rounds}, stopping optimization.\")\n            break\n</code></pre>"},{"location":"api/optimizers/#evoagentx.optimizers.AFlowOptimizer.test","title":"test","text":"<pre><code>test(benchmark: Benchmark, test_rounds: List[int] = None)\n</code></pre> <p>Run the test evaluation on optimized workflows.</p> <p>Evaluates specified rounds (or the best round if none specified) against the benchmark multiple times and logs the results.</p> <p>Parameters:</p> Name Type Description Default <code>benchmark</code> <code>Benchmark</code> <p>The benchmark to evaluate against</p> required <code>test_rounds</code> <code>List[int]</code> <p>Specific round numbers to test, or None to use the best round</p> <code>None</code> Source code in <code>evoagentx/optimizers/aflow_optimizer.py</code> <pre><code>def test(self, benchmark: Benchmark, test_rounds: List[int] = None):\n    \"\"\"Run the test evaluation on optimized workflows.\n\n    Evaluates specified rounds (or the best round if none specified) against\n    the benchmark multiple times and logs the results.\n\n    Args:\n        benchmark: The benchmark to evaluate against\n        test_rounds: Specific round numbers to test, or None to use the best round\n    \"\"\"\n    self.benchmark = benchmark\n    if test_rounds is None:\n        best_round = self._load_best_round()\n        logger.info(f\"No test rounds provided, using best round: {best_round}\")\n        test_rounds = [best_round]\n    for _ in tqdm(range(self.eval_rounds)):\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n        loop.run_until_complete(self._run_test(test_rounds))\n</code></pre>"},{"location":"api/optimizers/#evoagentx.optimizers.TextGradOptimizer","title":"TextGradOptimizer","text":"<pre><code>TextGradOptimizer(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModule</code></p> <p>Optimizes the prompt templates and system prompts in a workflow using TextGrad. For more information, see https://github.com/zou-group/textgrad.</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/optimizers/#evoagentx.optimizers.TextGradOptimizer.optimize","title":"optimize","text":"<pre><code>optimize(dataset: Benchmark, use_answers: bool = True, seed: Optional[int] = None)\n</code></pre> <p>Optimizes self.graph using <code>dataset</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Benchmark</code> <p>The dataset to use for optimization.</p> required <code>use_answers</code> <code>bool</code> <p>Whether to use the answers (labels) in the training set for optimization. If False, <code>dataset</code>'s training set does not need to have answers. If <code>eval_every_n_steps</code> is set to None, we can optimize the workflow without any labeled data.</p> <code>True</code> <code>seed</code> <code>Optional[int]</code> <p>The random seed to use for shuffling the data.</p> <code>None</code> Source code in <code>evoagentx/optimizers/textgrad_optimizer.py</code> <pre><code>def optimize(self, dataset: Benchmark, use_answers: bool = True, seed: Optional[int] = None):\n    \"\"\"Optimizes self.graph using `dataset`.\n\n    Args:\n        dataset (Benchmark): The dataset to use for optimization.\n        use_answers (bool): Whether to use the answers (labels) in the training set for optimization.\n            If False, `dataset`'s training set does not need to have answers.\n            If `eval_every_n_steps` is set to None, we can optimize the workflow without any labeled data.\n        seed (Optional[int]): The random seed to use for shuffling the data.\n    \"\"\"\n    self._init_textgrad(dataset, use_answers)\n\n    def iterator():\n        epoch = 0\n        while True:\n            # Shuffle train data every epoch\n            effective_seed = seed + epoch if seed is not None else None\n            train_data = dataset.get_train_data(sample_k=len(dataset._train_data), seed=effective_seed)\n            for i in range(0, len(train_data), self.batch_size):\n                batch = train_data[i:i + self.batch_size]\n                inputs = [self.evaluator.collate_func(x) for x in batch]\n                if use_answers:\n                    labels = dataset.get_labels(batch)\n                else:\n                    labels = None\n                yield inputs, labels\n            epoch += 1\n\n    data_iterator = iterator()\n\n    for step in tqdm(range(self.max_steps)):\n        inputs, labels = next(data_iterator)\n        self.step(inputs, labels, dataset, use_answers)\n\n        if self.eval_interval is not None and (step + 1) % self.eval_interval == 0:\n            logger.info(f\"Evaluating the workflow at step {step+1} ...\")\n            with suppress_logger_info():\n                metrics = self.evaluate(dataset, **self.eval_config)\n            self.log_snapshot(self.graph, metrics)\n            logger.info(f\"Step {step+1} metrics: {metrics}\")\n\n            # If rollback is enabled, keep track of the best snapshot\n            if self.rollback:\n                if len(self._snapshot) == 1:\n                    best_snapshot = self._snapshot[-1]\n                    best_average_score = np.mean(list(metrics.values()))\n                else:\n                    current_average_score = np.mean(list(metrics.values()))\n\n                    if current_average_score &gt;= best_average_score:\n                        # If the current average score is better than the best average score, update the best snapshot\n                        best_snapshot = self._snapshot[-1]\n                        best_average_score = current_average_score\n                    else:\n                        # If the current average score is worse than the best average score, roll back to the best snapshot\n                        logger.info(f\"Metrics are worse than the best snapshot which has {best_snapshot['metrics']}. Rolling back to the best snapshot.\")\n                        best_graph = SequentialWorkFlowGraph.from_dict(best_snapshot[\"graph\"])\n                        self.graph = best_graph\n                        self._create_textgrad_agents()\n\n        if self.save_interval is not None and (step + 1) % self.save_interval == 0:\n            logger.info(f\"Saving the workflow at step {step+1} ...\")\n            self.save(os.path.join(self.save_path, f\"{dataset.name}_textgrad_step_{step+1}.json\"))\n\n    logger.info(f\"Reached the maximum number of steps {self.max_steps}. Optimization has finished.\")\n    self.save(os.path.join(self.save_path, f\"{dataset.name}_textgrad_final.json\"))\n\n    # Saves the best graph\n    if len(self._snapshot) &gt; 0:\n        best_graph = self._select_graph_with_highest_score()\n        self.save(os.path.join(self.save_path, f\"{dataset.name}_textgrad_best.json\"), graph=best_graph)\n</code></pre>"},{"location":"api/optimizers/#evoagentx.optimizers.TextGradOptimizer.step","title":"step","text":"<pre><code>step(inputs: List[dict[str, str]], labels: Optional[List[str | dict[str, str]]], dataset: Benchmark, use_answers: bool = True)\n</code></pre> <p>Performs one optimization step using a batch of data.</p> Source code in <code>evoagentx/optimizers/textgrad_optimizer.py</code> <pre><code>def step(self, inputs: List[dict[str, str]], labels: Optional[List[str|dict[str, str]]], dataset: Benchmark, use_answers: bool = True):\n    \"\"\"Performs one optimization step using a batch of data.\"\"\"\n\n    if labels is None and use_answers:\n        raise ValueError(\"Labels must be provided if `use_answers` is True.\")\n\n    losses = []\n\n    if use_answers:\n        for input, label in zip(inputs, labels):\n            output = self.forward(input)\n            if isinstance(label, str):\n                label = Variable(label, requires_grad=False, role_description=\"correct answer for the query\")\n            elif isinstance(label, dict):\n                if not isinstance(dataset, CodingBenchmark):\n                    raise ValueError(\"Label must be a string for non-coding benchmarks.\")\n                end_node_name = self.graph.find_end_nodes()[0]\n                end_node = self.graph.get_node(end_node_name)\n                output_name = end_node.outputs[0].name\n                code = output.parsed_outputs[output_name]\n                label = self._format_code_label(code, label, dataset)\n                label = Variable(label, requires_grad=False, role_description=\"the task, the test result, and the correct code\")\n            loss = self.loss_fn([output, label])\n            losses.append(loss)\n    else:\n        for input in inputs:\n            output = self.forward(input)\n            loss = self.loss_fn(output)\n            losses.append(loss)\n\n    total_loss = tg.sum(losses)\n    total_loss.backward(self.optimizer_engine)\n    self.textgrad_optimizer.step()\n    self.textgrad_optimizer.zero_grad()\n\n    # Checks if all the prompt templates contain the required inputs.\n    # If not, fix them by appending the input placeholders at the end.\n    for node in self.graph.nodes:\n        prompt_template = node.textgrad_agent.prompt_template.value\n        prompt_template = self._add_missing_input_placeholder(prompt_template, node)\n        node.textgrad_agent.prompt_template.value = prompt_template\n\n    self._update_workflow_graph()\n</code></pre>"},{"location":"api/optimizers/#evoagentx.optimizers.TextGradOptimizer.forward","title":"forward","text":"<pre><code>forward(inputs: dict[str, str]) -&gt; Variable\n</code></pre> <p>Returns the final output from the workflow.</p> Source code in <code>evoagentx/optimizers/textgrad_optimizer.py</code> <pre><code>def forward(self, inputs: dict[str, str]) -&gt; Variable:\n    \"\"\"Returns the final output from the workflow.\"\"\"\n    self._visited_nodes = set()\n    end_node = self.graph.find_end_nodes()[0]\n    input_variables = self._initial_inputs_to_variables(inputs)\n    output = self._compute_node(end_node, input_variables)\n    return output\n</code></pre>"},{"location":"api/optimizers/#evoagentx.optimizers.TextGradOptimizer.evaluate","title":"evaluate","text":"<pre><code>evaluate(dataset: Benchmark, eval_mode: str = 'dev', graph: Optional[SequentialWorkFlowGraph] = None, indices: Optional[List[int]] = None, sample_k: Optional[int] = None, **kwargs) -&gt; dict\n</code></pre> <p>Evaluate the workflow. If <code>graph</code> is provided, use the provided graph for evaluation. Otherwise, use the graph in the optimizer. </p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Benchmark</code> <p>The dataset to evaluate the workflow on.</p> required <code>eval_mode</code> <code>str</code> <p>The evaluation mode. Choices: [\"test\", \"dev\", \"train\"].</p> <code>'dev'</code> <code>graph</code> <code>SequentialWorkFlowGraph</code> <p>The graph to evaluate. If not provided, use the graph in the optimizer.</p> <code>None</code> <code>indices</code> <code>List[int]</code> <p>The indices of the data to evaluate the workflow on.</p> <code>None</code> <code>sample_k</code> <code>int</code> <p>The number of data to evaluate the workflow on. If provided, a random sample of size <code>sample_k</code> will be used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The metrics of the workflow evaluation.</p> Source code in <code>evoagentx/optimizers/textgrad_optimizer.py</code> <pre><code>def evaluate(\n    self, \n    dataset: Benchmark, \n    eval_mode: str = \"dev\", \n    graph: Optional[SequentialWorkFlowGraph] = None,\n    indices: Optional[List[int]] = None,\n    sample_k: Optional[int] = None,\n    **kwargs\n) -&gt; dict:\n    \"\"\"Evaluate the workflow. If `graph` is provided, use the provided graph for evaluation. Otherwise, use the graph in the optimizer. \n\n    Args:\n        dataset (Benchmark): The dataset to evaluate the workflow on.\n        eval_mode (str): The evaluation mode. Choices: [\"test\", \"dev\", \"train\"].\n        graph (SequentialWorkFlowGraph, optional): The graph to evaluate. If not provided, use the graph in the optimizer.\n        indices (List[int], optional): The indices of the data to evaluate the workflow on.\n        sample_k (int, optional): The number of data to evaluate the workflow on. If provided, a random sample of size `sample_k` will be used.\n\n    Returns:\n        dict: The metrics of the workflow evaluation.\n    \"\"\"\n    if graph is None:\n        graph = self.graph\n\n    metrics_list = []\n    for i in range(self.eval_rounds):\n        eval_info = [\n            f\"[{type(graph).__name__}]\", \n            f\"Evaluation round {i+1}/{self.eval_rounds}\", \n            f\"Mode: {eval_mode}\"\n        ]\n        if indices is not None:\n            eval_info.append(f\"Indices: {len(indices)} samples\")\n        if sample_k is not None:\n            eval_info.append(f\"Sample size: {sample_k}\")\n        logger.info(\" | \".join(eval_info))\n        metrics = self.evaluator.evaluate(\n            graph=graph, \n            benchmark=dataset, \n            eval_mode=eval_mode, \n            indices=indices, \n            sample_k=sample_k,\n            update_agents=True, \n            **kwargs\n        )\n        metrics_list.append(metrics)\n    avg_metrics = self.evaluator._calculate_average_score(metrics_list)\n\n    return avg_metrics\n</code></pre>"},{"location":"api/optimizers/#evoagentx.optimizers.TextGradOptimizer.save","title":"save","text":"<pre><code>save(path: str, graph: Optional[SequentialWorkFlowGraph] = None, ignore: List[str] = [])\n</code></pre> <p>Save the workflow graph containing the optimized prompts to a file. </p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to save the workflow graph.</p> required <code>graph</code> <code>SequantialWorkFlowGraph</code> <p>The graph to save. If not provided, use the graph in the optimizer.</p> <code>None</code> <code>ignore</code> <code>List[str]</code> <p>The keys to ignore when saving the workflow graph.</p> <code>[]</code> Source code in <code>evoagentx/optimizers/textgrad_optimizer.py</code> <pre><code>def save(self, path: str, graph: Optional[SequentialWorkFlowGraph] = None, ignore: List[str] = []):\n    \"\"\"Save the workflow graph containing the optimized prompts to a file. \n\n    Args:\n        path (str): The path to save the workflow graph.\n        graph (SequantialWorkFlowGraph, optional): The graph to save. If not provided, use the graph in the optimizer.\n        ignore (List[str]): The keys to ignore when saving the workflow graph.\n    \"\"\"\n    if graph is None:\n        graph = self.graph\n    graph.save_module(path, ignore=ignore)\n</code></pre>"},{"location":"api/optimizers/#evoagentx.optimizers.TextGradOptimizer.log_snapshot","title":"log_snapshot","text":"<pre><code>log_snapshot(graph: SequentialWorkFlowGraph, metrics: dict)\n</code></pre> <p>Log the snapshot of the workflow.</p> Source code in <code>evoagentx/optimizers/textgrad_optimizer.py</code> <pre><code>def log_snapshot(self, graph: SequentialWorkFlowGraph, metrics: dict):\n    \"\"\"Log the snapshot of the workflow.\"\"\"\n    self._snapshot.append(\n        {\n            \"index\": len(self._snapshot),\n            \"graph\": deepcopy(graph.get_graph_info()),\n            \"metrics\": metrics,\n        }\n    )\n</code></pre>"},{"location":"api/optimizers/#evoagentx.optimizers.TextGradOptimizer.restore_best_graph","title":"restore_best_graph","text":"<pre><code>restore_best_graph()\n</code></pre> <p>Restore the best graph from the snapshot and set it to <code>self.graph</code>.</p> Source code in <code>evoagentx/optimizers/textgrad_optimizer.py</code> <pre><code>def restore_best_graph(self):\n    \"\"\"Restore the best graph from the snapshot and set it to `self.graph`.\"\"\"\n    if len(self._snapshot) == 0:\n        logger.info(\"No snapshot found. No graph to restore.\")\n        return\n\n    best_graph, best_metrics = self._select_graph_with_highest_score(return_metrics=True)\n    self.graph = best_graph\n    logger.info(f\"Restored the best graph from snapshot with metrics {best_metrics}\")\n</code></pre>"},{"location":"api/storages/","title":"\ud83d\udcbe Storages","text":""},{"location":"api/storages/#evoagentx.storages","title":"evoagentx.storages","text":""},{"location":"api/storages/#evoagentx.storages.BaseModule","title":"BaseModule","text":"<pre><code>BaseModule(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>Base module class that serves as the foundation for all modules in the EvoAgentX framework.</p> <p>This class provides serialization/deserialization capabilities, supports creating instances from dictionaries, JSON, or files, and exporting instances to these formats.</p> <p>Attributes:</p> Name Type Description <code>class_name</code> <code>str</code> <p>The class name, defaults to None but is automatically set during subclass initialization</p> <code>model_config</code> <p>Pydantic model configuration that controls type matching and behavior</p> <p>Initializes a BaseModule instance.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Keyword arguments used to initialize the instance</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValidationError</code> <p>When parameter validation fails</p> <code>Exception</code> <p>When other errors occur during initialization</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/storages/#evoagentx.storages.BaseModule.kwargs","title":"kwargs  <code>property</code>","text":"<pre><code>kwargs: dict\n</code></pre> <p>Returns the extra fields of the model.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary containing all extra keyword arguments</p>"},{"location":"api/storages/#evoagentx.storages.BaseModule.__init_subclass__","title":"__init_subclass__","text":"<pre><code>__init_subclass__(**kwargs)\n</code></pre> <p>Subclass initialization method that automatically sets the class_name attribute.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Type</code> <p>The subclass being initialized</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments</p> <code>{}</code> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init_subclass__(cls, **kwargs):\n    \"\"\"\n    Subclass initialization method that automatically sets the class_name attribute.\n\n    Args:\n        cls (Type): The subclass being initialized\n        **kwargs (Any): Additional keyword arguments\n    \"\"\"\n    super().__init_subclass__(**kwargs)\n    cls.class_name = cls.__name__\n</code></pre>"},{"location":"api/storages/#evoagentx.storages.BaseModule.init_module","title":"init_module","text":"<pre><code>init_module()\n</code></pre> <p>Module initialization method that subclasses can override to provide additional initialization logic.</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def init_module(self):\n    \"\"\"\n    Module initialization method that subclasses can override to provide additional initialization logic.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/storages/#evoagentx.storages.BaseModule.__str__","title":"__str__","text":"<pre><code>__str__() -&gt; str\n</code></pre> <p>Returns a string representation of the object.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>String representation of the object</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"\n    Returns a string representation of the object.\n\n    Returns:\n        str: String representation of the object\n    \"\"\"\n    return self.to_str()\n</code></pre>"},{"location":"api/storages/#evoagentx.storages.BaseModule.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: Dict[str, Any], **kwargs) -&gt; BaseModule\n</code></pre> <p>Instantiate the BaseModule from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>Dictionary containing instance data</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments, can include log to control logging output</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>BaseModule</code> <code>BaseModule</code> <p>The created module instance</p> <p>Raises:</p> Type Description <code>Exception</code> <p>When errors occur during initialization</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: Dict[str, Any], **kwargs) -&gt; \"BaseModule\":\n    \"\"\"\n    Instantiate the BaseModule from a dictionary.\n\n    Args:\n        data: Dictionary containing instance data\n        **kwargs (Any): Additional keyword arguments, can include log to control logging output\n\n    Returns:\n        BaseModule: The created module instance\n\n    Raises:\n        Exception: When errors occur during initialization\n    \"\"\"\n    use_logger = kwargs.get(\"log\", True)\n    with exception_buffer() as buffer:\n        try:\n            class_name = data.get(\"class_name\", None)\n            if class_name:\n                cls = MODULE_REGISTRY.get_module(class_name)\n            module = cls._create_instance(data)\n            # module = cls.model_validate(data)\n            if len(buffer.exceptions) &gt; 0:\n                error_message = get_base_module_init_error_message(cls, data, buffer.exceptions)\n                if use_logger:\n                    logger.error(error_message)\n                raise Exception(get_error_message(buffer.exceptions))\n        finally:\n            pass\n    return module\n</code></pre>"},{"location":"api/storages/#evoagentx.storages.BaseModule.from_json","title":"from_json  <code>classmethod</code>","text":"<pre><code>from_json(content: str, **kwargs) -&gt; BaseModule\n</code></pre> <p>Construct the BaseModule from a JSON string.</p> <p>This method uses yaml.safe_load to parse the JSON string into a Python object, which supports more flexible parsing than standard json.loads (including handling single quotes, trailing commas, etc). The parsed data is then passed to from_dict to create the instance.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>JSON string</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments, can include <code>log</code> to control logging output</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>BaseModule</code> <code>BaseModule</code> <p>The created module instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When the input is not a valid JSON string</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>@classmethod\ndef from_json(cls, content: str, **kwargs) -&gt; \"BaseModule\":\n    \"\"\"\n    Construct the BaseModule from a JSON string.\n\n    This method uses yaml.safe_load to parse the JSON string into a Python object,\n    which supports more flexible parsing than standard json.loads (including handling\n    single quotes, trailing commas, etc). The parsed data is then passed to from_dict\n    to create the instance.\n\n    Args:\n        content: JSON string\n        **kwargs (Any): Additional keyword arguments, can include `log` to control logging output\n\n    Returns:\n        BaseModule: The created module instance\n\n    Raises:\n        ValueError: When the input is not a valid JSON string\n    \"\"\"\n    use_logger = kwargs.get(\"log\", True)\n    try:\n        data = yaml.safe_load(content)\n    except Exception:\n        error_message = f\"Can not instantiate {cls.__name__}. The input to {cls.__name__}.from_json is not a valid JSON string.\"\n        if use_logger:\n            logger.error(error_message)\n        raise ValueError(error_message)\n\n    if not isinstance(data, (list, dict)):\n        error_message = f\"Can not instantiate {cls.__name__}. The input to {cls.__name__}.from_json is not a valid JSON string.\"\n        if use_logger:\n            logger.error(error_message)\n        raise ValueError(error_message)\n\n    return cls.from_dict(data, log=use_logger)\n</code></pre>"},{"location":"api/storages/#evoagentx.storages.BaseModule.from_str","title":"from_str  <code>classmethod</code>","text":"<pre><code>from_str(content: str, **kwargs) -&gt; BaseModule\n</code></pre> <p>Construct the BaseModule from a string that may contain JSON.</p> <p>This method is more forgiving than <code>from_json</code> as it can extract valid JSON objects embedded within larger text. It uses <code>parse_json_from_text</code> to extract  all potential JSON strings from the input text, then tries to create an instance  from each extracted JSON string until successful.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>Text that may contain JSON strings</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments, can include <code>log</code> to control logging output</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>BaseModule</code> <code>BaseModule</code> <p>The created module instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When the input does not contain valid JSON strings or the JSON is incompatible with the class</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>@classmethod\ndef from_str(cls, content: str, **kwargs) -&gt; \"BaseModule\":\n    \"\"\"\n    Construct the BaseModule from a string that may contain JSON.\n\n    This method is more forgiving than `from_json` as it can extract valid JSON\n    objects embedded within larger text. It uses `parse_json_from_text` to extract \n    all potential JSON strings from the input text, then tries to create an instance \n    from each extracted JSON string until successful.\n\n    Args:\n        content: Text that may contain JSON strings\n        **kwargs (Any): Additional keyword arguments, can include `log` to control logging output\n\n    Returns:\n        BaseModule: The created module instance\n\n    Raises:\n        ValueError: When the input does not contain valid JSON strings or the JSON is incompatible with the class\n    \"\"\"\n    use_logger = kwargs.get(\"log\", True)\n\n    extracted_json_list = parse_json_from_text(content)\n    if len(extracted_json_list) == 0:\n        error_message = f\"The input to {cls.__name__}.from_str does not contain any valid JSON str.\"\n        if use_logger:\n            logger.error(error_message)\n        raise ValueError(error_message)\n\n    module = None\n    for json_str in extracted_json_list:\n        try:\n            module = cls.from_json(json_str, log=False)\n        except Exception:\n            continue\n        break\n\n    if module is None:\n        error_message = f\"Can not instantiate {cls.__name__}. The input to {cls.__name__}.from_str either does not contain a valide JSON str, or the JSON str is incomplete or incompatable (incorrect variables or types) with {cls.__name__}.\"\n        error_message += f\"\\nInput:\\n{content}\"\n        if use_logger:\n            logger.error(error_message)\n        raise ValueError(error_message)\n\n    return module\n</code></pre>"},{"location":"api/storages/#evoagentx.storages.BaseModule.load_module","title":"load_module  <code>classmethod</code>","text":"<pre><code>load_module(path: str, **kwargs) -&gt; dict\n</code></pre> <p>Load the values for a module from a file.</p> <p>By default, it opens the specified file and uses <code>yaml.safe_load</code> to parse its contents  into a Python object (typically a dictionary).</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path of the file</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The JSON object instantiated from the file</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>@classmethod \ndef load_module(cls, path: str, **kwargs) -&gt; dict:\n    \"\"\"\n    Load the values for a module from a file.\n\n    By default, it opens the specified file and uses `yaml.safe_load` to parse its contents \n    into a Python object (typically a dictionary).\n\n    Args:\n        path: The path of the file\n        **kwargs (Any): Additional keyword arguments\n\n    Returns:\n        dict: The JSON object instantiated from the file\n    \"\"\"\n    with open(path, mode=\"r\", encoding=\"utf-8\") as file:\n        content = yaml.safe_load(file.read())\n    return content\n</code></pre>"},{"location":"api/storages/#evoagentx.storages.BaseModule.from_file","title":"from_file  <code>classmethod</code>","text":"<pre><code>from_file(path: str, load_function: Callable = None, **kwargs) -&gt; BaseModule\n</code></pre> <p>Construct the BaseModule from a file.</p> <p>This method reads and parses a file into a data structure, then creates a module instance from that data. It first verifies that the file exists, then uses either the provided <code>load_function</code> or the default <code>load_module</code> method to read and parse the file content, and finally calls <code>from_dict</code> to create the instance.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path of the file</p> required <code>load_function</code> <code>Callable</code> <p>The function used to load the data, takes a file path as input and returns a JSON object</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments, can include <code>log</code> to control logging output</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>BaseModule</code> <code>BaseModule</code> <p>The created module instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When the file does not exist</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>@classmethod\ndef from_file(cls, path: str, load_function: Callable=None, **kwargs) -&gt; \"BaseModule\":\n    \"\"\"\n    Construct the BaseModule from a file.\n\n    This method reads and parses a file into a data structure, then creates\n    a module instance from that data. It first verifies that the file exists,\n    then uses either the provided `load_function` or the default `load_module`\n    method to read and parse the file content, and finally calls `from_dict`\n    to create the instance.\n\n    Args:\n        path: The path of the file\n        load_function: The function used to load the data, takes a file path as input and returns a JSON object\n        **kwargs (Any): Additional keyword arguments, can include `log` to control logging output\n\n    Returns:\n        BaseModule: The created module instance\n\n    Raises:\n        ValueError: When the file does not exist\n    \"\"\"\n    use_logger = kwargs.get(\"log\", True)\n    if not os.path.exists(path):\n        error_message = f\"File \\\"{path}\\\" does not exist!\"\n        if use_logger:\n            logger.error(error_message)\n        raise ValueError(error_message)\n\n    function = load_function or cls.load_module\n    content = function(path, **kwargs)\n    module = cls.from_dict(content, log=use_logger)\n\n    return module\n</code></pre>"},{"location":"api/storages/#evoagentx.storages.BaseModule.to_dict","title":"to_dict","text":"<pre><code>to_dict(exclude_none: bool = True, ignore: List[str] = [], **kwargs) -&gt; dict\n</code></pre> <p>Convert the BaseModule to a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>exclude_none</code> <code>bool</code> <p>Whether to exclude fields with None values</p> <code>True</code> <code>ignore</code> <code>List[str]</code> <p>List of field names to ignore</p> <code>[]</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary containing the object data</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def to_dict(self, exclude_none: bool = True, ignore: List[str] = [], **kwargs) -&gt; dict:\n    \"\"\"\n    Convert the BaseModule to a dictionary.\n\n    Args:\n        exclude_none: Whether to exclude fields with None values\n        ignore: List of field names to ignore\n        **kwargs (Any): Additional keyword arguments\n\n    Returns:\n        dict: Dictionary containing the object data\n    \"\"\"\n    data = {}\n    for field_name, _ in type(self).model_fields.items():\n        if field_name in ignore:\n            continue\n        field_value = getattr(self, field_name, None)\n        if exclude_none and field_value is None:\n            continue\n        if isinstance(field_value, BaseModule):\n            data[field_name] = field_value.to_dict(exclude_none=exclude_none, ignore=ignore)\n        elif isinstance(field_value, list):\n            data[field_name] = [\n                item.to_dict(exclude_none=exclude_none, ignore=ignore) if isinstance(item, BaseModule) else item\n                for item in field_value\n            ]\n        elif isinstance(field_value, dict):\n            data[field_name] = {\n                key: value.to_dict(exclude_none=exclude_none, ignore=ignore) if isinstance(value, BaseModule) else value\n                for key, value in field_value.items()\n            }\n        else:\n            data[field_name] = field_value\n\n    return data\n</code></pre>"},{"location":"api/storages/#evoagentx.storages.BaseModule.to_json","title":"to_json","text":"<pre><code>to_json(use_indent: bool = False, ignore: List[str] = [], **kwargs) -&gt; str\n</code></pre> <p>Convert the BaseModule to a JSON string.</p> <p>Parameters:</p> Name Type Description Default <code>use_indent</code> <code>bool</code> <p>Whether to use indentation</p> <code>False</code> <code>ignore</code> <code>List[str]</code> <p>List of field names to ignore</p> <code>[]</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The JSON string</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def to_json(self, use_indent: bool=False, ignore: List[str] = [], **kwargs) -&gt; str:\n    \"\"\"\n    Convert the BaseModule to a JSON string.\n\n    Args:\n        use_indent: Whether to use indentation\n        ignore: List of field names to ignore\n        **kwargs (Any): Additional keyword arguments\n\n    Returns:\n        str: The JSON string\n    \"\"\"\n    if use_indent:\n        kwargs[\"indent\"] = kwargs.get(\"indent\", 4)\n    else:\n        kwargs.pop(\"indent\", None)\n    if kwargs.get(\"default\", None) is None:\n        kwargs[\"default\"] = custom_serializer\n    data = self.to_dict(exclude_none=True)\n    for ignore_field in ignore:\n        data.pop(ignore_field, None)\n    return json.dumps(data, **kwargs)\n</code></pre>"},{"location":"api/storages/#evoagentx.storages.BaseModule.to_str","title":"to_str","text":"<pre><code>to_str(**kwargs) -&gt; str\n</code></pre> <p>Convert the BaseModule to a string. Use .to_json to output JSON string by default.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The string</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def to_str(self, **kwargs) -&gt; str:\n    \"\"\"\n    Convert the BaseModule to a string. Use .to_json to output JSON string by default.\n\n    Args:\n        **kwargs (Any): Additional keyword arguments\n\n    Returns:\n        str: The string\n    \"\"\"\n    return self.to_json(use_indent=False)\n</code></pre>"},{"location":"api/storages/#evoagentx.storages.BaseModule.save_module","title":"save_module","text":"<pre><code>save_module(path: str, ignore: List[str] = [], **kwargs) -&gt; str\n</code></pre> <p>Save the BaseModule to a file.</p> <p>This method will set non-serializable objects to None by default. If you want to save non-serializable objects, override this method. Remember to also override the <code>load_module</code> function to ensure the loaded object can be correctly parsed by <code>cls.from_dict</code>.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to save the file</p> required <code>ignore</code> <code>List[str]</code> <p>List of field names to ignore</p> <code>[]</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The path where the file is saved, same as the input path</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def save_module(self, path: str, ignore: List[str] = [], **kwargs)-&gt; str:\n    \"\"\"\n    Save the BaseModule to a file.\n\n    This method will set non-serializable objects to None by default.\n    If you want to save non-serializable objects, override this method.\n    Remember to also override the `load_module` function to ensure the loaded\n    object can be correctly parsed by `cls.from_dict`.\n\n    Args:\n        path: The path to save the file\n        ignore: List of field names to ignore\n        **kwargs (Any): Additional keyword arguments\n\n    Returns:\n        str: The path where the file is saved, same as the input path\n    \"\"\"\n    logger.info(\"Saving {} to {}\", self.__class__.__name__, path)\n    return save_json(self.to_json(use_indent=True, default=lambda x: None, ignore=ignore), path=path)\n</code></pre>"},{"location":"api/storages/#evoagentx.storages.BaseModule.deepcopy","title":"deepcopy","text":"<pre><code>deepcopy()\n</code></pre> <p>Deep copy the module.</p> <p>This is a tweak to the default python deepcopy that only deep copies <code>self.parameters()</code>, and for other attributes, we just do the shallow copy.</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def deepcopy(self):\n    \"\"\"Deep copy the module.\n\n    This is a tweak to the default python deepcopy that only deep copies `self.parameters()`, and for other\n    attributes, we just do the shallow copy.\n    \"\"\"\n    try:\n        # If the instance itself is copyable, we can just deep copy it.\n        # Otherwise we will have to create a new instance and copy over the attributes one by one.\n        return copy.deepcopy(self)\n    except Exception:\n        pass\n\n    # Create an empty instance.\n    new_instance = self.__class__.__new__(self.__class__)\n    # Set attribuetes of the copied instance.\n    for attr, value in self.__dict__.items():\n        if isinstance(value, BaseModule):\n            setattr(new_instance, attr, value.deepcopy())\n        else:\n            try:\n                # Try to deep copy the attribute\n                setattr(new_instance, attr, copy.deepcopy(value))\n            except Exception:\n                logging.warning(\n                    f\"Failed to deep copy attribute '{attr}' of {self.__class__.__name__}, \"\n                    \"falling back to shallow copy or reference copy.\"\n                )\n                try:\n                    # Fallback to shallow copy if deep copy fails\n                    setattr(new_instance, attr, copy.copy(value))\n                except Exception:\n                    # If even the shallow copy fails, we just copy over the reference.\n                    setattr(new_instance, attr, value)\n\n    return new_instance\n</code></pre>"},{"location":"api/storages/#evoagentx.storages.StorageHandler","title":"StorageHandler","text":"<pre><code>StorageHandler(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModule</code>, <code>ABC</code></p> <p>An interface for all the storage handlers. </p> <p>StorageHandler defines an abstraction of storage used for reading and writing data (such as memory, agents, workflow, ect.).  It can be implemented in various ways such as file storage, database storage, cloud storage, etc.</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/storages/#evoagentx.storages.StorageHandler.load","title":"load  <code>abstractmethod</code>","text":"<pre><code>load(*args, **kwargs)\n</code></pre> <p>Load all data from the underlying storage (file, database, etc.)</p> Source code in <code>evoagentx/storages/base.py</code> <pre><code>@abstractmethod\ndef load(self, *args, **kwargs):\n    \"\"\"\n    Load all data from the underlying storage (file, database, etc.)\n    \"\"\"\n    pass \n</code></pre>"},{"location":"api/storages/#evoagentx.storages.StorageHandler.save","title":"save  <code>abstractmethod</code>","text":"<pre><code>save(*args, **kwargs)\n</code></pre> <p>Save all data to the underlying storage at once.</p> Source code in <code>evoagentx/storages/base.py</code> <pre><code>@abstractmethod\ndef save(self, *args, **kwargs):\n    \"\"\"\n    Save all data to the underlying storage at once.\n    \"\"\"\n    pass \n</code></pre>"},{"location":"api/storages/#evoagentx.storages.StorageHandler.load_memory","title":"load_memory  <code>abstractmethod</code>","text":"<pre><code>load_memory(memory_id: str, **kwargs) -&gt; Dict[str, Any]\n</code></pre> <p>Load a single long term memory data. </p> <p>Parameters:</p> Name Type Description Default <code>memory_id</code> <code>str</code> <p>the id of the long term memory. </p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: the data that can be used to create an LongTermMemory instance.</p> Source code in <code>evoagentx/storages/base.py</code> <pre><code>@abstractmethod\ndef load_memory(self, memory_id: str, **kwargs) -&gt; Dict[str, Any]:\n    \"\"\"\n    Load a single long term memory data. \n\n    Args:\n        memory_id (str): the id of the long term memory. \n\n    Returns:\n        Dict[str, Any]: the data that can be used to create an LongTermMemory instance. \n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/storages/#evoagentx.storages.StorageHandler.save_memory","title":"save_memory  <code>abstractmethod</code>","text":"<pre><code>save_memory(memory_data: Dict[str, Any], **kwargs)\n</code></pre> <p>Save or update a single memory. </p> <p>Parameters:</p> Name Type Description Default <code>memory_data</code> <code>Dict[str, Any]</code> <p>the long term memory's data.</p> required Source code in <code>evoagentx/storages/base.py</code> <pre><code>@abstractmethod\ndef save_memory(self, memory_data: Dict[str, Any], **kwargs):\n    \"\"\"\n    Save or update a single memory. \n\n    Args:\n        memory_data (Dict[str, Any]): the long term memory's data. \n    \"\"\"\n    pass \n</code></pre>"},{"location":"api/storages/#evoagentx.storages.StorageHandler.load_agent","title":"load_agent  <code>abstractmethod</code>","text":"<pre><code>load_agent(agent_name: str, **kwargs) -&gt; Dict[str, Any]\n</code></pre> <p>Load a single agent's data.</p> <p>Parameters:</p> Name Type Description Default <code>agent_name</code> <code>str</code> <p>use agent name (unique identifier) to retrieve the data. </p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: the data that can be used to create an Agent instance.</p> Source code in <code>evoagentx/storages/base.py</code> <pre><code>@abstractmethod\ndef load_agent(self, agent_name: str, **kwargs) -&gt; Dict[str, Any]:\n    # TODO\n    \"\"\"\n    Load a single agent's data.\n\n    Args: \n        agent_name (str): use agent name (unique identifier) to retrieve the data. \n\n    Returns:\n        Dict[str, Any]: the data that can be used to create an Agent instance. \n    \"\"\"\n    pass \n</code></pre>"},{"location":"api/storages/#evoagentx.storages.StorageHandler.remove_agent","title":"remove_agent  <code>abstractmethod</code>","text":"<pre><code>remove_agent(agent_name: str, **kwargs)\n</code></pre> <p>Remove an agent from storage if the agent exists. </p> <p>Parameters:</p> Name Type Description Default <code>agent_name</code> <code>str</code> <p>The name of the agent to be deleted.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for the operation.</p> <code>{}</code> Source code in <code>evoagentx/storages/base.py</code> <pre><code>@abstractmethod\ndef remove_agent(self, agent_name: str, **kwargs):\n    # TODO \n    \"\"\"\n    Remove an agent from storage if the agent exists. \n\n    Args:\n        agent_name: The name of the agent to be deleted.\n        **kwargs (Any): Additional keyword arguments for the operation.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/storages/#evoagentx.storages.StorageHandler.save_agent","title":"save_agent  <code>abstractmethod</code>","text":"<pre><code>save_agent(agent_data: Dict[str, Any], **kwargs)\n</code></pre> <p>Save or update a single agent's data.</p> <p>Parameters:</p> Name Type Description Default <code>agent_data</code> <code>Dict[str, Any]</code> <p>the agent's data.</p> required Source code in <code>evoagentx/storages/base.py</code> <pre><code>@abstractmethod\ndef save_agent(self, agent_data: Dict[str, Any], **kwargs):\n    \"\"\"\n    Save or update a single agent's data.\n\n    Args:\n        agent_data (Dict[str, Any]): the agent's data. \n    \"\"\"\n    pass \n</code></pre>"},{"location":"api/storages/#evoagentx.storages.StorageHandler.load_workflow","title":"load_workflow  <code>abstractmethod</code>","text":"<pre><code>load_workflow(workflow_id: str, **kwargs) -&gt; Dict[str, Any]\n</code></pre> <p>Load a single workflow's data. </p> <p>Parameters:</p> Name Type Description Default <code>workflow_id</code> <code>str</code> <p>the id of the workflow. </p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: the data that can be used to create a WorkFlow instance.</p> Source code in <code>evoagentx/storages/base.py</code> <pre><code>@abstractmethod\ndef load_workflow(self, workflow_id: str, **kwargs) -&gt; Dict[str, Any]:\n    \"\"\"\n    Load a single workflow's data. \n\n    Args: \n        workflow_id (str): the id of the workflow. \n\n    Returns:\n        Dict[str, Any]: the data that can be used to create a WorkFlow instance.\n    \"\"\"\n    pass \n</code></pre>"},{"location":"api/storages/#evoagentx.storages.StorageHandler.save_workflow","title":"save_workflow  <code>abstractmethod</code>","text":"<pre><code>save_workflow(workflow_data: Dict[str, Any], **kwargs)\n</code></pre> <p>Save or update a workflow's data.</p> <p>Parameters:</p> Name Type Description Default <code>workflow_data</code> <code>Dict[str, Any]</code> <p>the workflow's data.</p> required Source code in <code>evoagentx/storages/base.py</code> <pre><code>@abstractmethod\ndef save_workflow(self, workflow_data: Dict[str, Any], **kwargs):\n    \"\"\"\n    Save or update a workflow's data.\n\n    Args:\n        workflow_data (Dict[str, Any]): the workflow's data.\n    \"\"\"\n    pass \n</code></pre>"},{"location":"api/tools/","title":"\ud83d\udee0\ufe0f Tools","text":""},{"location":"api/tools/#evoagentx.tools","title":"evoagentx.tools","text":""},{"location":"api/tools/#evoagentx.tools.BaseModule","title":"BaseModule","text":"<pre><code>BaseModule(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>Base module class that serves as the foundation for all modules in the EvoAgentX framework.</p> <p>This class provides serialization/deserialization capabilities, supports creating instances from dictionaries, JSON, or files, and exporting instances to these formats.</p> <p>Attributes:</p> Name Type Description <code>class_name</code> <code>str</code> <p>The class name, defaults to None but is automatically set during subclass initialization</p> <code>model_config</code> <p>Pydantic model configuration that controls type matching and behavior</p> <p>Initializes a BaseModule instance.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Keyword arguments used to initialize the instance</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValidationError</code> <p>When parameter validation fails</p> <code>Exception</code> <p>When other errors occur during initialization</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/tools/#evoagentx.tools.BaseModule.kwargs","title":"kwargs  <code>property</code>","text":"<pre><code>kwargs: dict\n</code></pre> <p>Returns the extra fields of the model.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary containing all extra keyword arguments</p>"},{"location":"api/tools/#evoagentx.tools.BaseModule.__init_subclass__","title":"__init_subclass__","text":"<pre><code>__init_subclass__(**kwargs)\n</code></pre> <p>Subclass initialization method that automatically sets the class_name attribute.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Type</code> <p>The subclass being initialized</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments</p> <code>{}</code> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init_subclass__(cls, **kwargs):\n    \"\"\"\n    Subclass initialization method that automatically sets the class_name attribute.\n\n    Args:\n        cls (Type): The subclass being initialized\n        **kwargs (Any): Additional keyword arguments\n    \"\"\"\n    super().__init_subclass__(**kwargs)\n    cls.class_name = cls.__name__\n</code></pre>"},{"location":"api/tools/#evoagentx.tools.BaseModule.init_module","title":"init_module","text":"<pre><code>init_module()\n</code></pre> <p>Module initialization method that subclasses can override to provide additional initialization logic.</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def init_module(self):\n    \"\"\"\n    Module initialization method that subclasses can override to provide additional initialization logic.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/tools/#evoagentx.tools.BaseModule.__str__","title":"__str__","text":"<pre><code>__str__() -&gt; str\n</code></pre> <p>Returns a string representation of the object.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>String representation of the object</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"\n    Returns a string representation of the object.\n\n    Returns:\n        str: String representation of the object\n    \"\"\"\n    return self.to_str()\n</code></pre>"},{"location":"api/tools/#evoagentx.tools.BaseModule.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: Dict[str, Any], **kwargs) -&gt; BaseModule\n</code></pre> <p>Instantiate the BaseModule from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>Dictionary containing instance data</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments, can include log to control logging output</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>BaseModule</code> <code>BaseModule</code> <p>The created module instance</p> <p>Raises:</p> Type Description <code>Exception</code> <p>When errors occur during initialization</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: Dict[str, Any], **kwargs) -&gt; \"BaseModule\":\n    \"\"\"\n    Instantiate the BaseModule from a dictionary.\n\n    Args:\n        data: Dictionary containing instance data\n        **kwargs (Any): Additional keyword arguments, can include log to control logging output\n\n    Returns:\n        BaseModule: The created module instance\n\n    Raises:\n        Exception: When errors occur during initialization\n    \"\"\"\n    use_logger = kwargs.get(\"log\", True)\n    with exception_buffer() as buffer:\n        try:\n            class_name = data.get(\"class_name\", None)\n            if class_name:\n                cls = MODULE_REGISTRY.get_module(class_name)\n            module = cls._create_instance(data)\n            # module = cls.model_validate(data)\n            if len(buffer.exceptions) &gt; 0:\n                error_message = get_base_module_init_error_message(cls, data, buffer.exceptions)\n                if use_logger:\n                    logger.error(error_message)\n                raise Exception(get_error_message(buffer.exceptions))\n        finally:\n            pass\n    return module\n</code></pre>"},{"location":"api/tools/#evoagentx.tools.BaseModule.from_json","title":"from_json  <code>classmethod</code>","text":"<pre><code>from_json(content: str, **kwargs) -&gt; BaseModule\n</code></pre> <p>Construct the BaseModule from a JSON string.</p> <p>This method uses yaml.safe_load to parse the JSON string into a Python object, which supports more flexible parsing than standard json.loads (including handling single quotes, trailing commas, etc). The parsed data is then passed to from_dict to create the instance.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>JSON string</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments, can include <code>log</code> to control logging output</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>BaseModule</code> <code>BaseModule</code> <p>The created module instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When the input is not a valid JSON string</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>@classmethod\ndef from_json(cls, content: str, **kwargs) -&gt; \"BaseModule\":\n    \"\"\"\n    Construct the BaseModule from a JSON string.\n\n    This method uses yaml.safe_load to parse the JSON string into a Python object,\n    which supports more flexible parsing than standard json.loads (including handling\n    single quotes, trailing commas, etc). The parsed data is then passed to from_dict\n    to create the instance.\n\n    Args:\n        content: JSON string\n        **kwargs (Any): Additional keyword arguments, can include `log` to control logging output\n\n    Returns:\n        BaseModule: The created module instance\n\n    Raises:\n        ValueError: When the input is not a valid JSON string\n    \"\"\"\n    use_logger = kwargs.get(\"log\", True)\n    try:\n        data = yaml.safe_load(content)\n    except Exception:\n        error_message = f\"Can not instantiate {cls.__name__}. The input to {cls.__name__}.from_json is not a valid JSON string.\"\n        if use_logger:\n            logger.error(error_message)\n        raise ValueError(error_message)\n\n    if not isinstance(data, (list, dict)):\n        error_message = f\"Can not instantiate {cls.__name__}. The input to {cls.__name__}.from_json is not a valid JSON string.\"\n        if use_logger:\n            logger.error(error_message)\n        raise ValueError(error_message)\n\n    return cls.from_dict(data, log=use_logger)\n</code></pre>"},{"location":"api/tools/#evoagentx.tools.BaseModule.from_str","title":"from_str  <code>classmethod</code>","text":"<pre><code>from_str(content: str, **kwargs) -&gt; BaseModule\n</code></pre> <p>Construct the BaseModule from a string that may contain JSON.</p> <p>This method is more forgiving than <code>from_json</code> as it can extract valid JSON objects embedded within larger text. It uses <code>parse_json_from_text</code> to extract  all potential JSON strings from the input text, then tries to create an instance  from each extracted JSON string until successful.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>Text that may contain JSON strings</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments, can include <code>log</code> to control logging output</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>BaseModule</code> <code>BaseModule</code> <p>The created module instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When the input does not contain valid JSON strings or the JSON is incompatible with the class</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>@classmethod\ndef from_str(cls, content: str, **kwargs) -&gt; \"BaseModule\":\n    \"\"\"\n    Construct the BaseModule from a string that may contain JSON.\n\n    This method is more forgiving than `from_json` as it can extract valid JSON\n    objects embedded within larger text. It uses `parse_json_from_text` to extract \n    all potential JSON strings from the input text, then tries to create an instance \n    from each extracted JSON string until successful.\n\n    Args:\n        content: Text that may contain JSON strings\n        **kwargs (Any): Additional keyword arguments, can include `log` to control logging output\n\n    Returns:\n        BaseModule: The created module instance\n\n    Raises:\n        ValueError: When the input does not contain valid JSON strings or the JSON is incompatible with the class\n    \"\"\"\n    use_logger = kwargs.get(\"log\", True)\n\n    extracted_json_list = parse_json_from_text(content)\n    if len(extracted_json_list) == 0:\n        error_message = f\"The input to {cls.__name__}.from_str does not contain any valid JSON str.\"\n        if use_logger:\n            logger.error(error_message)\n        raise ValueError(error_message)\n\n    module = None\n    for json_str in extracted_json_list:\n        try:\n            module = cls.from_json(json_str, log=False)\n        except Exception:\n            continue\n        break\n\n    if module is None:\n        error_message = f\"Can not instantiate {cls.__name__}. The input to {cls.__name__}.from_str either does not contain a valide JSON str, or the JSON str is incomplete or incompatable (incorrect variables or types) with {cls.__name__}.\"\n        error_message += f\"\\nInput:\\n{content}\"\n        if use_logger:\n            logger.error(error_message)\n        raise ValueError(error_message)\n\n    return module\n</code></pre>"},{"location":"api/tools/#evoagentx.tools.BaseModule.load_module","title":"load_module  <code>classmethod</code>","text":"<pre><code>load_module(path: str, **kwargs) -&gt; dict\n</code></pre> <p>Load the values for a module from a file.</p> <p>By default, it opens the specified file and uses <code>yaml.safe_load</code> to parse its contents  into a Python object (typically a dictionary).</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path of the file</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The JSON object instantiated from the file</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>@classmethod \ndef load_module(cls, path: str, **kwargs) -&gt; dict:\n    \"\"\"\n    Load the values for a module from a file.\n\n    By default, it opens the specified file and uses `yaml.safe_load` to parse its contents \n    into a Python object (typically a dictionary).\n\n    Args:\n        path: The path of the file\n        **kwargs (Any): Additional keyword arguments\n\n    Returns:\n        dict: The JSON object instantiated from the file\n    \"\"\"\n    with open(path, mode=\"r\", encoding=\"utf-8\") as file:\n        content = yaml.safe_load(file.read())\n    return content\n</code></pre>"},{"location":"api/tools/#evoagentx.tools.BaseModule.from_file","title":"from_file  <code>classmethod</code>","text":"<pre><code>from_file(path: str, load_function: Callable = None, **kwargs) -&gt; BaseModule\n</code></pre> <p>Construct the BaseModule from a file.</p> <p>This method reads and parses a file into a data structure, then creates a module instance from that data. It first verifies that the file exists, then uses either the provided <code>load_function</code> or the default <code>load_module</code> method to read and parse the file content, and finally calls <code>from_dict</code> to create the instance.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path of the file</p> required <code>load_function</code> <code>Callable</code> <p>The function used to load the data, takes a file path as input and returns a JSON object</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments, can include <code>log</code> to control logging output</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>BaseModule</code> <code>BaseModule</code> <p>The created module instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When the file does not exist</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>@classmethod\ndef from_file(cls, path: str, load_function: Callable=None, **kwargs) -&gt; \"BaseModule\":\n    \"\"\"\n    Construct the BaseModule from a file.\n\n    This method reads and parses a file into a data structure, then creates\n    a module instance from that data. It first verifies that the file exists,\n    then uses either the provided `load_function` or the default `load_module`\n    method to read and parse the file content, and finally calls `from_dict`\n    to create the instance.\n\n    Args:\n        path: The path of the file\n        load_function: The function used to load the data, takes a file path as input and returns a JSON object\n        **kwargs (Any): Additional keyword arguments, can include `log` to control logging output\n\n    Returns:\n        BaseModule: The created module instance\n\n    Raises:\n        ValueError: When the file does not exist\n    \"\"\"\n    use_logger = kwargs.get(\"log\", True)\n    if not os.path.exists(path):\n        error_message = f\"File \\\"{path}\\\" does not exist!\"\n        if use_logger:\n            logger.error(error_message)\n        raise ValueError(error_message)\n\n    function = load_function or cls.load_module\n    content = function(path, **kwargs)\n    module = cls.from_dict(content, log=use_logger)\n\n    return module\n</code></pre>"},{"location":"api/tools/#evoagentx.tools.BaseModule.to_dict","title":"to_dict","text":"<pre><code>to_dict(exclude_none: bool = True, ignore: List[str] = [], **kwargs) -&gt; dict\n</code></pre> <p>Convert the BaseModule to a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>exclude_none</code> <code>bool</code> <p>Whether to exclude fields with None values</p> <code>True</code> <code>ignore</code> <code>List[str]</code> <p>List of field names to ignore</p> <code>[]</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary containing the object data</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def to_dict(self, exclude_none: bool = True, ignore: List[str] = [], **kwargs) -&gt; dict:\n    \"\"\"\n    Convert the BaseModule to a dictionary.\n\n    Args:\n        exclude_none: Whether to exclude fields with None values\n        ignore: List of field names to ignore\n        **kwargs (Any): Additional keyword arguments\n\n    Returns:\n        dict: Dictionary containing the object data\n    \"\"\"\n    data = {}\n    for field_name, _ in type(self).model_fields.items():\n        if field_name in ignore:\n            continue\n        field_value = getattr(self, field_name, None)\n        if exclude_none and field_value is None:\n            continue\n        if isinstance(field_value, BaseModule):\n            data[field_name] = field_value.to_dict(exclude_none=exclude_none, ignore=ignore)\n        elif isinstance(field_value, list):\n            data[field_name] = [\n                item.to_dict(exclude_none=exclude_none, ignore=ignore) if isinstance(item, BaseModule) else item\n                for item in field_value\n            ]\n        elif isinstance(field_value, dict):\n            data[field_name] = {\n                key: value.to_dict(exclude_none=exclude_none, ignore=ignore) if isinstance(value, BaseModule) else value\n                for key, value in field_value.items()\n            }\n        else:\n            data[field_name] = field_value\n\n    return data\n</code></pre>"},{"location":"api/tools/#evoagentx.tools.BaseModule.to_json","title":"to_json","text":"<pre><code>to_json(use_indent: bool = False, ignore: List[str] = [], **kwargs) -&gt; str\n</code></pre> <p>Convert the BaseModule to a JSON string.</p> <p>Parameters:</p> Name Type Description Default <code>use_indent</code> <code>bool</code> <p>Whether to use indentation</p> <code>False</code> <code>ignore</code> <code>List[str]</code> <p>List of field names to ignore</p> <code>[]</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The JSON string</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def to_json(self, use_indent: bool=False, ignore: List[str] = [], **kwargs) -&gt; str:\n    \"\"\"\n    Convert the BaseModule to a JSON string.\n\n    Args:\n        use_indent: Whether to use indentation\n        ignore: List of field names to ignore\n        **kwargs (Any): Additional keyword arguments\n\n    Returns:\n        str: The JSON string\n    \"\"\"\n    if use_indent:\n        kwargs[\"indent\"] = kwargs.get(\"indent\", 4)\n    else:\n        kwargs.pop(\"indent\", None)\n    if kwargs.get(\"default\", None) is None:\n        kwargs[\"default\"] = custom_serializer\n    data = self.to_dict(exclude_none=True)\n    for ignore_field in ignore:\n        data.pop(ignore_field, None)\n    return json.dumps(data, **kwargs)\n</code></pre>"},{"location":"api/tools/#evoagentx.tools.BaseModule.to_str","title":"to_str","text":"<pre><code>to_str(**kwargs) -&gt; str\n</code></pre> <p>Convert the BaseModule to a string. Use .to_json to output JSON string by default.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The string</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def to_str(self, **kwargs) -&gt; str:\n    \"\"\"\n    Convert the BaseModule to a string. Use .to_json to output JSON string by default.\n\n    Args:\n        **kwargs (Any): Additional keyword arguments\n\n    Returns:\n        str: The string\n    \"\"\"\n    return self.to_json(use_indent=False)\n</code></pre>"},{"location":"api/tools/#evoagentx.tools.BaseModule.save_module","title":"save_module","text":"<pre><code>save_module(path: str, ignore: List[str] = [], **kwargs) -&gt; str\n</code></pre> <p>Save the BaseModule to a file.</p> <p>This method will set non-serializable objects to None by default. If you want to save non-serializable objects, override this method. Remember to also override the <code>load_module</code> function to ensure the loaded object can be correctly parsed by <code>cls.from_dict</code>.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to save the file</p> required <code>ignore</code> <code>List[str]</code> <p>List of field names to ignore</p> <code>[]</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The path where the file is saved, same as the input path</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def save_module(self, path: str, ignore: List[str] = [], **kwargs)-&gt; str:\n    \"\"\"\n    Save the BaseModule to a file.\n\n    This method will set non-serializable objects to None by default.\n    If you want to save non-serializable objects, override this method.\n    Remember to also override the `load_module` function to ensure the loaded\n    object can be correctly parsed by `cls.from_dict`.\n\n    Args:\n        path: The path to save the file\n        ignore: List of field names to ignore\n        **kwargs (Any): Additional keyword arguments\n\n    Returns:\n        str: The path where the file is saved, same as the input path\n    \"\"\"\n    logger.info(\"Saving {} to {}\", self.__class__.__name__, path)\n    return save_json(self.to_json(use_indent=True, default=lambda x: None, ignore=ignore), path=path)\n</code></pre>"},{"location":"api/tools/#evoagentx.tools.BaseModule.deepcopy","title":"deepcopy","text":"<pre><code>deepcopy()\n</code></pre> <p>Deep copy the module.</p> <p>This is a tweak to the default python deepcopy that only deep copies <code>self.parameters()</code>, and for other attributes, we just do the shallow copy.</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def deepcopy(self):\n    \"\"\"Deep copy the module.\n\n    This is a tweak to the default python deepcopy that only deep copies `self.parameters()`, and for other\n    attributes, we just do the shallow copy.\n    \"\"\"\n    try:\n        # If the instance itself is copyable, we can just deep copy it.\n        # Otherwise we will have to create a new instance and copy over the attributes one by one.\n        return copy.deepcopy(self)\n    except Exception:\n        pass\n\n    # Create an empty instance.\n    new_instance = self.__class__.__new__(self.__class__)\n    # Set attribuetes of the copied instance.\n    for attr, value in self.__dict__.items():\n        if isinstance(value, BaseModule):\n            setattr(new_instance, attr, value.deepcopy())\n        else:\n            try:\n                # Try to deep copy the attribute\n                setattr(new_instance, attr, copy.deepcopy(value))\n            except Exception:\n                logging.warning(\n                    f\"Failed to deep copy attribute '{attr}' of {self.__class__.__name__}, \"\n                    \"falling back to shallow copy or reference copy.\"\n                )\n                try:\n                    # Fallback to shallow copy if deep copy fails\n                    setattr(new_instance, attr, copy.copy(value))\n                except Exception:\n                    # If even the shallow copy fails, we just copy over the reference.\n                    setattr(new_instance, attr, value)\n\n    return new_instance\n</code></pre>"},{"location":"api/tools/#evoagentx.tools.Tool","title":"Tool","text":"<pre><code>Tool(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModule</code></p> <p>An interface for all the tools.</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/workflow/","title":"\ud83d\udd01 Workflow","text":""},{"location":"api/workflow/#evoagentx.workflow","title":"evoagentx.workflow","text":""},{"location":"api/workflow/#evoagentx.workflow.WorkFlowGenerator","title":"WorkFlowGenerator","text":"<pre><code>WorkFlowGenerator(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModule</code></p> <p>Automated workflow generation system based on high-level goals.</p> <p>The WorkFlowGenerator is responsible for creating complete workflow graphs from high-level goals or task descriptions. It breaks down the goal into subtasks, creates the necessary dependency connections between tasks, and assigns or generates appropriate agents for each task.</p> <p>Attributes:</p> Name Type Description <code>llm</code> <code>Optional[BaseLLM]</code> <p>Language model used for generation and planning</p> <code>task_planner</code> <code>Optional[TaskPlanner]</code> <p>Component responsible for breaking down goals into subtasks</p> <code>agent_generator</code> <code>Optional[AgentGenerator]</code> <p>Component responsible for agent assignment or creation</p> <code>workflow_reviewer</code> <code>Optional[WorkFlowReviewer]</code> <p>Component for reviewing and improving workflows</p> <code>num_turns</code> <code>Optional[PositiveInt]</code> <p>Number of refinement iterations for the workflow</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/workflow/#evoagentx.workflow.WorkFlowGraph","title":"WorkFlowGraph","text":"<pre><code>WorkFlowGraph(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModule</code></p> <p>Represents a complete workflow as a directed graph.</p> <p>WorkFlowGraph models a workflow as a directed graph where nodes represent tasks and edges represent dependencies and data flow between tasks. It provides methods for constructing, validating, traversing, and executing workflows.</p> <p>The graph structure supports advanced features like detecting and handling loops, determining execution order, and tracking execution state.</p> <p>Attributes:</p> Name Type Description <code>goal</code> <code>str</code> <p>The high-level objective of this workflow</p> <code>nodes</code> <code>Optional[List[WorkFlowNode]]</code> <p>List of WorkFlowNode instances representing tasks</p> <code>edges</code> <code>Optional[List[WorkFlowEdge]]</code> <p>List of WorkFlowEdge instances representing dependencies</p> <code>graph</code> <code>Optional[Union[MultiDiGraph, WorkFlowGraph]]</code> <p>Internal NetworkX MultiDiGraph or another WorkFlowGraph</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/workflow/#evoagentx.workflow.WorkFlowGraph.edge_exists","title":"edge_exists","text":"<pre><code>edge_exists(edge: Union[Tuple[str, str], WorkFlowEdge], **attr_filters) -&gt; bool\n</code></pre> <p>Check whether an edge exists in the workflow graph. The input <code>edge</code> can either be a tuple or a WorkFlowEdge instance.</p> <ol> <li>If a tuple is passed, it should be (source, target). The function will only determin whether there is an edge between the source node and the target node.  If attr_filters is passed, they will also be used to match the edge attributes. </li> <li>If a WorkFlowEdge is passed, it will use the eq method in WorkFlowEdge to determine </li> </ol> <pre><code>edge (Union[Tuple[str, str], WorkFlowEdge]):\n    - If a tuple is provided, it should be in the format `(source, target)`. \n    The method will check whether there is an edge between the source and target nodes.\n    If `attr_filters` are provided, they will be used to match edge attributes.\n    - If a WorkFlowEdge instance is provided, the method will use the `__eq__` method in WorkFlowEdge \n    to determine whether the edge exists.\n\nattr_filters (dict, optional):\n    Additional attributes to filter edges when `edge` is a tuple.\n</code></pre> <pre><code>bool: True if the edge exists and matches the filters (if provided); False otherwise.\n</code></pre> Source code in <code>evoagentx/workflow/workflow_graph.py</code> <pre><code>def edge_exists(self, edge: Union[Tuple[str, str], WorkFlowEdge], **attr_filters) -&gt; bool:\n\n    \"\"\"\n    Check whether an edge exists in the workflow graph. The input `edge` can either be a tuple or a WorkFlowEdge instance.\n\n    1. If a tuple is passed, it should be (source, target). The function will only determin whether there is an edge between the source node and the target node. \n    If attr_filters is passed, they will also be used to match the edge attributes. \n    2. If a WorkFlowEdge is passed, it will use the __eq__ method in WorkFlowEdge to determine \n\n    Parameters:\n    ----------\n        edge (Union[Tuple[str, str], WorkFlowEdge]):\n            - If a tuple is provided, it should be in the format `(source, target)`. \n            The method will check whether there is an edge between the source and target nodes.\n            If `attr_filters` are provided, they will be used to match edge attributes.\n            - If a WorkFlowEdge instance is provided, the method will use the `__eq__` method in WorkFlowEdge \n            to determine whether the edge exists.\n\n        attr_filters (dict, optional):\n            Additional attributes to filter edges when `edge` is a tuple.\n\n    Returns:\n    -------\n        bool: True if the edge exists and matches the filters (if provided); False otherwise.\n    \"\"\"\n    if isinstance(edge, tuple):\n        assert len(edge) == 2, \"edge must be a tuple (source, target) or WorkFlowEdge instance\"\n        source, target = edge \n        return self._edge_exists(source, target, **attr_filters)\n    elif isinstance(edge, WorkFlowEdge):\n        return edge in self.edges \n    else:\n        raise TypeError(\"edge must be a tuple (source, target) or WorkFlowEdge instance\")\n</code></pre>"},{"location":"api/workflow/#evoagentx.workflow.WorkFlowGraph.list_nodes","title":"list_nodes","text":"<pre><code>list_nodes() -&gt; List[str]\n</code></pre> <p>return the names of all nodes</p> Source code in <code>evoagentx/workflow/workflow_graph.py</code> <pre><code>def list_nodes(self) -&gt; List[str]:\n    \"\"\"\n    return the names of all nodes \n    \"\"\"\n    return [node.name for node in self.nodes]\n</code></pre>"},{"location":"api/workflow/#evoagentx.workflow.WorkFlowGraph.get_node","title":"get_node","text":"<pre><code>get_node(node_name: str) -&gt; WorkFlowNode\n</code></pre> <p>return a WorkFlowNode instance based on its name.</p> Source code in <code>evoagentx/workflow/workflow_graph.py</code> <pre><code>def get_node(self, node_name: str) -&gt; WorkFlowNode:\n    \"\"\"\n    return a WorkFlowNode instance based on its name.\n    \"\"\"\n    if not self.node_exists(node=node_name):\n        raise KeyError(f\"{node_name} is an invalid node name. Currently available node names: {self.list_nodes()}\")\n    return self.graph.nodes[node_name][\"ref\"]\n</code></pre>"},{"location":"api/workflow/#evoagentx.workflow.WorkFlowGraph.reset_graph","title":"reset_graph","text":"<pre><code>reset_graph()\n</code></pre> <p>set the status of all nodes to pending</p> Source code in <code>evoagentx/workflow/workflow_graph.py</code> <pre><code>def reset_graph(self):\n    \"\"\"\n    set the status of all nodes to pending\n    \"\"\"\n    for node in self.nodes:\n        node.set_status(WorkFlowNodeState.PENDING)\n</code></pre>"},{"location":"api/workflow/#evoagentx.workflow.WorkFlowGraph.set_node_status","title":"set_node_status","text":"<pre><code>set_node_status(node: Union[str, WorkFlowNode], new_state: WorkFlowNodeState) -&gt; bool\n</code></pre> <p>Update the state of a specific node. </p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Union[str, WorkFlowNode]</code> <p>The name of a node or the node instance.</p> required <code>new_state</code> <code>WorkFlowNodeState</code> <p>The new state to set.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the state was updated successfully, False otherwise.</p> Source code in <code>evoagentx/workflow/workflow_graph.py</code> <pre><code>def set_node_status(self, node: Union[str, WorkFlowNode], new_state: WorkFlowNodeState) -&gt; bool:\n    \"\"\"\n    Update the state of a specific node. \n\n    Args:\n        node (Union[str, WorkFlowNode]): The name of a node or the node instance.\n        new_state (WorkFlowNodeState): The new state to set.\n\n    Returns:\n        bool: True if the state was updated successfully, False otherwise.\n    \"\"\"\n    flag = False\n    try:\n        if isinstance(node, str):\n            node = self.get_node(node_name=node)\n        node.set_status(new_state)\n        flag = True\n    except Exception as e:\n        raise ValueError(f\"An error occurs when setting node status: {e}\")\n    return flag\n</code></pre>"},{"location":"api/workflow/#evoagentx.workflow.WorkFlowGraph.filter_completed_nodes","title":"filter_completed_nodes","text":"<pre><code>filter_completed_nodes(nodes: List[Union[str, WorkFlowNode]]) -&gt; List[str]\n</code></pre> <p>remove completed nodes from <code>nodes</code></p> Source code in <code>evoagentx/workflow/workflow_graph.py</code> <pre><code>def filter_completed_nodes(self, nodes: List[Union[str, WorkFlowNode]]) -&gt; List[str]:\n    \"\"\"\n    remove completed nodes from `nodes`\n    \"\"\"\n    node_names = [node if isinstance(node, str) else node.name for node in nodes]\n    uncompleted_nodes = [] \n    for node_name in node_names:\n        if self.get_node(node_name).is_complete:\n            continue\n        uncompleted_nodes.append(node_name)\n    return uncompleted_nodes\n</code></pre>"},{"location":"api/workflow/#evoagentx.workflow.WorkFlowGraph.get_candidate_children_nodes","title":"get_candidate_children_nodes","text":"<pre><code>get_candidate_children_nodes(completed_nodes: List[Union[str, WorkFlowNode]]) -&gt; List[str]\n</code></pre> <p>Return the next set of possible tasks to execute. If there are no loops in the graph, consider only the uncompleted children.  If there exists loops, also consider the previous completed tasks.</p> <p>Parameters:</p> Name Type Description Default <code>completed_nodes</code> <code>List[Union[str, WorkFlowNode]]</code> <p>A list of completed nodes.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of node names that are candidates for execution.</p> Source code in <code>evoagentx/workflow/workflow_graph.py</code> <pre><code>def get_candidate_children_nodes(self, completed_nodes: List[Union[str, WorkFlowNode]]) -&gt; List[str]:\n    \"\"\"\n    Return the next set of possible tasks to execute. If there are no loops in the graph, consider only the uncompleted children. \n    If there exists loops, also consider the previous completed tasks.\n\n    Args:\n        completed_nodes (List[Union[str, WorkFlowNode]]): A list of completed nodes.\n\n    Returns:\n        List[str]: List of node names that are candidates for execution.\n    \"\"\"\n    node_names = [node if isinstance(node, str) else node.name for node in completed_nodes]\n    has_loop = (len(self._loops) &gt; 0)\n    if has_loop:\n        # if there exists loops, we need to check the completed nodes and their children nodes\n        uncompleted_children_nodes = []\n        for node_name in node_names:\n            children_nodes = self.get_all_children_nodes(nodes=[node_name])\n            if self.is_loop_end(node=node_name):\n                current_uncompleted_children_nodes = [] \n                for child in children_nodes:\n                    if self.is_loop_start(node=child):\n                        # node_name\u662f\u4e00\u4e2a\u73af\u7684\u7ed3\u675f\u7684\u65f6\u5019\uff0c\u5982\u679c\u5b83\u7684\u5b50\u8282\u70b9\u662f\u73af\u7684\u5f00\u59cb\uff0c\u90a3\u4e48\u65e0\u8bba\u5b83\u662f\u5426completed\uff0c\u90fd\u6dfb\u52a0\u5230\u4e0b\u4e00\u6b65\u53ef\u6267\u884c\u7684\u64cd\u4f5c\n                        current_uncompleted_children_nodes.append(child)\n                    else:\n                        # node_name\u662f\u73af\u7684\u7ed3\u675f\uff0c\u4f46\u662f\u5b50\u8282\u70b9\u4e0d\u662f\u73af\u7684\u5f00\u59cb\u65f6\uff0c\u9700\u8981\u68c0\u67e5child\u662f\u5426\u5df2\u7ecfcompleted\uff0c\u53ea\u6dfb\u52a0\u672a\u5b8c\u6210\u7684\u4efb\u52a1\n                        current_uncompleted_children_nodes.extend(self.filter_completed_nodes(nodes=[child]))\n            else:\n                current_uncompleted_children_nodes = self.filter_completed_nodes(nodes=children_nodes)\n            for child in current_uncompleted_children_nodes:\n                if child not in uncompleted_children_nodes:\n                    uncompleted_children_nodes.append(child)\n    else:\n        # \u4e0d\u5b58\u5728\u73af\u7684\u65f6\u5019\u76f4\u63a5\u5f97\u5230\u6240\u6709\u7684\u5b50\u8282\u70b9\uff0c\u5e76\u53bb\u6389\u5176\u4e2d\u5df2\u5b8c\u6210\u7684\u90e8\u5206\n        children_nodes = self.get_all_children_nodes(nodes=node_names)\n        uncompleted_children_nodes = self.filter_completed_nodes(nodes=children_nodes)\n\n    return uncompleted_children_nodes\n</code></pre>"},{"location":"api/workflow/#evoagentx.workflow.WorkFlowGraph.are_dependencies_complete","title":"are_dependencies_complete","text":"<pre><code>are_dependencies_complete(node_name: str) -&gt; bool\n</code></pre> <p>Check if all predecessors for a node are complete.</p> <p>Parameters:</p> Name Type Description Default <code>node_name</code> <code>str</code> <p>The name of the task/node to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if all predecessors are complete, False otherwise.</p> Source code in <code>evoagentx/workflow/workflow_graph.py</code> <pre><code>def are_dependencies_complete(self, node_name: str) -&gt; bool:\n    \"\"\"\n    Check if all predecessors for a node are complete.\n\n    Args:\n        node_name (str): The name of the task/node to check.\n\n    Returns:\n        bool: True if all predecessors are complete, False otherwise.\n    \"\"\"\n    has_loop = (len(self._loops) &gt; 0)\n    predecessors = self.get_node_predecessors(node=node_name)\n    if has_loop and self.is_loop_start(node=node_name):\n        flag = True \n        for pre in predecessors:\n            if self.is_loop_end(pre):\n                pass \n            else:\n                flag &amp;= self.get_node(pre).is_complete\n    else:\n        flag = all(self.get_node(pre).is_complete for pre in predecessors)\n    return flag\n</code></pre>"},{"location":"api/workflow/#evoagentx.workflow.WorkFlowGraph.display","title":"display","text":"<pre><code>display()\n</code></pre> <p>Display the workflow graph with node and edge attributes. Nodes are colored based on their status.</p> Source code in <code>evoagentx/workflow/workflow_graph.py</code> <pre><code>def display(self):\n    \"\"\"\n    Display the workflow graph with node and edge attributes.\n    Nodes are colored based on their status.\n    \"\"\"\n    import matplotlib.pyplot as plt\n\n    # Define colors for node statuses\n    status_colors = {\n        WorkFlowNodeState.PENDING: 'lightgray',\n        WorkFlowNodeState.RUNNING: 'orange',\n        WorkFlowNodeState.COMPLETED: 'green',\n        WorkFlowNodeState.FAILED: 'red'\n    }\n\n    if not self.graph.nodes:\n        print(\"Graph is empty. No nodes to display.\")\n        return\n\n    # Get node colors based on their statuses\n    node_colors = [status_colors.get(self.get_node_status(node), 'lightgray') for node in self.graph.nodes]\n\n    # Prepare node labels with additional information\n    node_labels = {node: self.get_node_description(data[\"ref\"]) for node, data in self.graph.nodes(data=True)}\n\n    # Draw the graph\n    # pos = nx.shell_layout(self.graph)\n    if len(self.graph.nodes) == 1:\n        single_node = list(self.graph.nodes)[0]\n        pos = {single_node: (0, 0)}  # Place the single node at the center\n    else:\n        pos = nx.shell_layout(self.graph)\n\n    plt.figure(figsize=(12, 8))\n    nx.draw(\n        self.graph, pos, with_labels=False, node_color=node_colors, edge_color='black',\n        node_size=1500, font_size=8, font_color='black', font_weight='bold'\n    )\n\n    if len(self.graph.nodes) == 1:\n        for node, (x, y) in pos.items():\n            plt.text(x+0.005, y, node_labels[node], ha='left', va='center', fontsize=9, bbox=dict(facecolor='white', alpha=0.7))\n    else:\n        # Draw node labels next to the nodes (left-aligned)\n        # text_offsets = {node: (pos[node][0]-0.2, pos[node][1]-0.22) for node in self.graph.nodes}\n        y_positions = [y for _, y in pos.values()]\n        y_min, y_max = min(y_positions), max(y_positions)\n        lower_third_boundary = y_min + (y_max - y_min) / 3\n\n        # Adjust text offsets based on node position in the graph\n        text_offsets = {}\n        for node, (x, y) in pos.items():\n            if y &lt; lower_third_boundary:  # If in the lower third, display label above the node\n                text_offsets[node] = (x-0.2, y + 0.23)\n            else:  # Otherwise, display label below the node\n                text_offsets[node] = (x-0.2, y - 0.23)\n\n        for node, (x, y) in text_offsets.items():\n            plt.text(x, y, node_labels[node], ha='left', va='center', fontsize=9, bbox=dict(facecolor='white', alpha=0.7))\n\n    # Draw edge labels for priorities\n    edge_labels = nx.get_edge_attributes(self.graph, 'priority')\n    nx.draw_networkx_edge_labels(self.graph, pos, edge_labels=edge_labels)\n\n    # Add a legend to show node status colors\n    legend_elements = [\n        plt.Line2D([0], [0], marker='o', color='w', label=status.name, markersize=10, markerfacecolor=color)\n        for status, color in status_colors.items()\n    ]\n    plt.legend(handles=legend_elements, title=\"Workflow Node Status\", loc='upper left', fontsize='medium')\n\n    plt.title(\"Workflow Graph\")\n    plt.show()\n</code></pre>"},{"location":"api/workflow/#evoagentx.workflow.SequentialWorkFlowGraph","title":"SequentialWorkFlowGraph","text":"<pre><code>SequentialWorkFlowGraph(goal: str, tasks: List[dict], **kwargs)\n</code></pre> <p>               Bases: <code>WorkFlowGraph</code></p> <p>A linear workflow graph with a single path from start to end.</p> <p>Parameters:</p> Name Type Description Default <code>goal</code> <code>str</code> <p>The goal of the workflow.</p> required <code>tasks</code> <code>List[dict]</code> <p>A list of tasks with their descriptions and inputs. Each task should have the following format: {     \"name\": str,     \"description\": str,     \"inputs\": [{\"name\": str, \"type\": str, \"required\": bool, \"description\": str}, ...],     \"outputs\": [{\"name\": str, \"type\": str, \"required\": bool, \"description\": str}, ...],     \"prompt\": str,      \"system_prompt\" (optional): str, default is DEFAULT_SYSTEM_PROMPT,     \"output_parser\" (optional): Type[ActionOutput],     \"parse_mode\" (optional): str, default is \"str\"      \"parse_func\" (optional): Callable,     \"parse_title\" (optional): str  }</p> required Source code in <code>evoagentx/workflow/workflow_graph.py</code> <pre><code>def __init__(self, goal: str, tasks: List[dict], **kwargs):\n    nodes = self._infer_nodes_from_tasks(tasks=tasks)\n    edges = self._infer_edges_from_nodes(nodes=nodes)\n    super().__init__(goal=goal, nodes=nodes, edges=edges, **kwargs)\n</code></pre>"},{"location":"api/workflow/#evoagentx.workflow.SequentialWorkFlowGraph.get_graph_info","title":"get_graph_info","text":"<pre><code>get_graph_info(**kwargs) -&gt; dict\n</code></pre> <p>Get the information of the workflow graph.</p> Source code in <code>evoagentx/workflow/workflow_graph.py</code> <pre><code>def get_graph_info(self, **kwargs) -&gt; dict:\n    \"\"\"\n    Get the information of the workflow graph.\n    \"\"\"\n    config = {\n        \"class_name\": self.__class__.__name__,\n        \"goal\": self.goal, \n        \"tasks\": [\n            {\n                \"name\": node.name,\n                \"description\": node.description,\n                \"inputs\": [param.to_dict(ignore=[\"class_name\"]) for param in node.inputs],\n                \"outputs\": [param.to_dict(ignore=[\"class_name\"]) for param in node.outputs],\n                \"prompt\": node.agents[0].get(\"prompt\", None),\n                \"system_prompt\": node.agents[0].get(\"system_prompt\", None),\n                \"parse_mode\": node.agents[0].get(\"parse_mode\", \"str\"), \n                \"parse_func\": node.agents[0].get(\"parse_func\", None).__name__ if node.agents[0].get(\"parse_func\", None) else None,\n                \"parse_title\": node.agents[0].get(\"parse_title\", None)\n            }\n            for node in self.nodes\n        ]\n    }\n    return config\n</code></pre>"},{"location":"api/workflow/#evoagentx.workflow.SequentialWorkFlowGraph.save_module","title":"save_module","text":"<pre><code>save_module(path: str, ignore: List[str] = [], **kwargs)\n</code></pre> <p>Save the workflow graph to a module file.</p> Source code in <code>evoagentx/workflow/workflow_graph.py</code> <pre><code>def save_module(self, path: str, ignore: List[str] = [], **kwargs):\n    \"\"\"\n    Save the workflow graph to a module file.\n    \"\"\"\n    logger.info(\"Saving {} to {}\", self.__class__.__name__, path)\n    config = self.get_graph_info()\n    for ignore_key in ignore:\n        config.pop(ignore_key, None)\n    make_parent_folder(path)\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(config, f, indent=4)\n    return path\n</code></pre>"},{"location":"api/workflow/#evoagentx.workflow.WorkFlow","title":"WorkFlow","text":"<pre><code>WorkFlow(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModule</code></p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/workflow/#evoagentx.workflow.WorkFlow.execute","title":"execute","text":"<pre><code>execute(inputs: dict = {}, **kwargs) -&gt; str\n</code></pre> <p>Synchronous wrapper for async_execute. Creates a new event loop and runs the async method.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict</code> <p>Dictionary of inputs for workflow execution</p> <code>{}</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The output of the workflow execution</p> Source code in <code>evoagentx/workflow/workflow.py</code> <pre><code>def execute(self, inputs: dict = {}, **kwargs) -&gt; str:\n    \"\"\"\n    Synchronous wrapper for async_execute. Creates a new event loop and runs the async method.\n\n    Args:\n        inputs: Dictionary of inputs for workflow execution\n        **kwargs (Any): Additional keyword arguments\n\n    Returns:\n        str: The output of the workflow execution\n    \"\"\"\n    loop = asyncio.new_event_loop()\n    asyncio.set_event_loop(loop)\n    try:\n        return loop.run_until_complete(self.async_execute(inputs, **kwargs))\n    finally:\n        loop.close()\n</code></pre>"},{"location":"api/workflow/#evoagentx.workflow.WorkFlow.async_execute","title":"async_execute  <code>async</code>","text":"<pre><code>async_execute(inputs: dict = {}, **kwargs) -&gt; str\n</code></pre> <p>Asynchronously execute the workflow.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict</code> <p>Dictionary of inputs for workflow execution</p> <code>{}</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The output of the workflow execution</p> Source code in <code>evoagentx/workflow/workflow.py</code> <pre><code>async def async_execute(self, inputs: dict = {}, **kwargs) -&gt; str:\n    \"\"\"\n    Asynchronously execute the workflow.\n\n    Args:\n        inputs: Dictionary of inputs for workflow execution\n        **kwargs (Any): Additional keyword arguments\n\n    Returns:\n        str: The output of the workflow execution\n    \"\"\"\n    goal = self.graph.goal\n    # inputs.update({\"goal\": goal})\n    inputs = self._prepare_inputs(inputs)\n\n    # check the inputs and outputs of the task \n    self._validate_workflow_structure(inputs=inputs, **kwargs)\n    inp_message = Message(content=inputs, msg_type=MessageType.INPUT, wf_goal=goal)\n    self.environment.update(message=inp_message, state=TrajectoryState.COMPLETED)\n\n    failed = False\n    error_message = None\n    while not self.graph.is_complete and not failed:\n        try:\n            task: WorkFlowNode = await self.get_next_task()\n            if task is None:\n                break\n            logger.info(f\"Executing subtask: {task.name}\")\n            await self.execute_task(task=task)\n        except Exception as e:\n            failed = True\n            error_message = Message(\n                content=f\"An Error occurs when executing the workflow: {e}\",\n                msg_type=MessageType.ERROR, \n                wf_goal=goal\n            )\n            self.environment.update(message=error_message, state=TrajectoryState.FAILED, error=str(e))\n\n    if failed:\n        logger.error(error_message.content)\n        return \"Workflow Execution Failed\"\n\n    logger.info(\"Extracting WorkFlow Output ...\")\n    output: str = await self.workflow_manager.extract_output(graph=self.graph, env=self.environment)\n    return output\n</code></pre>"},{"location":"api/workflow/#evoagentx.workflow.WorkFlow.execute_task","title":"execute_task  <code>async</code>","text":"<pre><code>execute_task(task: WorkFlowNode)\n</code></pre> <p>Asynchronously execute a workflow task.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>WorkFlowNode</code> <p>The workflow node to execute</p> required Source code in <code>evoagentx/workflow/workflow.py</code> <pre><code>async def execute_task(self, task: WorkFlowNode):\n    \"\"\"\n    Asynchronously execute a workflow task.\n\n    Args:\n        task: The workflow node to execute\n    \"\"\"\n    last_executed_task = self.environment.get_last_executed_task()\n    self.graph.step(source_node=last_executed_task, target_node=task)\n    next_action: NextAction = await self.workflow_manager.schedule_next_action(\n        goal=self.graph.goal,\n        task=task, \n        agent_manager=self.agent_manager, \n        env=self.environment\n    )\n    if next_action.action_graph is not None:\n        await self._async_execute_task_by_action_graph(task=task, next_action=next_action)\n    else:\n        await self._async_execute_task_by_agents(task=task, next_action=next_action)\n    self.graph.completed(node=task)\n</code></pre>"},{"location":"api/workflow/#evoagentx.workflow.ActionGraph","title":"ActionGraph","text":"<pre><code>ActionGraph(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModule</code></p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/workflow/#evoagentx.workflow.ActionGraph.get_graph_info","title":"get_graph_info","text":"<pre><code>get_graph_info(**kwargs) -&gt; dict\n</code></pre> <p>Get the information of the action graph, including all operators from the instance.</p> Source code in <code>evoagentx/workflow/action_graph.py</code> <pre><code>def get_graph_info(self, **kwargs) -&gt; dict:\n    \"\"\"\n    Get the information of the action graph, including all operators from the instance.\n    \"\"\"\n    operators = {}\n    # the extra fields are the fields that are not defined in the Pydantic model \n    for extra_name, extra_value in self.__pydantic_extra__.items():\n        if isinstance(extra_value, Operator):\n            operators[extra_name] = extra_value\n\n    config = {\n        \"class_name\": self.__class__.__name__,\n        \"name\": self.name,\n        \"description\": self.description, \n        \"operators\": {\n            operator_name: {\n                \"class_name\": operator.__class__.__name__,\n                \"name\": operator.name,\n                \"description\": operator.description,\n                \"interface\": operator.interface,\n                \"prompt\": operator.prompt\n            }\n            for operator_name, operator in operators.items()\n        }\n    }\n    return config\n</code></pre>"},{"location":"api/workflow/#evoagentx.workflow.ActionGraph.load_module","title":"load_module  <code>classmethod</code>","text":"<pre><code>load_module(path: str, llm_config: LLMConfig = None, **kwargs) -&gt; Dict\n</code></pre> <p>Load the ActionGraph from a file.</p> Source code in <code>evoagentx/workflow/action_graph.py</code> <pre><code>@classmethod\ndef load_module(cls, path: str, llm_config: LLMConfig = None, **kwargs) -&gt; Dict:\n    \"\"\"\n    Load the ActionGraph from a file.\n    \"\"\"\n    assert llm_config is not None, \"must provide `llm_config` when using `load_module` or `from_file` to load the ActionGraph from local storage\" \n    action_graph_data = super().load_module(path, **kwargs) \n    action_graph_data[\"llm_config\"] = llm_config.to_dict()\n    return action_graph_data\n</code></pre>"},{"location":"api/workflow/#evoagentx.workflow.ActionGraph.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: Dict[str, Any], **kwargs) -&gt; ActionGraph\n</code></pre> <p>Create an ActionGraph from a dictionary.</p> Source code in <code>evoagentx/workflow/action_graph.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: Dict[str, Any], **kwargs) -&gt; \"ActionGraph\":\n    \"\"\"\n    Create an ActionGraph from a dictionary.\n    \"\"\"\n    class_name = data.get(\"class_name\", None)\n    if class_name:\n        cls = MODULE_REGISTRY.get_module(class_name)\n    operators_info = data.pop(\"operators\", None)\n    module = cls._create_instance(data)\n    if operators_info:\n        for extra_name, extra_value in module.__pydantic_extra__.items():\n            if isinstance(extra_value, Operator) and extra_name in operators_info:\n                extra_value.set_operator(operators_info[extra_name])\n    return module\n</code></pre>"},{"location":"api/workflow/#evoagentx.workflow.ActionGraph.save_module","title":"save_module","text":"<pre><code>save_module(path: str, ignore: List[str] = [], **kwargs)\n</code></pre> <p>Save the workflow graph to a module file.</p> Source code in <code>evoagentx/workflow/action_graph.py</code> <pre><code>def save_module(self, path: str, ignore: List[str] = [], **kwargs):\n    \"\"\"\n    Save the workflow graph to a module file.\n    \"\"\"\n    logger.info(\"Saving {} to {}\", self.__class__.__name__, path)\n    config = self.get_graph_info()\n    for ignore_key in ignore:\n        config.pop(ignore_key, None)\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(config, f, indent=4)\n\n    return path\n</code></pre>"},{"location":"modules/action_graph/","title":"Action Graph","text":""},{"location":"modules/action_graph/#introduction","title":"Introduction","text":"<p>The <code>ActionGraph</code> class is a fundamental component in the EvoAgentX framework for creating and executing sequences of operations (actions) within a single task. It provides a structured way to define, manage, and execute a series of operations that need to be performed in a specific order to complete a task.</p> <p>An action graph represents a collection of operators (actions) that are executed in a predefined sequence to process inputs and produce outputs. Unlike the <code>WorkFlowGraph</code> which manages multiple tasks and their dependencies at a higher level, the <code>ActionGraph</code> focuses on the detailed execution steps within a single task.</p>"},{"location":"modules/action_graph/#architecture","title":"Architecture","text":""},{"location":"modules/action_graph/#actiongraph-architecture","title":"ActionGraph Architecture","text":"<p>An <code>ActionGraph</code> consists of several key components:</p> <ol> <li> <p>Operators: </p> <p>Each operator represents a specific operation or action that can be performed as part of a task, with the following properties:</p> <ul> <li><code>name</code>: A unique identifier for the operator</li> <li><code>description</code>: Detailed description of what the operator does</li> <li><code>llm</code>: The LLM used to execute the operator</li> <li><code>outputs_format</code>: The structured format of the output of the operator</li> <li><code>interface</code>: The interface for calling the operator.</li> <li><code>prompt</code>: Template used to guide the LLM when executing this operator</li> </ul> </li> <li> <p>LLM: </p> <p>The ActionGraph uses a Language Learning Model (LLM) to execute the operators. It receives a <code>llm_config</code> as input and create an LLM instance, which will be passed to the operators for execution. The LLM provides the reasoning and generation capabilities needed to perform each action.</p> </li> <li> <p>Execution Flow:</p> <p>The ActionGraph defines a specific execution sequence:</p> <ul> <li>Actions are executed in a predetermined order (specified in the <code>execute</code> or <code>async_execute</code> method using code)</li> <li>Each action can use the results from previous actions</li> <li>The final output is produced after all actions have been executed</li> </ul> </li> </ol>"},{"location":"modules/action_graph/#comparison-with-workflowgraph","title":"Comparison with WorkFlowGraph","text":"<p>While both <code>ActionGraph</code> and <code>WorkFlowGraph</code> manage execution flows, they operate at different levels of abstraction:</p> Feature ActionGraph WorkFlowGraph Scope Single task execution Multi-task workflow orchestration Components Operators (actions) Nodes (tasks) and edges (dependencies) Focus Detailed steps within a task Relationships between different tasks Flexibility Fixed execution sequence Dynamic execution based on dependencies Primary use Define reusable task execution patterns Orchestrate complex multi-step workflows Granularity Fine-grained operations Coarse-grained tasks"},{"location":"modules/action_graph/#usage","title":"Usage","text":""},{"location":"modules/action_graph/#basic-actiongraph-creation","title":"Basic ActionGraph Creation","text":"<pre><code>from evoagentx.workflow import ActionGraph\nfrom evoagentx.workflow.operators import Custom\nfrom evoagentx.models import OpenAILLMConfig \n\n# Create LLM configuration\nllm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=\"xxx\")\n\n# Create a custom ActionGraph\nclass MyActionGraph(ActionGraph):\n    def __init__(self, llm_config, **kwargs):\n\n        name = kwargs.pop(\"name\") if \"name\" in kwargs else \"Custom Action Graph\"\n        description = kwargs.pop(\"description\") if \"description\" in kwargs else \"A custom action graph for text processing\"\n        # create an LLM instance `self._llm` based on the `llm_config` and pass it to the operators\n        super().__init__(name=name, description=description, llm_config=llm_config, **kwargs)\n        # Define operators\n        self.extract_entities = Custom(self._llm) # , prompt=\"Extract key entities from the following text: {input}\")\n        self.analyze_sentiment = Custom(self._llm) # , prompt=\"Analyze the sentiment of the following text: {input}\")\n        self.summarize = Custom(self._llm) # , prompt=\"Summarize the following text in one paragraph: {input}\")\n\n    def execute(self, text: str) -&gt; dict:\n        # Execute operators in sequence (specify the execution order of operators)\n        entities = self.extract_entities(input=text, instruction=\"Extract key entities from the provided text\")[\"response\"]\n        sentiment = self.analyze_sentiment(input=text, instruction=\"Analyze the sentiment of the provided text\")[\"response\"]\n        summary = self.summarize(input=text, instruction=\"Summarize the provided text in one paragraph\")[\"response\"]\n\n        # Return combined results\n        return {\n            \"entities\": entities,\n            \"sentiment\": sentiment,\n            \"summary\": summary\n        }\n\n# Create the action graph\naction_graph = MyActionGraph(llm_config=llm_config)\n\n# Execute the action graph\nresult = action_graph.execute(text=\"This is a test text\")\nprint(result)\n</code></pre>"},{"location":"modules/action_graph/#using-actiongraph-in-workflowgraph","title":"Using ActionGraph in WorkFlowGraph","text":"<p>You can either use <code>ActionGraph</code> directly or use it in <code>WorkFlowGraph</code> as a node. </p> <pre><code>from evoagentx.workflow.workflow_graph import WorkFlowNode, WorkFlowGraph\nfrom evoagentx.workflow.action_graph import QAActionGraph\nfrom evoagentx.core.base_config import Parameter\nfrom evoagentx.models import OpenAILLMConfig, OpenAILLM\nfrom evoagentx.workflow import WorkFlow\n\n# Create LLM configuration\nllm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=\"xxx\", stream=True, output_response=True)\nllm = OpenAILLM(llm_config)\n\n# Create an action graph\nqa_graph = QAActionGraph(llm_config=llm_config)\n\n# Create a workflow node that uses the action graph\nqa_node = WorkFlowNode(\n    name=\"QATask\",\n    description=\"Answer questions using a QA system\",\n    # input names should match the parameters in the `execute` method of the action graph\n    inputs=[Parameter(name=\"problem\", type=\"string\", description=\"The problem to answer\")],\n    outputs=[Parameter(name=\"answer\", type=\"string\", description=\"The answer to the problem\")],\n    action_graph=qa_graph  # Using action_graph instead of agents\n)\n\n# Create the workflow graph\nworkflow_graph = WorkFlowGraph(goal=\"Answer a question\", nodes=[qa_node])\n\n# define the workflow \nworkflow = WorkFlow(graph=workflow_graph, llm=llm)\n\n# Execute the workflow\nresult = workflow.execute(inputs={\"problem\": \"What is the capital of France?\"})\nprint(result)\n</code></pre> <p>Warning</p> <p>When using <code>ActionGraph</code> in <code>WorkFlowNode</code>, the <code>inputs</code> parameter of the <code>WorkFlowNode</code> should match the required parameters in the <code>execute</code> method of the <code>ActionGraph</code>. The <code>execute</code> method is expected to return a dictionary or <code>LLMOutputParser</code> instance with keys matching the names of the <code>outputs</code> in the <code>WorkFlowNode</code>. </p>"},{"location":"modules/action_graph/#saving-and-loading-an-actiongraph","title":"Saving and Loading an ActionGraph","text":"<pre><code># Save action graph\naction_graph.save_module(\"examples/output/my_action_graph.json\")\n\n# Load action graph\nfrom evoagentx.workflow.action_graph import ActionGraph\nloaded_graph = ActionGraph.from_file(\"examples/output/my_action_graph.json\", llm_config=llm_config)\n</code></pre> <p>The <code>ActionGraph</code> class provides a powerful way to define complex sequences of operations within a single task, complementing the higher-level orchestration capabilities of the <code>WorkFlowGraph</code> in the EvoAgentX framework.</p>"},{"location":"modules/agent/","title":"Agent","text":""},{"location":"modules/agent/#introduction","title":"Introduction","text":"<p>The <code>Agent</code> class is the fundamental building block for creating intelligent AI agents within the EvoAgentX framework. It provides a structured way to combine language models with actions, and memory management. </p>"},{"location":"modules/agent/#architecture","title":"Architecture","text":"<p>An Agent consists of several key components:</p> <ol> <li> <p>Large Language Model (LLM): </p> <p>The LLM is specified through the <code>llm</code> or <code>llm_config</code> parameter and serve as the building block for the agent. It is responsible for interpreting context, generating responses, and making high-level decisions. The LLM will be passed to an action for executing a specific task. </p> </li> <li> <p>Actions: </p> <p>Actions are the fundamental operational units of an agent. Each Action encapsulates a specific task and is the actual point where the LLM is invoked to reason, generate, or make decisions. While the Agent provides overall orchestration, it is through Actions that the LLM performs its core functions. Each Action is designed to do exactly one thing\u2014such as retrieving knowledge, summarizing input, or calling an API\u2014and can include the following components:</p> <ul> <li>prompt: The prompt template used to guide the LLM's behavior for this specific task.</li> <li>inputs_format: The expected structure and keys of the inputs passed into the action.</li> <li>outputs_format: The format used to interpret and parse the LLM's output.</li> <li>tools: Optional tools that can be integrated and utilized within the action.</li> </ul> </li> <li> <p>Memory Components:</p> <p>Memory allows the agent to retain and recall relevant information across interactions, enhancing contextual awareness. There are two types of memory within the EvoAgentX framework: </p> <ul> <li>Short-term memory: Maintains the intermediate conversation or context for the current task. </li> <li>Long-term memory (optional): Stores persistent knowledge that can span across sessions or tasks. This enables the agent to learn from past experiences, maintain user preferences, or build knowledge bases over time.</li> </ul> </li> </ol>"},{"location":"modules/agent/#usage","title":"Usage","text":""},{"location":"modules/agent/#basic-agent-creation","title":"Basic Agent Creation","text":"<p>In order to create an agent, you need to define the actions that the agent will perform. Each action is defined as a class that inherits from the <code>Action</code> class. The action class should define the following components: <code>name</code>, <code>description</code>, <code>prompt</code>, <code>inputs_format</code>, and <code>outputs_format</code>, and implement the <code>execute</code> method (and <code>async_exectue</code> if you want to use the agent asynchronously). </p> <pre><code>from evoagentx.agents import Agent\nfrom evoagentx.models import OpenAILLMConfig\nfrom evoagentx.actions import Action, ActionInput, ActionOutput\n\n# Define a simple action that uses the LLM to answer a question\n\nclass AnswerQuestionInput(ActionInput):\n    question: str\n\nclass AnswerQuestionOutput(ActionOutput):\n    answer: str\n\nclass AnswerQuestionAction(Action):\n\n    def __init__(\n        self, \n        name = \"answer_question\",\n        description = \"Answers a factual question using the LLM\",   \n        prompt = \"Answer the following question as accurately as possible:\\n\\n{question}\",\n        inputs_format = AnswerQuestionInput,\n        outputs_format = AnswerQuestionOutput,\n        **kwargs\n    ):\n        super().__init__(\n            name=name, \n            description=description, \n            prompt=prompt, \n            inputs_format=inputs_format, \n            outputs_format=outputs_format, \n            **kwargs\n        )\n\n    def execute(self, llm, inputs, sys_msg = None, return_prompt = False, **kwargs) -&gt; AnswerQuestionOutput:\n        question = inputs.get(\"question\")\n        prompt = self.prompt.format(question=question)\n        response = llm.generate(\n            prompt=prompt, \n            system_message=sys_msg,\n            parser=self.outputs_format, \n            parse_mode=\"str\"\n        )\n\n        if return_prompt:\n            return response, prompt\n        return response \n\n    async def async_execute(self, llm, inputs, sys_msg = None, return_prompt = False, **kwargs) -&gt; AnswerQuestionOutput:\n        question = inputs.get(\"question\")\n        prompt = self.prompt.format(question=question)\n        response = await llm.async_generate(\n            prompt=prompt, \n            system_message=sys_msg,\n            parser=self.outputs_format, \n            parse_mode=\"str\"\n        )   \n        if return_prompt:\n            return response, prompt\n        return response \n\n# Configure LLM\nllm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=\"your-api-key\")\n\n# Create an agent\nagent = Agent(\n    name=\"AssistantAgent\",\n    description=\"Answers a factual question using the LLM\",\n    llm_config=llm_config,\n    system_prompt=\"You are a helpful assistant.\",\n    actions=[AnswerQuestionAction()]\n)\n</code></pre>"},{"location":"modules/agent/#executing-actions","title":"Executing Actions","text":"<p>You can directly call the <code>Agent</code> instance like a function. This will internally invoke the <code>execute()</code> method of the matching action using the specified <code>action_name</code> and <code>action_input_data</code>.</p> <pre><code># Execute an action with input data\nmessage = agent(\n    action_name=\"answer_question\",\n    action_input_data={\"question\": \"What is the capital of France?\"}\n)\n\n# Access the output\nresult = message.content.answer \n</code></pre>"},{"location":"modules/agent/#asynchronous-execution","title":"Asynchronous Execution","text":"<p>You can also call the <code>Agent</code> instance in an asynchronous context. If the action defines an <code>async_execute</code> method, it will be used automatically when you <code>await</code> the agent.</p> <pre><code># Execute an action asynchronously\nimport asyncio \n\nasync def main():\n    message = await agent(\n        action_name=\"answer_question\",\n        action_input_data={\"question\": \"What is the capital of France?\"}\n    )\n    return message.content.answer \n\nresult = asyncio.run(main())\nprint(result)\n</code></pre>"},{"location":"modules/agent/#memory-management","title":"Memory Management","text":"<p>The Agent maintains a short-term memory for tracking conversation context:</p> <pre><code># Access the agent's memory\nmessages = agent.short_term_memory.get(n=5)  # Get last 5 messages\n\n# Clear memory\nagent.clear_short_term_memory()\n</code></pre>"},{"location":"modules/agent/#agent-profile","title":"Agent Profile","text":"<p>You can get a human-readable description of an agent and its capabilities:</p> <pre><code># Get description of all actions\nprofile = agent.get_agent_profile()\nprint(profile)\n\n# Get description of specific actions\nprofile = agent.get_agent_profile(action_names=[\"answer_question\"])\nprint(profile)\n</code></pre>"},{"location":"modules/agent/#prompt-management","title":"Prompt Management","text":"<p>Access and modify the prompts used by an agent:</p> <pre><code># Get all prompts\nprompts = agent.get_prompts()\n# prompts is a dictionary with the structure:\n# {'answer_question': {'system_prompt': 'You are a helpful assistant.', 'prompt': 'Answer the following question as accurately as possible:\\n\\n{question}'}}\n\n# Set a specific prompt\nagent.set_prompt(\n    action_name=\"answer_question\",\n    prompt=\"Please provide a clear and concise answer to the following query:\\n\\n{question}\",\n    system_prompt=\"You are a helpful assistant.\" # optional, if not provided, the system prompt will remain unchanged \n)\n\n# Update all prompts\nprompts_dict = {\n    \"answer_question\": {\n        \"system_prompt\": \"You are an expert in providing concise, accurate information.\",\n        \"prompt\": \"Please answer this question with precision and clarity:\\n\\n{question}\"\n    }\n}\nagent.set_prompts(prompts_dict)\n</code></pre>"},{"location":"modules/agent/#saving-and-loading-agents","title":"Saving and Loading Agents","text":"<p>Agents can be persisted and reloaded:</p> <pre><code># Save agent\nagent.save_module(\"./agents/my_agent.json\")\n\n# Load agent (requires providing llm_config again)\nloaded_agent = Agent.from_file(\n    \"./agents/my_agent.json\", \n    llm_config=llm_config\n)\n</code></pre>"},{"location":"modules/agent/#context-extraction","title":"Context Extraction","text":"<p>The Agent includes a built-in context extraction mechanism that automatically derives appropriate inputs for actions from conversation history:</p> <pre><code># Context is automatically extracted when executing without explicit input data\nresponse = agent.execute(\n    action_name=\"action_name\",\n    msgs=conversation_history\n)\n\n# Get action inputs manually\naction = agent.get_action(\"action_name\")\ninputs = agent.get_action_inputs(action)\n</code></pre>"},{"location":"modules/benchmark/","title":"Benchmark","text":""},{"location":"modules/benchmark/#benchmark-overview","title":"Benchmark Overview","text":"<p>EvoAgentX provides a set of benchmarks to facilitate the evaluation of different agent-based systems. Below is a summary of the benchmarks currently included, along with basic dataset statistics: </p> Task Dataset Name # Train # Dev # Test QA NQ 79,168 8,757 3,610 Multi-Hop QA HotPotQA 90,447 7,405 / Math GSM8K 7,473 / 1,319 Math MATH 7,500 / 5,000 Code Generation HumanEval / / 164 Code Generation MBPP / / 427 Code Generation LiveCodeBench(v1~v5) / / 400~880 Code Execution LiveCodeBench / / 479 Test Output Prediction LiveCodeBench / / 442 <p>Our framework provides automatic dataset downloading capabilities, and all benchmarks have built-in evaluation methods. The framework is designed to allow users to easily load, use, and evaluate datasets for various tasks without manually handling data downloading and evaluation logic.  All datasets are automatically downloaded to the default path (~/.evoagentx/data/) when first used, or users can specify a custom path via parameter <code>path</code>. Each benchmark class implements a standardized interface, including methods for data loading, label retrieval, and prediction evaluation. </p> <p>Below, we introduce the preprocessing steps and evaluation metrics for each benchmark. </p> <ul> <li>Question Answering<ul> <li>NQ</li> <li>HotPotQA</li> </ul> </li> <li>Math<ul> <li>GSM8K</li> <li>MATH</li> </ul> </li> <li>Code Generation<ul> <li>HumanEval</li> <li>MBPP</li> <li>LiveCodeBench</li> </ul> </li> </ul>"},{"location":"modules/benchmark/#preprocessing-and-evaluation-metrics","title":"Preprocessing and Evaluation Metrics","text":""},{"location":"modules/benchmark/#question-answering","title":"Question Answering","text":"<p>For the QA datasets, we use Exact Match (EM), F1, and Accuracy (ACC) as evaluation metrics by default. EM requires the predicted answer to be exactly the same as the ground truth answer. ACC requires that the predicted answer contains the ground-truth answer, which is useful when the LLM is used to generate the answer. </p>"},{"location":"modules/benchmark/#nq","title":"NQ","text":"<p>Natural Questions (NQ) contains questions from the Google search engine and the answers, annotated by human annotators, are paragraphs or entities in the Wikipedia page of the top 5 search results. We use the dataset splits provided by the DPR repository, which contains 79,168 training, 8,757 development, and 3,610 test examples. </p> <p>You can load the dataset using the following code:  <pre><code>from evoagentx.benchmark import NQ\nnq_dataset = NQ() # optional: path=\"/path/to/save_data\"\ntest_data = nq_dataset.get_test_data()\n</code></pre> Each example in the dataset is in the following format:  <pre><code>{\n    \"id\": \"test-1\", \n    \"question\": \"the question\", \n    \"answers\": [\"possible answers\"]\n}\n</code></pre></p>"},{"location":"modules/benchmark/#hotpotqa","title":"HotPotQA","text":"<p>HotPotQA is a multi-hop QA dataset that requires multi-step reasoning to answer the question. We use the distractor setting of the dataset. Each example contains a question, an answer, some context that contians both supporting and distractor information, and supporting facts. We only include the training and development sets, as the test set is not publicly available. </p> <p>You can load the dataset using the following code:  <pre><code>from evoagentx.benchmark import HotPotQA\nhotpotqa_dataset = HotPotQA() # optional: path=\"/path/to/save_data\"\ntest_data = hotpotqa_dataset.get_test_data()\n</code></pre> Each example in the dataset is in the following format, where the second element (int) of a supporting_fact is the index of the sentence in the context that supports the answer.  <pre><code>{\n        \"_id\": \"the id of the example\", \n        \"question\": \"the question\", \n        \"answer\": \"the answer\", \n        \"context\": [[\"context_title\", [\"context_sentence\", \"another_sentence\"]]],\n        \"supporting_facts\": [[\"supporting_title\", 0]]\n    }\n</code></pre></p>"},{"location":"modules/benchmark/#math","title":"Math","text":"<p>For match datasets, we use the solve rate as the evaluation metric. The solve rate is the ratio of the number of examples that are solved correctly to the total number of examples.</p>"},{"location":"modules/benchmark/#gsm8k","title":"GSM8K","text":"<p>GSM8K consists of high quality grade school math problems created by human problem writers. These problems require multi-step mathematical reasoning to solve. We use the dataset splits provided by the original repository, which contains 7.5K training problems and 1K test problems. </p> <p>You can load the dataset using the following code:  <pre><code>from evoagentx.benchmark import GSM8K\ngsm8k_dataset = GSM8K() # optional: path=\"/path/to/save_data\"\ntest_data = gsm8k_dataset.get_test_data()\n</code></pre> Each example in the dataset is in the following format:  <pre><code>{\n    \"id\": \"test-1\", \n    \"question\": \"the question\", \n    \"answer\": \"the answer\"\n}\n</code></pre></p>"},{"location":"modules/benchmark/#math_1","title":"MATH","text":"<p>The Mathematics Aptitude Test of Heuristics (MATH) dataset consists of problems from mathematics competitions, including the AMC 10, AMC 12, AIME, etc. Each problem in MATH has a step-by-step solution. We use the dataset splits provided by the original repository, which contains 7.5K training problems and 5K test problems. </p> <p>You can load the dataset using the following code:  <pre><code>from evoagentx.benchmark import MATH\nmath_dataset = MATH() # optional: path=\"/path/to/save_data\"\ntest_data = math_dataset.get_test_data()\n</code></pre> Each example in the dataset is in the following format. For the <code>level</code> field, valid values are: \"Level 1\", \"Level 2\", \"Level 3\", \"Level 4\", \"Level 5\", and \"Level ?\". The <code>type</code> field can be one of: \"Geometry\", \"Algebra\", \"Intermediate Algebra\", \"Counting &amp; Probability\", \"Precalculus\", \"Number Theory\", or \"Prealgebra\".</p> <pre><code>{\n    \"id\": \"test-1\", \n    \"problem\": \"the problem\", \n    \"solution\": \"the solution\",\n    \"level\": \"Level 1\",\n    \"type\": \"Algebra\"\n}\n</code></pre>"},{"location":"modules/benchmark/#code-generation","title":"Code Generation","text":"<p>For the code generation benchmarks, we use pass@k as the evaluation metric, where k is the number of solutions for each problem. By default, k is set to 1. </p>"},{"location":"modules/benchmark/#humaneval","title":"HumanEval","text":"<p>HumanEval is a dataset of 164 coding problems from the HumanEval benchmark. Each problem contains a function signature, a canonical solution, and a set of unit tests. </p> <p>You can load the dataset using the following code:  <pre><code>from evoagentx.benchmark import HumanEval\nhumaneval_dataset = HumanEval() # optional: path=\"/path/to/save_data\"\ntest_data = humaneval_dataset.get_test_data()\n</code></pre> Each example in the dataset is in the following format:  <pre><code>{\n    \"task_id\": \"HumanEval/0\", \n    \"prompt\": \"the prompt of the problem\", \n    \"entry_point\": \"the name of the function to be tested\",\n    \"canonical_solution\": \"the canonical solution of the problem\", \n    \"test\": \"the unit tests of the problem\"\n}\n</code></pre></p>"},{"location":"modules/benchmark/#mbpp","title":"MBPP","text":"<p>Mostly Basic Python Problems (MBPP) consists of hundreds of entry-level Python programming problems. Each problem consists of a task description, code solution and 3 automated test cases. We use the sanitized subset of the MBPP dataset, which consists of 427 problems with data that are hand-verfied by the authors. To facilitate the evaluation, we convert the MBPP dataset into the HumanEval format. </p> <p>You can load the dataset using the following code:  <pre><code>from evoagentx.benchmark import MBPP\nmbpp_dataset = MBPP() # optional: path=\"/path/to/save_data\"\ntest_data = mbpp_dataset.get_test_data()\n</code></pre> Each example in the dataset is in the following format, where we keep the original MBPP <code>task_id</code>. <pre><code>{\n    \"task_id\": 2, \n    \"prompt\": \"the prompt of the problem\", \n    \"entry_point\": \"the name of the function to be tested\",\n    \"canonical_solution\": \"the canonical solution of the problem\", \n    \"test\": \"the unit tests of the problem\"\n}\n</code></pre> You can also access the original MBPP attributes such as \"code\", \"test_list\" in the example by using <code>example[\"code\"]</code>. </p>"},{"location":"modules/benchmark/#livecodebench","title":"LiveCodeBench","text":"<p>LiveCodeBench is a contamination-free evaluation benchmark of LLMs for code that continuously collects new problems over time. Particularly, LiveCodeBench also focuses on broader code-related capabilities, such as code execution, and test output prediction, beyond mere code generation. Currently, LiveCodeBench hosts over three hundred high-quality coding problems published between May 2023 and February 2024. </p> <p>You can load the dataset using the following code, where <code>scenario</code> can be one of [<code>code_generation</code>, <code>test_output_prediction</code>, <code>code_execution</code>] indicating different tasks. <code>version</code> denotes different versions of the code generation datasets, which is only available for <code>code_generation</code> scenario, and can be one of <code>[\"release_v1\", \"release_v2\", \"release_v3\", \"release_v4\", \"release_v5\", \"release_latest\"]</code>. Please refer to the LiveCodeBench repository for more details. </p> <pre><code>from evoagentx.benchmark import LiveCodeBench\nlivecodebench_dataset = LiveCodeBench(scenario=\"code_generation\", version=\"release_v1\") # optional: path=\"/path/to/save_data\"\ntest_data = livecodebench_dataset.get_test_data()\n</code></pre>"},{"location":"modules/customize_agent/","title":"CustomizeAgent","text":""},{"location":"modules/customize_agent/#introduction","title":"Introduction","text":"<p>The <code>CustomizeAgent</code> class provides a flexible framework for creating specialized LLM-powered agents. It enables the definition of agents with well-defined inputs, outputs, custom prompt templates, and configurable parsing strategies, making it suitable for rapid prototyping and deployment of domain-specific agents.</p>"},{"location":"modules/customize_agent/#key-features","title":"Key Features","text":"<ul> <li>No Custom Code Required: Create specialized agents through configuration rather than writing custom agent classes</li> <li>Flexible Input/Output Definitions: Define exactly what inputs your agent accepts and what outputs it produces</li> <li>Customizable Parsing Strategies: Multiple parsing modes to extract structured data from LLM responses</li> <li>Reusable Components: Save and load agent definitions for reuse across projects</li> </ul>"},{"location":"modules/customize_agent/#basic-usage","title":"Basic Usage","text":""},{"location":"modules/customize_agent/#simple-agent","title":"Simple Agent","text":"<p>The simplest way to create a <code>CustomizeAgent</code> is with just a name, description and prompt:</p> <p><pre><code>import os \nfrom dotenv import load_dotenv\nfrom evoagentx.models import OpenAILLMConfig\nfrom evoagentx.agents import CustomizeAgent\n\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\n# Configure LLM\nopenai_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=OPENAI_API_KEY)\n\n# Create a simple agent\nsimple_agent = CustomizeAgent(\n    name=\"SimpleAgent\",\n    description=\"A basic agent that responds to queries\",\n    prompt=\"Answer the following question: {question}\",\n    llm_config=openai_config,\n    inputs=[\n        {\"name\": \"question\", \"type\": \"string\", \"description\": \"The question to answer\"}\n    ]\n)\n\n# Execute the agent\nresponse = simple_agent(inputs={\"question\": \"What is a language model?\"})\nprint(response.content.content)  # Access the raw response content\n</code></pre> In this example, 1. We specify the input information (including its name, type, and description) in the <code>inputs</code> parameter since the prompt requires an input. 2. Moreover, when executing the agent with <code>simple_agent(...)</code>, you should provide all the inputs in the <code>inputs</code> parameter. </p> <p>The output after executing the agent is a <code>Message</code> object, which contains the raw LLM response in <code>message.content.content</code>. </p> <p>Note</p> <p>All the input names specified in the <code>CustomizeAgent(inputs=[...])</code> should appear in the <code>prompt</code>. Otherwise, an error will be raised.</p>"},{"location":"modules/customize_agent/#structured-outputs","title":"Structured Outputs","text":"<p>One of the most powerful features of <code>CustomizeAgent</code> is the ability to define structured outputs. This allows you to transform unstructured LLM responses into well-defined data structures that are easier to work with programmatically.</p>"},{"location":"modules/customize_agent/#basic-structured-output","title":"Basic Structured Output","text":"<p>Here's a simple example of defining structured outputs:</p> <pre><code>from evoagentx.core.module_utils import extract_code_blocks\n\n# Create a CodeWriter agent with structured output\ncode_writer = CustomizeAgent(\n    name=\"CodeWriter\",\n    description=\"Writes Python code based on requirements\",\n    prompt=\"Write Python code that implements the following requirement: {requirement}\",\n    llm_config=openai_config,\n    inputs=[\n        {\"name\": \"requirement\", \"type\": \"string\", \"description\": \"The coding requirement\"}\n    ],\n    outputs=[\n        {\"name\": \"code\", \"type\": \"string\", \"description\": \"The generated Python code\"}\n    ],\n    parse_mode=\"custom\",  # Use custom parsing function\n    parse_func=lambda content: {\"code\": extract_code_blocks(content)[0]}  # Extract first code block\n)\n\n# Execute the agent\nmessage = code_writer(\n    inputs={\"requirement\": \"Write a function that returns the sum of two numbers\"}\n)\nprint(message.content.code)  # Access the parsed code directly\n</code></pre> <p>In this example: 1. We define an output field named <code>code</code> in the <code>outputs</code> parameter. 2. We set <code>parse_mode=\"custom\"</code> to use a custom parsing function. 3. The <code>parse_func</code> extracts the first code block from the LLM response. 4. We can directly access the parsed code with <code>message.content.code</code>.</p> <p>You can also access the raw LLM response by <code>message.content.content</code>. </p> <p>Note</p> <ol> <li> <p>If the <code>outputs</code> parameter is set in <code>CustomizeAgent</code>, the agent will try to parse the LLM response based on the output field names. If you don't want to parse the LLM response, you should not set the <code>outputs</code> parameter. The raw LLM response can be accessed by <code>message.content.content</code>. </p> </li> <li> <p>CustomizeAgent supports different parsing modes, such as `['str', 'json', 'xml', 'title', 'custom']. Please refer to the Parsing Modes section for more details. </p> </li> </ol>"},{"location":"modules/customize_agent/#multiple-structured-outputs","title":"Multiple Structured Outputs","text":"<p>You can define multiple output fields to create more complex structured data:</p> <pre><code># Agent that generates both code and explanation\nanalyzer = CustomizeAgent(\n    name=\"CodeAnalyzer\",\n    description=\"Generates and explains Python code\",\n    prompt=\"\"\"\n    Write Python code for: {requirement}\n\n    Provide your response in the following format:\n\n    ## code\n    [Your code implementation here]\n\n    ## explanation\n    [A brief explanation of how the code works]\n\n    ## complexity\n    [Time and space complexity analysis]\n    \"\"\",\n    llm_config=openai_config,\n    inputs=[\n        {\"name\": \"requirement\", \"type\": \"string\", \"description\": \"The coding requirement\"}\n    ],\n    outputs=[\n        {\"name\": \"code\", \"type\": \"string\", \"description\": \"The generated Python code\"},\n        {\"name\": \"explanation\", \"type\": \"string\", \"description\": \"Explanation of the code\"},\n        {\"name\": \"complexity\", \"type\": \"string\", \"description\": \"Complexity analysis\"}\n    ],\n    parse_mode=\"title\"  # Use default title parsing mode\n)\n\n# Execute the agent\nresult = analyzer(inputs={\"requirement\": \"Write a binary search algorithm\"})\n\n# Access each structured output separately\nprint(\"CODE:\")\nprint(result.content.code)\nprint(\"\\nEXPLANATION:\")\nprint(result.content.explanation)\nprint(\"\\nCOMPLEXITY:\")\nprint(result.content.complexity)\n</code></pre>"},{"location":"modules/customize_agent/#parsing-modes","title":"Parsing Modes","text":"<p>CustomizeAgent supports different ways to parse the LLM output:</p>"},{"location":"modules/customize_agent/#1-string-mode-parse_modestr","title":"1. String Mode (<code>parse_mode=\"str\"</code>)","text":"<p>Uses the raw LLM output as the value for each output field. Useful for simple agents where structured parsing isn't needed.</p> <pre><code>agent = CustomizeAgent(\n    name=\"SimpleAgent\",\n    description=\"Returns raw output\",\n    prompt=\"Generate a greeting for {name}\",\n    inputs=[{\"name\": \"name\", \"type\": \"string\", \"description\": \"The name to greet\"}],\n    outputs=[{\"name\": \"greeting\", \"type\": \"string\", \"description\": \"The generated greeting\"}],\n    parse_mode=\"str\",\n    # other parameters...\n)\n</code></pre> <p>After executing the agent, you can access the raw LLM response by <code>message.content.content</code> or <code>message.content.greeting</code>.  </p>"},{"location":"modules/customize_agent/#2-title-mode-parse_modetitle-default","title":"2. Title Mode (<code>parse_mode=\"title\"</code>, default)","text":"<p>Extracts content between titles matching output field names. This is the default parsing mode.</p> <pre><code>agent = CustomizeAgent(\n    name=\"ReportGenerator\",\n    description=\"Generates a structured report\",\n    prompt=\"Create a report about {topic}\",\n    outputs=[\n        {\"name\": \"summary\", \"type\": \"string\", \"description\": \"Brief summary\"},\n        {\"name\": \"analysis\", \"type\": \"string\", \"description\": \"Detailed analysis\"}\n    ],\n    # Default title pattern is \"## {title}\"\n    title_format=\"### {title}\",  # Optional: customize title format\n    # other parameters...\n)\n</code></pre> <p>With this configuration, the LLM should be instructed to format its response like:</p> <pre><code>### summary\nBrief summary of the topic here.\n\n### analysis\nDetailed analysis of the topic here.\n</code></pre> <p>Note</p> <p>The section titles output by the LLM should be exactly the same as the output field names. Otherwise, the parsing will fail. For instance, in above example, if the LLM outputs <code>### Analysis</code>, which is different from the output field name <code>analysis</code>, the parsing will fail. </p>"},{"location":"modules/customize_agent/#3-json-mode-parse_modejson","title":"3. JSON Mode (<code>parse_mode=\"json\"</code>)","text":"<p>Parse the JSON string output by the LLM. The keys of the JSON string should be exactly the same as the output field names. </p> <p><pre><code>agent = CustomizeAgent(\n    name=\"DataExtractor\",\n    description=\"Extracts structured data\",\n    prompt=\"Extract key information from this text: {text}\",\n    inputs=[\n        {\"name\": \"text\", \"type\": \"string\", \"description\": \"The text to extract information from\"}\n    ],\n    outputs=[\n        {\"name\": \"people\", \"type\": \"string\", \"description\": \"Names of people mentioned\"},\n        {\"name\": \"places\", \"type\": \"string\", \"description\": \"Locations mentioned\"},\n        {\"name\": \"dates\", \"type\": \"string\", \"description\": \"Dates mentioned\"}\n    ],\n    parse_mode=\"json\",\n    # other parameters...\n)\n</code></pre> When using this mode, the LLM should output a valid JSON string with keys matching the output field names. For instance, you should instruct the LLM to output:</p> <p><pre><code>{\n    \"people\": \"extracted people\",\n    \"places\": \"extracted places\",\n    \"dates\": \"extracted dates\"\n}\n</code></pre> If there are multiple JSON string in the LLM response, only the first one will be used. </p>"},{"location":"modules/customize_agent/#4-xml-mode-parse_modexml","title":"4. XML Mode (<code>parse_mode=\"xml\"</code>)","text":"<p>Parse the XML string output by the LLM. The keys of the XML string should be exactly the same as the output field names.  </p> <pre><code>agent = CustomizeAgent(\n    name=\"DataExtractor\",\n    description=\"Extracts structured data\",\n    prompt=\"Extract key information from this text: {text}\",\n    inputs=[\n        {\"name\": \"text\", \"type\": \"string\", \"description\": \"The text to extract information from\"}\n    ],\n    outputs=[\n        {\"name\": \"people\", \"type\": \"string\", \"description\": \"Names of people mentioned\"},\n    ],\n    parse_mode=\"xml\",\n    # other parameters...\n)\n</code></pre> <p>When using this mode, the LLM should generte texts containing xml tags with keys matching the output field names. For instance, you should instruct the LLM to output:</p> <pre><code>The people mentioned in the text are: &lt;people&gt;John Doe and Jane Smith&lt;/people&gt;.\n</code></pre> <p>If the LLM output contains multiple xml tags with the same name, only the first one will be used. </p>"},{"location":"modules/customize_agent/#5-custom-parsing-parse_modecustom","title":"5. Custom Parsing (<code>parse_mode=\"custom\"</code>)","text":"<p>For maximum flexibility, you can define a custom parsing function:</p> <pre><code>from evoagentx.core.registry import register_parse_function\n\n@register_parse_function  # Register the function for serialization\ndef extract_python_code(content: str) -&gt; dict:\n    \"\"\"Extract Python code from LLM response\"\"\"\n    code_blocks = extract_code_blocks(content)\n    return {\"code\": code_blocks[0] if code_blocks else \"\"}\n\nagent = CustomizeAgent(\n    name=\"CodeExplainer\",\n    description=\"Generates and explains code\",\n    prompt=\"Write a Python function that {requirement}\",\n    inputs=[\n        {\"name\": \"requirement\", \"type\": \"string\", \"description\": \"The requirement to generate code for\"}\n    ],\n    outputs=[\n        {\"name\": \"code\", \"type\": \"string\", \"description\": \"The generated code\"},\n    ],\n    parse_mode=\"custom\",\n    parse_func=extract_python_code,\n    # other parameters...\n)\n</code></pre> <p>Note</p> <ol> <li> <p>The parsing function should have an input parameter <code>content</code> that takes the raw LLM response as input, and return a dictionary with keys matching the output field names. </p> </li> <li> <p>It is recommended to use the <code>@register_parse_function</code> decorator to register the parsing function for serialization, so that you can save the agent and load it later. </p> </li> </ol>"},{"location":"modules/customize_agent/#saving-and-loading-agents","title":"Saving and Loading Agents","text":"<p>You can save agent definitions to reuse them later:</p> <pre><code># Save agent configuration. By default, the `llm_config` will not be saved. \ncode_writer.save_module(\"./agents/code_writer.json\")\n\n# Load agent from file (requires providing llm_config again)\nloaded_agent = CustomizeAgent.from_file(\n    \"./agents/code_writer.json\", \n    llm_config=openai_config\n)\n</code></pre>"},{"location":"modules/customize_agent/#advanced-example-multi-step-code-generator","title":"Advanced Example: Multi-Step Code Generator","text":"<p>Here's a more advanced example that demonstrates creating a specialized code generation agent with multiple structured outputs:</p> <pre><code>from pydantic import Field\nfrom evoagentx.actions import ActionOutput\nfrom evoagentx.core.registry import register_parse_function\n\nclass CodeGeneratorOutput(ActionOutput):\n    code: str = Field(description=\"The generated Python code\")\n    documentation: str = Field(description=\"Documentation for the code\")\n    tests: str = Field(description=\"Unit tests for the code\")\n\n@register_parse_function\ndef parse_code_documentation_tests(content: str) -&gt; dict:\n    \"\"\"Parse LLM output into code, documentation, and tests sections\"\"\"\n    sections = content.split(\"## \")\n    result = {\"code\": \"\", \"documentation\": \"\", \"tests\": \"\"}\n\n    for section in sections:\n        if not section.strip():\n            continue\n\n        lines = section.strip().split(\"\\n\")\n        section_name = lines[0].lower()\n        section_content = \"\\n\".join(lines[1:]).strip()\n\n        if \"code\" in section_name:\n            # Extract code from code blocks\n            code_blocks = extract_code_blocks(section_content)\n            result[\"code\"] = code_blocks[0] if code_blocks else section_content\n        elif \"documentation\" in section_name:\n            result[\"documentation\"] = section_content\n        elif \"test\" in section_name:\n            # Extract code from code blocks if present\n            code_blocks = extract_code_blocks(section_content)\n            result[\"tests\"] = code_blocks[0] if code_blocks else section_content\n\n    return result\n\n# Create the advanced code generator agent\nadvanced_generator = CustomizeAgent(\n    name=\"AdvancedCodeGenerator\",\n    description=\"Generates complete code packages with documentation and tests\",\n    prompt=\"\"\"\n    Create a complete implementation based on this requirement:\n    {requirement}\n\n    Provide your response in the following format:\n\n    ## Code\n    [Include the Python code implementation here]\n\n    ## Documentation\n    [Include clear documentation explaining the code]\n\n    ## Tests\n    [Include unit tests that verify the code works correctly]\n    \"\"\",\n    llm_config=openai_config,\n    inputs=[\n        {\"name\": \"requirement\", \"type\": \"string\", \"description\": \"The coding requirement\"}\n    ],\n    outputs=[\n        {\"name\": \"code\", \"type\": \"string\", \"description\": \"The generated Python code\"},\n        {\"name\": \"documentation\", \"type\": \"string\", \"description\": \"Documentation for the code\"},\n        {\"name\": \"tests\", \"type\": \"string\", \"description\": \"Unit tests for the code\"}\n    ],\n    output_parser=CodeGeneratorOutput,\n    parse_mode=\"custom\",\n    parse_func=parse_code_documentation_tests,\n    system_prompt=\"You are an expert Python developer specialized in writing clean, efficient code with comprehensive documentation and tests.\"\n)\n\n# Execute the agent\nresult = advanced_generator(\n    inputs={\n        \"requirement\": \"Create a function to validate if a string is a valid email address\"\n    }\n)\n\n# Access the structured outputs\nprint(\"CODE:\")\nprint(result.content.code)\nprint(\"\\nDOCUMENTATION:\")\nprint(result.content.documentation)\nprint(\"\\nTESTS:\")\nprint(result.content.tests)\n</code></pre> <p>This advanced example demonstrates how to create a specialized agent that produces multiple structured outputs from a single LLM call, providing a complete code package with implementation, documentation, and tests.</p>"},{"location":"modules/evaluator/","title":"Evaluator","text":""},{"location":"modules/evaluator/#introduction","title":"Introduction","text":"<p>The <code>Evaluator</code> class is a fundamental component in the EvoAgentX framework for evaluating the performance of workflows and action graphs against benchmarks. It provides a structured way to measure how well AI agents perform on specific tasks by running evaluations on test data and computing metrics.</p>"},{"location":"modules/evaluator/#architecture","title":"Architecture","text":""},{"location":"modules/evaluator/#evaluator-architecture","title":"Evaluator Architecture","text":"<p>An <code>Evaluator</code> consists of several key components:</p> <ol> <li> <p>LLM Instance: </p> <p>The language model used for executing workflows during evaluation:</p> <ul> <li>Provides the reasoning and generation capabilities needed for workflow execution</li> <li>Can be any implementation that follows the <code>BaseLLM</code> interface</li> </ul> </li> <li> <p>Agent Manager: </p> <p>Manages the agents used by workflow graphs during evaluation:</p> <ul> <li>Provides access to agents needed for workflow execution</li> <li>Only required when evaluating <code>WorkFlowGraph</code> instances, and can be ignored when evaluating <code>ActionGraph</code> instances </li> </ul> </li> <li> <p>Data Processing Functions:</p> <p>Functions that prepare and process data during evaluation:</p> <ul> <li><code>collate_func</code>: Prepares benchmark examples for workflow execution</li> <li><code>output_postprocess_func</code>: Processes workflow outputs before evaluation</li> </ul> </li> </ol>"},{"location":"modules/evaluator/#evaluation-process","title":"Evaluation Process","text":"<p>The evaluation process follows these steps:</p> <ol> <li>Data Processing: Obtain examples from the benchmark dataset and process them into the format expected by the workflow graph or action graph</li> <li>Workflow Execution: Run each example through the workflow graph or action graph</li> <li>Output Processing: Process the outputs into the format expected by the benchmark</li> <li>Metric Calculation: Compute performance metrics by comparing outputs to ground truth</li> <li>Result Aggregation: Aggregate individual metrics into overall performance scores</li> </ol>"},{"location":"modules/evaluator/#usage","title":"Usage","text":""},{"location":"modules/evaluator/#basic-evaluator-creation--execution","title":"Basic Evaluator Creation &amp; Execution","text":"<pre><code>from evoagentx.evaluators import Evaluator\nfrom evoagentx.models import OpenAILLMConfig, OpenAILLM\nfrom evoagentx.agents import AgentManager\nfrom evoagentx.workflow.workflow_graph import WorkFlowGraph\nfrom evoagentx.benchmark import SomeBenchmark\nfrom evoagentx.core.callbacks import suppress_logger_info\n\n# Initialize LLM\nllm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=\"xxx\")\nllm = OpenAILLM(llm_config)\n\n# Initialize agent manager\nagent_manager = AgentManager()\n\n# Load your workflow graph\nworkflow_graph = WorkFlowGraph.from_file(\"path/to/workflow.json\")\n\n# Add agents to the agent manager\nagent_manager.add_agents_from_workflow(workflow_graph, llm_config=llm_config)\n\n# Create benchmark\nbenchmark = SomeBenchmark()\n\n# Create evaluator\nevaluator = Evaluator(\n    llm=llm,\n    agent_manager=agent_manager,\n    num_workers=4,  # Use 4 parallel workers\n    verbose=True    # Show progress bars\n)\n\n# Run evaluation with suppressed logging\nwith suppress_logger_info():\n    results = evaluator.evaluate(\n        graph=workflow_graph,\n        benchmark=benchmark,\n        eval_mode=\"test\",    # Evaluate on test split (default)\n        sample_k=100         # Use 100 random examples\n    )\n\nprint(f\"Evaluation results: {results}\")\n</code></pre>"},{"location":"modules/evaluator/#customizing-data-processing","title":"Customizing Data Processing","text":"<pre><code>from evoagentx.evaluators import Evaluator\nfrom evoagentx.models import OpenAILLMConfig, OpenAILLM\nfrom evoagentx.agents import AgentManager\nfrom evoagentx.core.callbacks import suppress_logger_info\n\n# Custom collate function to prepare inputs. The keys should match the input parameters of the workflow graph or action graph. The return value will be directly passed to the `execute` method of the workflow graph or action graph. \ndef custom_collate(example):\n    return {\n        \"input_text\": example[\"question\"],\n        \"context\": example.get(\"context\", \"\")\n    }\n\n# Custom output processing, `output` is the output of the workflow and the return value will be passed to the `evaluate` method of the benchmark.  \ndef custom_postprocess(output):\n    if isinstance(output, dict):\n        return output.get(\"answer\", \"\")\n    return output\n\n# Create evaluator with custom functions\nevaluator = Evaluator(\n    llm=llm,\n    agent_manager=agent_manager,\n    collate_func=custom_collate,\n    output_postprocess_func=custom_postprocess,\n    num_workers=4,  # Use 4 parallel workers\n    verbose=True    # Show progress bars\n)\n</code></pre>"},{"location":"modules/evaluator/#evaluating-an-action-graph","title":"Evaluating an Action Graph","text":"<pre><code>from evoagentx.workflow.action_graph import ActionGraph\nfrom evoagentx.evaluators import Evaluator\nfrom evoagentx.models import OpenAILLMConfig, OpenAILLM\nfrom evoagentx.core.callbacks import suppress_logger_info\n\n# Initialize LLM\nllm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=\"xxx\")\nllm = OpenAILLM(llm_config)\n\n# Load your action graph\naction_graph = ActionGraph.from_file(\"path/to/action_graph.json\", llm_config=llm_config)\n\n# Create evaluator (no agent_manager needed for action graphs)\nevaluator = Evaluator(llm=llm, num_workers=4, verbose=True)\n\n# Run evaluation with suppressed logging\nwith suppress_logger_info():\n    results = evaluator.evaluate(\n        graph=action_graph,\n        benchmark=benchmark\n    )\n</code></pre>"},{"location":"modules/evaluator/#asynchronous-evaluation","title":"Asynchronous Evaluation","text":"<pre><code>import asyncio\nfrom evoagentx.evaluators import Evaluator\nfrom evoagentx.models import OpenAILLMConfig, OpenAILLM\nfrom evoagentx.agents import AgentManager\nfrom evoagentx.workflow.workflow_graph import WorkFlowGraph\nfrom evoagentx.benchmark import SomeBenchmark\nfrom evoagentx.core.callbacks import suppress_logger_info\n\n# Initialize LLM and components\nllm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=\"xxx\")\nllm = OpenAILLM(llm_config)\nagent_manager = AgentManager()\nworkflow_graph = WorkFlowGraph.from_file(\"path/to/workflow.json\")\nbenchmark = SomeBenchmark()\n\n# Create evaluator\nevaluator = Evaluator(\n    llm=llm,\n    agent_manager=agent_manager,\n    num_workers=4\n)\n\n# Run async evaluation\nasync def run_async_eval():\n    with suppress_logger_info():\n        results = await evaluator.async_evaluate(\n            graph=workflow_graph,\n            benchmark=benchmark\n        )\n    return results\n\n# Execute async evaluation\nresults = asyncio.run(run_async_eval())\n</code></pre>"},{"location":"modules/evaluator/#accessing-evaluation-records","title":"Accessing Evaluation Records","text":"<pre><code>from evoagentx.evaluators import Evaluator\nfrom evoagentx.models import OpenAILLMConfig, OpenAILLM\nfrom evoagentx.benchmark import SomeBenchmark\nfrom evoagentx.core.callbacks import suppress_logger_info\n\n# Initialize components\nllm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=\"xxx\")\nllm = OpenAILLM(llm_config)\nbenchmark = SomeBenchmark()\nevaluator = Evaluator(llm=llm)\n\n# Run evaluation with suppressed logging\nwith suppress_logger_info():\n    evaluator.evaluate(graph=graph, benchmark=benchmark)\n\n# Get all evaluation records\nall_records = evaluator.get_all_evaluation_records()\n\n# Get record for a specific example\nexample = benchmark.get_test_data()[0]\nrecord = evaluator.get_example_evaluation_record(benchmark, example)\n\n# Get record by example ID\nrecord_by_id = evaluator.get_evaluation_record_by_id(\n    benchmark=benchmark,\n    example_id=\"example-123\",\n    eval_mode=\"test\"\n)\n\n# Access trajectory for workflow graph evaluations\nif \"trajectory\" in record:\n    for message in record[\"trajectory\"]:\n        print(f\"{message.role}: {message.content}\")\n</code></pre> <p>The <code>Evaluator</code> class provides a powerful way to assess the performance of workflows and action graphs, enabling quantitative comparison and improvement tracking in the EvoAgentX framework.</p>"},{"location":"modules/llm/","title":"LLM","text":""},{"location":"modules/llm/#introduction","title":"Introduction","text":"<p>The LLM (Large Language Model) module provides a unified interface for interacting with various language model providers in the EvoAgentX framework. It abstracts away provider-specific implementation details, offering a consistent API for generating text, managing costs, and handling responses.</p>"},{"location":"modules/llm/#supported-llm-providers","title":"Supported LLM Providers","text":"<p>EvoAgentX currently supports the following LLM providers:</p>"},{"location":"modules/llm/#openaillm","title":"OpenAILLM","text":"<p>The primary implementation for accessing OpenAI's language models. It handles authentication, request formatting, and response parsing for models like GPT-4, GPT-3.5-Turbo, and other OpenAI models.</p> <p>Basic Usage:</p> <pre><code>from evoagentx.models import OpenAILLMConfig, OpenAILLM\n\n# Configure the model\nconfig = OpenAILLMConfig(\n    model=\"gpt-4o-mini\",  \n    openai_key=\"your-api-key\",\n    temperature=0.7,\n    max_tokens=1000\n)\n\n# Initialize the model\nllm = OpenAILLM(config=config)\n\n# Generate text\nresponse = llm.generate(\n    prompt=\"Explain quantum computing in simple terms.\",\n    system_message=\"You are a helpful assistant that explains complex topics simply.\"\n)\n</code></pre>"},{"location":"modules/llm/#litellm","title":"LiteLLM","text":"<p>LiteLLM is an adapter for the LiteLLM project, which provides a unified Python SDK and proxy server for calling over 100 LLM APIs using the OpenAI API format. It supports providers such as Bedrock, Azure, OpenAI, VertexAI, Cohere, Anthropic, Sagemaker, HuggingFace, Replicate, and Groq. Thanks to this project, the <code>LiteLLM</code> model class in EvoAgentX can be used to seamlessly access a wide range of LLM providers through a single interface. </p> <p>Basic Usage:</p> <p>To faciliate seamless integration with LiteLLM, you should specify the model name using the naming convention defied in the LiteLLM platform. For example, you need to specify <code>anthropic/claude-3-opus-20240229</code> for Claude 3.0 Opus. You can find a full list of supported providers and model names in their official documentation: https://docs.litellm.ai/docs/providers.</p> <pre><code>from evoagentx.models import LiteLLMConfig, LiteLLM\n\n# Configure the model\nconfig = LiteLLMConfig(\n    model=\"anthropic/claude-3-opus-20240229\", \n    anthropic_key=\"your-anthropic-api-key\",\n    temperature=0.7,\n    max_tokens=1000\n)\n\n# Initialize the model\nllm = LiteLLM(config=config)\n\n# Generate text\nresponse = llm.generate(\n    prompt=\"Design a system for autonomous vehicles.\",\n    system_message=\"You are an expert in autonomous systems design.\"\n)\n</code></pre>"},{"location":"modules/llm/#siliconflowllm","title":"SiliconFlowLLM","text":"<p>SiliconFlowLLM is an adapter for models hosted on the SiliconFlow platform, which offers access to both open-source and proprietary models via an OpenAI-compatible API. It enables you to integrate models like Qwen, DeepSeek, or Mixtral by specifying their names using the SiliconFlow platform's naming conventions.</p> <p>Thanks to SiliconFlow's unified interface, the <code>SiliconFlowLLM</code> model class in EvoAgentX allows seamless switching between a variety of powerful LLMs hosted on SiliconFlow using the same API format.</p> <p>Basic Usage:</p> <pre><code>from evoagentx.models import SiliconFlowConfig, SiliconFlowLLM\n\n# Configure the model\nconfig = SiliconFlowConfig(\n    model=\"deepseek-ai/DeepSeek-V3\",\n    siliconflow_key=\"your-siliconflow-api-key\",\n    temperature=0.7,\n    max_tokens=1000\n)\n\n# Initialize the model\nllm = SiliconFlowLLM(config=config)\n\n# Generate text\nresponse = llm.generate(\n    prompt=\"Write a poem about artificial intelligence.\",\n    system_message=\"You are a creative poet.\"\n)\n</code></pre>"},{"location":"modules/llm/#core-functions","title":"Core Functions","text":"<p>All LLM implementations in EvoAgentX provide a consistent set of core functions for generating text and managing the generation process.</p>"},{"location":"modules/llm/#generate-function","title":"Generate Function","text":"<p>The <code>generate</code> function is the primary method for producing text with language models:</p> <pre><code>def generate(\n    self,\n    prompt: Optional[Union[str, List[str]]] = None,\n    system_message: Optional[Union[str, List[str]]] = None,\n    messages: Optional[Union[List[dict],List[List[dict]]]] = None,\n    parser: Optional[Type[LLMOutputParser]] = None,\n    parse_mode: Optional[str] = \"json\", \n    parse_func: Optional[Callable] = None,\n    **kwargs\n) -&gt; Union[LLMOutputParser, List[LLMOutputParser]]:\n    \"\"\"\n    Generate text based on the prompt and optional system message.\n\n    Args:\n        prompt: Input prompt(s) to the LLM.\n        system_message: System message(s) for the LLM.\n        messages: Chat message(s) for the LLM, already in the required format (either `prompt` or `messages` must be provided).\n        parser: Parser class to use for processing the output into a structured format.\n        parse_mode: The mode to use for parsing, must be the `parse_mode` supported by the `parser`. \n        parse_func: A function to apply to the parsed output.\n        **kwargs: Additional generation configuration parameters.\n\n    Returns:\n        For single generation: An LLMOutputParser instance.\n        For batch generation: A list of LLMOutputParser instances.\n    \"\"\"\n</code></pre>"},{"location":"modules/llm/#inputs","title":"Inputs","text":"<p>In EvoAgentX, there are several ways to provide inputs to LLMs using the <code>generate</code> function:</p> <p>Method 1: Prompt and System Message</p> <ol> <li> <p>Prompt: The specific query or instruction for which you want a response. </p> </li> <li> <p>System Message (optional): Instructions that guide the model's overall behavior and role. This sets the context for how the model should respond.</p> </li> </ol> <p>Together, these components are converted into a standardized message format that the language model can understand:</p> <pre><code># Simple example with prompt and system message\nresponse = llm.generate(\n    prompt=\"What are three ways to improve productivity?\",\n    system_message=\"You are a productivity expert providing concise, actionable advice.\"\n)\n</code></pre> <p>Behind the scenes, this gets converted into messages with appropriate roles:</p> <pre><code>messages = [\n    {\"role\": \"system\", \"content\": \"You are a productivity expert providing concise, actionable advice.\"},\n    {\"role\": \"user\", \"content\": \"What are three ways to improve productivity?\"}\n]\n</code></pre> <p>Method 2: Using Messages Directly</p> <p>For more complex conversations or when you need precise control over the message format, you can use the <code>messages</code> parameter directly:</p> <pre><code># Using messages directly for a multi-turn conversation\nresponse = llm.generate(\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello, who are you?\"},\n        {\"role\": \"assistant\", \"content\": \"I'm an AI assistant designed to help with various tasks.\"},\n        {\"role\": \"user\", \"content\": \"Can you help me with programming?\"}\n    ]\n)\n</code></pre>"},{"location":"modules/llm/#batch-generation","title":"Batch Generation","text":"<p>For batch processing, you can provide lists of prompts/system messages or list of messages. For example: </p> <pre><code># Batch processing example\nresponses = llm.generate(\n    prompt=[\"What is machine learning?\", \"Explain neural networks.\"],\n    system_message=[\"You are a data scientist.\", \"You are an AI researcher.\"]\n)\n</code></pre>"},{"location":"modules/llm/#output-parsing","title":"Output Parsing","text":"<p>The <code>generate</code> function provides flexible options for parsing and structuring the raw text output from language models:</p> <ul> <li>parser: Accepts a class (typically inheriting from <code>LLMOutputParser/ActionOutput</code>) that defines the structure for the parsed output. If not provided, the LLM output will not be parsed. In both cases, the raw LLM output can be accessed through the <code>.content</code> attribute of the returned object.   </li> <li>parse_mode: Determines how the raw LLM output is parsed into the structure defined by the parser, valid options are: <code>'str'</code>, <code>'json'</code> (default), <code>'xml'</code>, <code>'title'</code>, <code>'custom'</code>.</li> <li>parse_func: A custom function to handle parsing in more complex scenarios, only used when <code>parse_mode</code> is <code>'custom'</code>. </li> </ul> <p>Example with structured output:  <pre><code>from evoagentx.models import LLMOutputParser \nfrom pydantic import Field\n\nclass CodeWriterOutput(LLMOutputParser):\n    thought: str = Field(description=\"Thought process for writing the code\") \n    code: str = Field(description=\"The generated code\")\n\nprompt = \"\"\"\nWrite a Python function to calculate Fibonacci numbers. \n\nYour output should always be in the following format:\n\n## thought \n[Your thought process for writing the code]\n\n## code\n[The generated code]\n\"\"\"\nresponse = llm.generate(\n    prompt=prompt,\n    parser=CodeWriterOutput,\n    parse_mode=\"title\"\n)\n\nprint(\"Thought:\\n\", response.thought)\nprint(\"Code:\\n\", response.code)\n</code></pre></p>"},{"location":"modules/llm/#parse-modes","title":"Parse Modes","text":"<p>EvoAgentX supports several parsing strategies:</p> <ol> <li>\"str\": Uses the raw output as-is for each field defined in the parser.</li> <li>\"json\" (default): Extracts fields from a JSON string in the output.</li> <li>\"xml\": Extracts content from XML tags matching field names.</li> <li>\"title\": Extracts content from markdown sections (default format: \"## {title}\").</li> <li>\"custom\": Uses a custom parsing function specified by <code>parse_func</code>.</li> </ol> <p>Note</p> <p>For <code>'json'</code>, <code>'xml'</code> and <code>'title'</code>, you should instruct the LLM (through the <code>prompt</code>) to output the content in the specified format that can be parsed by the parser. Otherwise, the parsing will fail. </p> <ol> <li> <p>For <code>'json'</code>, you should instruct the LLM to output a valid JSON string containing keys that match the field names in the parser class. If there are multiple JSON string in the raw LLM output, only the first one will be parsed.  </p> </li> <li> <p>For <code>xml</code>, you should instruct the LLM to output content that contains XML tags matching the field names in the parser class, e.g., <code>&lt;{field_name}&gt;...&lt;/{field_name}&gt;</code>. If there are multiple XML tags with the same field name, only the first one will be used. </p> </li> <li> <p>For <code>title</code>, you should instruct the LLM to output content that contains markdown sections with the title exactly matching the field names in the parser class. The default title format is \"## {title}\". You can change it by setting the <code>title_format</code> parameter in the <code>generate</code> function, e.g., <code>generate(..., title_format=\"### {title}\")</code>. The <code>title_format</code> must contain <code>{title}</code> as a placeholder for the field name.  </p> </li> </ol>"},{"location":"modules/llm/#custom-parsing-function","title":"Custom Parsing Function","text":"<p>For maximum flexibility, you can define a custom parsing function with <code>parse_func</code>:</p> <pre><code>from evoagentx.models import LLMOutputParser\nfrom evoagentx.core.module_utils import extract_code_block\n\nclass CodeOutput(LLMOutputParser):\n    code: str = Field(description=\"The generated code\")\n\n# Use custom parsing\nresponse = llm.generate(\n    prompt=\"Write a Python function to calculate Fibonacci numbers.\",\n    parser=CodeOutput,\n    parse_mode=\"custom\",\n    parse_func=lambda content: {\"code\": extract_code_block(content)[0]}\n)\n</code></pre> <p>Note</p> <p>The <code>parse_func</code> should have an input parameter <code>content</code> that receives the raw LLM output, and return a dictionary with keys matching the field names in the parser class.  </p>"},{"location":"modules/llm/#async-generate-function","title":"Async Generate Function","text":"<p>For applications requiring asynchronous operation, the <code>async_generate</code> function provides the same functionality as the <code>generate</code> function, but in a non-blocking manner:</p> <pre><code>async def async_generate(\n        self,\n        prompt: Optional[Union[str, List[str]]] = None,\n        system_message: Optional[Union[str, List[str]]] = None,\n        messages: Optional[Union[List[dict],List[List[dict]]]] = None,\n        parser: Optional[Type[LLMOutputParser]] = None,\n        parse_mode: Optional[str] = \"json\", \n        parse_func: Optional[Callable] = None,\n        **kwargs\n    ) -&gt; Union[LLMOutputParser, List[LLMOutputParser]]:\n    \"\"\"\n    Asynchronously generate text based on the prompt and optional system message.\n\n    Args:\n        prompt: Input prompt(s) to the LLM.\n        system_message: System message(s) for the LLM.\n        messages: Chat message(s) for the LLM, already in the required format (either `prompt` or `messages` must be provided).\n        parser: Parser class to use for processing the output into a structured format.\n        parse_mode: The mode to use for parsing, must be the `parse_mode` supported by the `parser`. \n        parse_func: A function to apply to the parsed output.\n        **kwargs: Additional generation configuration parameters.\n\n    Returns:\n        For single generation: An LLMOutputParser instance.\n        For batch generation: A list of LLMOutputParser instances.\n    \"\"\"\n</code></pre>"},{"location":"modules/llm/#streaming-responses","title":"Streaming Responses","text":"<p>EvoAgentX supports streaming responses from LLMs, which allows you to see the model's output as it's being generated token by token, rather than waiting for the complete response. This is especially useful for long-form content generation or providing a more interactive experience.</p> <p>There are two ways to enable streaming:</p>"},{"location":"modules/llm/#configure-streaming-in-the-llm-config","title":"Configure Streaming in the LLM Config","text":"<p>You can enable streaming when initializing the LLM by setting appropriate parameters in the config:</p> <pre><code># Enable streaming at initialization time\nconfig = OpenAILLMConfig(\n    model=\"gpt-4o-mini\",\n    openai_key=\"your-api-key\",\n    stream=True,  # Enable streaming\n    output_response=True  # Print tokens to console in real-time\n)\n\nllm = OpenAILLM(config=config)\n\n# All calls to generate() will now stream by default\nresponse = llm.generate(\n    prompt=\"Write a story about space exploration.\"\n)\n</code></pre>"},{"location":"modules/llm/#enable-streaming-in-the-generate-method","title":"Enable Streaming in the Generate Method","text":"<p>Alternatively, you can enable streaming for specific generate calls:</p> <pre><code># LLM initialized with default non-streaming behavior\nconfig = OpenAILLMConfig(\n    model=\"gpt-4o-mini\",\n    openai_key=\"your-api-key\"\n)\n\nllm = OpenAILLM(config=config)\n\n# Override for this specific call\nresponse = llm.generate(\n    prompt=\"Write a story about space exploration.\",\n    stream=True,  # Enable streaming for this call only\n    output_response=True  # Print tokens to console in real-time\n)\n</code></pre>"},{"location":"modules/workflow_graph/","title":"Workflow Graph","text":""},{"location":"modules/workflow_graph/#introduction","title":"Introduction","text":"<p>The <code>WorkFlowGraph</code> class is a fundamental component in the EvoAgentX framework for creating, managing, and executing complex AI agent workflows. It provides a structured way to define task dependencies, execution order, and the flow of data between tasks.</p> <p>A workflow graph represents a collection of tasks (nodes) and their dependencies (edges) that need to be executed in a specific order to achieve a goal. The <code>SequentialWorkFlowGraph</code> is a specialized implementation that focuses on linear workflows with a single path from start to end.</p>"},{"location":"modules/workflow_graph/#architecture","title":"Architecture","text":""},{"location":"modules/workflow_graph/#workflowgraph-architecture","title":"WorkFlowGraph Architecture","text":"<p>A <code>WorkFlowGraph</code> consists of several key components:</p> <ol> <li> <p>Nodes (WorkFlowNode): </p> <p>Each node represents a task or operation in the workflow, with the following properties:</p> <ul> <li><code>name</code>: A unique identifier for the task</li> <li><code>description</code>: Detailed description of what the task does</li> <li><code>inputs</code>: List of input parameters required by the task, each input parameter is an instance of <code>Parameter</code> class. </li> <li><code>outputs</code>: List of output parameters produced by the task, each output parameter is an instance of <code>Parameter</code> class. </li> <li><code>agents</code> (optional): List of agents that can execute this task, each agent should be a string that matches the name of the agent in the <code>agent_manager</code> or a dictionary that specifies the agent name and its configuration, which will be used to create a <code>CustomizeAgent</code> instance in the <code>agent_manager</code>.  Please refer to the Customize Agent documentation for more details about the agent configuration. </li> <li><code>action_graph</code> (optional): An instance of <code>ActionGraph</code> class, where each action is an instance of the <code>Operator</code> class. Please refer to the Action Graph documentation for more details about the action graph. </li> <li><code>status</code>: Current execution state of the task (PENDING, RUNNING, COMPLETED, FAILED).</li> </ul> <p>Note</p> <ol> <li> <p>You should provide either <code>agents</code> or <code>action_graph</code> to execute the task. If both are provided, <code>action_graph</code> will be used. </p> </li> <li> <p>If you provide a set of <code>agents</code>, these agents will work together to complete the task. When executing the task using <code>WorkFlow</code>, the system will automatically determine the execution sequence (actions) based on the agent information and execution history. Specifically, when executing the task, <code>WorkFlow</code> will analyze all the possible actions within these agents and repeatly select the best action to execute based on the task description and execution history. </p> </li> <li> <p>If you provide an <code>action_graph</code>, it will be directly used to complete the task. When executing the task with <code>WorkFlow</code>, the system will execute the actions in the order defined by the <code>action_graph</code> and return the results.  </p> </li> </ol> </li> <li> <p>Edges (WorkFlowEdge): </p> <p>Edges represent dependencies between tasks, defining execution order and data flow. Each edge has:</p> <ul> <li><code>source</code>: Name of the source node (where the edge starts)</li> <li><code>target</code>: Name of the target node (where the edge ends) </li> <li><code>priority</code> (optional): numeric priority to influence execution order</li> </ul> </li> <li> <p>Graph Structure:</p> <p>Internally, the workflow is represented as a directed graph where:</p> <ul> <li>Nodes represent tasks</li> <li>Edges represent dependencies and data flow between tasks</li> <li>The graph structure supports both linear sequences and more complex patterns:<ul> <li>Fork-join patterns (parallel execution paths that rejoin later)</li> <li>Conditional branches</li> <li>Potential cycles (loops) in the workflow</li> </ul> </li> </ul> </li> <li> <p>Node States:</p> <p>Each node in the workflow can be in one of the following states:</p> <ul> <li><code>PENDING</code>: The task is waiting to be executed</li> <li><code>RUNNING</code>: The task is currently being executed</li> <li><code>COMPLETED</code>: The task has been successfully executed</li> <li><code>FAILED</code>: The task execution has failed</li> </ul> </li> </ol>"},{"location":"modules/workflow_graph/#sequentialworkflowgraph-architecture","title":"SequentialWorkFlowGraph Architecture","text":"<p>The <code>SequentialWorkFlowGraph</code> is a specialized implementation of <code>WorkFlowGraph</code> that automatically infers node connections to create a linear workflow. It's designed for simpler use cases where tasks need to be executed in sequence, with outputs from one task feeding into the next.</p>"},{"location":"modules/workflow_graph/#input-format","title":"Input Format","text":"<p>The <code>SequentialWorkFlowGraph</code> accepts a simplified input format that makes it easy to define linear workflows. Instead of explicitly defining nodes and edges, you provide a list of tasks in the order they should be executed. Each task is defined as a dictionary with the following fields:</p> <ul> <li><code>name</code> (required): A unique identifier for the task</li> <li><code>description</code> (required): Detailed description of what the task does</li> <li><code>inputs</code> (required): List of input parameters for the task</li> <li><code>outputs</code> (required): List of output parameters produced by the task</li> <li><code>prompt</code> (required): The prompt template to guide the agent's behavior</li> <li><code>system_prompt</code> (optional): System message to provide context to the agent</li> <li><code>output_parser</code> (optional): The output parser to parse the output of the task </li> <li><code>parse_mode</code> (optional): Mode for parsing outputs, defaults to \"str\"</li> <li><code>parse_func</code> (optional): Custom function for parsing outputs</li> <li><code>parse_title</code> (optional): Title for the parsed output</li> </ul> <p>The parameters related to prompts and parsing will be used to create a <code>CustomizeAgent</code> instance in the <code>agent_manager</code>. Please refer to the Customize Agent documentation for more details about the agent configuration. </p>"},{"location":"modules/workflow_graph/#internal-conversion-to-workflowgraph","title":"Internal Conversion to WorkFlowGraph","text":"<p>Internally, <code>SequentialWorkFlowGraph</code> automatically converts this simplified task list into a complete <code>WorkFlowGraph</code> by:</p> <ol> <li> <p>Creating WorkFlowNode instances: For each task in the input list, it creates a <code>WorkFlowNode</code> with appropriate properties. During this process:</p> <ul> <li>It converts the task definition into a node with inputs, outputs, and an associated agent.</li> <li>It automatically generates a unique agent name based on the task name.</li> <li>It configures the agent with the provided prompt, system_prompt, and parsing options.</li> </ul> </li> <li> <p>Inferring edge connections: It examines the input and output parameters of each task and automatically creates <code>WorkFlowEdge</code> instances to connect tasks where outputs from one task match the inputs of another.</p> </li> <li> <p>Building the graph structure: Finally, it constructs the complete directed graph representing the workflow, with all nodes and edges properly connected.</p> </li> </ol> <p>This automatic conversion process makes it significantly easier to define sequential workflows without needing to manually specify all the graph components.</p>"},{"location":"modules/workflow_graph/#usage","title":"Usage","text":""},{"location":"modules/workflow_graph/#basic-workflowgraph-creation--execution","title":"Basic WorkFlowGraph Creation &amp; Execution","text":"<pre><code>from evoagentx.workflow.workflow_graph import WorkFlowNode, WorkFlowGraph, WorkFlowEdge\nfrom evoagentx.workflow.workflow import WorkFlow \nfrom evoagentx.agents import AgentManager, CustomizeAgent \nfrom evoagentx.models import OpenAILLMConfig, OpenAILLM \n\nllm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=\"xxx\", stream=True, output_response=True)\nllm = OpenAILLM(llm_config)\n\nagent_manager = AgentManager()\n\ndata_extraction_agent = CustomizeAgent(\n    name=\"DataExtractionAgent\",\n    description=\"Extract data from source\",\n    inputs=[{\"name\": \"data_source\", \"type\": \"string\", \"description\": \"Source data location\"}],\n    outputs=[{\"name\": \"extracted_data\", \"type\": \"string\", \"description\": \"Extracted data\"}],\n    prompt=\"Extract data from source: {data_source}\",\n    llm_config=llm_config\n)  \n\ndata_transformation_agent = CustomizeAgent(\n    name=\"DataTransformationAgent\",\n    description=\"Transform data\",\n    inputs=[{\"name\": \"extracted_data\", \"type\": \"string\", \"description\": \"Extracted data\"}],\n    outputs=[{\"name\": \"transformed_data\", \"type\": \"string\", \"description\": \"Transformed data\"}],\n    prompt=\"Transform data: {extracted_data}\",\n    llm_config=llm_config\n)\n\n# add agents to the agent manager for workflow execution \ndata_extraction_agent = agent_manager.add_agents(agents = [data_extraction_agent, data_transformation_agent])\n\n# Create workflow nodes\ntask1 = WorkFlowNode(\n    name=\"Task1\",\n    description=\"Extract data from source\",\n    inputs=[{\"name\": \"data_source\", \"type\": \"string\", \"description\": \"Source data location\"}],\n    outputs=[{\"name\": \"extracted_data\", \"type\": \"string\", \"description\": \"Extracted data\"}],\n    agents=[\"DataExtractionAgent\"] # should match the name of the agent in the agent manager\n)\n\ntask2 = WorkFlowNode(\n    name=\"Task2\",\n    description=\"Transform data\",\n    inputs=[{\"name\": \"extracted_data\", \"type\": \"string\", \"description\": \"Data to transform\"}],\n    outputs=[{\"name\": \"transformed_data\", \"type\": \"string\", \"description\": \"Transformed data\"}],\n    agents=[\"DataTransformationAgent\"] # should match the name of the agent in the agent manager\n)\n\ntask3 = WorkFlowNode(\n    name=\"Task3\",\n    description=\"Analyze data and generate insights\",\n    inputs=[{\"name\": \"transformed_data\", \"type\": \"string\", \"description\": \"Data to analyze\"}],\n    outputs=[{\"name\": \"insights\", \"type\": \"string\", \"description\": \"Generated insights\"}],\n    agents=[\n        {\n            \"name\": \"DataAnalysisAgent\",\n            \"description\": \"Analyze data and generate insights\",\n            \"inputs\": [{\"name\": \"transformed_data\", \"type\": \"string\", \"description\": \"Data to analyze\"}],\n            \"outputs\": [{\"name\": \"insights\", \"type\": \"string\", \"description\": \"Generated insights\"}],\n            \"prompt\": \"Analyze data and generate insights: {transformed_data}\",\n            \"parse_mode\": \"str\",\n        } # will be used to create a `CustomizeAgent` instance in the `agent_manager`\n    ]\n)\n\n# Create workflow edges\nedge1 = WorkFlowEdge(source=\"Task1\", target=\"Task2\")\nedge2 = WorkFlowEdge(source=\"Task2\", target=\"Task3\")\n\n# Create the workflow graph\nworkflow_graph = WorkFlowGraph(\n    goal=\"Extract, transform, and analyze data to generate insights\",\n    nodes=[task1, task2, task3],\n    edges=[edge1, edge2]\n)\n\n# add agents to the agent manager for workflow execution \nagent_manager.add_agents_from_workflow(workflow_graph, llm_config=llm_config)\n\n# create a workflow instance for execution \nworkflow = WorkFlow(graph=workflow_graph, agent_manager=agent_manager, llm=llm)\nworkflow.execute(inputs={\"data_source\": \"xxx\"})\n</code></pre>"},{"location":"modules/workflow_graph/#creating-a-sequentialworkflowgraph","title":"Creating a SequentialWorkFlowGraph","text":"<pre><code>from evoagentx.workflow.workflow_graph import SequentialWorkFlowGraph\n\n# Define tasks with their inputs, outputs, and prompts\ntasks = [\n    {\n        \"name\": \"DataExtraction\",\n        \"description\": \"Extract data from the specified source\",\n        \"inputs\": [\n            {\"name\": \"data_source\", \"type\": \"string\", \"required\": True, \"description\": \"Source data location\"}\n        ],\n        \"outputs\": [\n            {\"name\": \"extracted_data\", \"type\": \"string\", \"required\": True, \"description\": \"Extracted data\"}\n        ],\n        \"prompt\": \"Extract data from the following source: {data_source}\", \n        \"parse_mode\": \"str\"\n    },\n    {\n        \"name\": \"DataTransformation\",\n        \"description\": \"Transform the extracted data\",\n        \"inputs\": [\n            {\"name\": \"extracted_data\", \"type\": \"string\", \"required\": True, \"description\": \"Data to transform\"}\n        ],\n        \"outputs\": [\n            {\"name\": \"transformed_data\", \"type\": \"string\", \"required\": True, \"description\": \"Transformed data\"}\n        ],\n        \"prompt\": \"Transform the following data: {extracted_data}\", \n        \"parse_mode\": \"str\"\n    },\n    {\n        \"name\": \"DataAnalysis\",\n        \"description\": \"Analyze data and generate insights\",\n        \"inputs\": [\n            {\"name\": \"transformed_data\", \"type\": \"string\", \"required\": True, \"description\": \"Data to analyze\"}\n        ],\n        \"outputs\": [\n            {\"name\": \"insights\", \"type\": \"string\", \"required\": True, \"description\": \"Generated insights\"}\n        ],\n        \"prompt\": \"Analyze the following data and generate insights: {transformed_data}\", \n        \"parse_mode\": \"str\"\n    }\n]\n\n# Create the sequential workflow graph\nsequential_workflow_graph = SequentialWorkFlowGraph(\n    goal=\"Extract, transform, and analyze data to generate insights\",\n    tasks=tasks\n)\n</code></pre>"},{"location":"modules/workflow_graph/#saving-and-loading-a-workflow","title":"Saving and Loading a Workflow","text":"<pre><code># Save workflow\nworkflow_graph.save_module(\"examples/output/my_workflow.json\")\n\n# For SequentialWorkFlowGraph, use save_module and get_graph_info\nsequential_workflow_graph.save_module(\"examples/output/my_sequential_workflow.json\")\n</code></pre>"},{"location":"modules/workflow_graph/#visualizing-the-workflow","title":"Visualizing the Workflow","text":"<pre><code># Display the workflow graph with node statuses visually\nworkflow_graph.display()\n</code></pre> <p>The <code>WorkFlowGraph</code> and <code>SequentialWorkFlowGraph</code> classes provide a flexible and powerful way to design complex agent workflows, track their execution, and manage the flow of data between tasks. </p>"},{"location":"tutorial/aflow_optimizer/","title":"AFlow Optimizer Tutorial","text":"<p>This tutorial will guide you through the process of setting up and running the AFlow optimizer in EvoAgentX. We'll use the HumanEval benchmark as an example to demonstrate how to optimize a multi-agent workflow for code generation tasks.</p>"},{"location":"tutorial/aflow_optimizer/#1-overview","title":"1. Overview","text":"<p>The AFlow optimizer in EvoAgentX enables you to:</p> <ul> <li>Automatically optimize multi-agent workflows for specific task types (code generation, QA, math, etc.)</li> <li>Support different types of operators (Custom, CustomCodeGenerate, Test, ScEnsemble, etc.)</li> <li>Evaluate optimization results on benchmark datasets</li> <li>Use different LLMs for optimization and execution</li> </ul>"},{"location":"tutorial/aflow_optimizer/#2-setting-up-the-environment","title":"2. Setting Up the Environment","text":"<p>First, let's import the necessary modules for setting up the AFlow optimizer:</p> <pre><code>import os\nfrom dotenv import load_dotenv\nfrom evoagentx.optimizers import AFlowOptimizer\nfrom evoagentx.models import LiteLLMConfig, LiteLLM, OpenAILLMConfig, OpenAILLM\nfrom evoagentx.benchmark import AFlowHumanEval\n\n# Load environment variables\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")\n</code></pre>"},{"location":"tutorial/aflow_optimizer/#configure-the-llm-models","title":"Configure the LLM Models","text":"<p>Following the settings in the original AFlow implementation, the AFlow optimizer uses two different LLMs: 1. An optimizer LLM (e.g., Claude 3.5 Sonnet) for workflow optimization 2. An executor LLM (e.g., GPT-4o-mini) for task execution</p> <pre><code># Configure the optimizer LLM (Claude 3.5 Sonnet)\nclaude_config = LiteLLMConfig(\n    model=\"anthropic/claude-3-5-sonnet-20240620\", \n    anthropic_key=ANTHROPIC_API_KEY\n)\noptimizer_llm = LiteLLM(config=claude_config)\n\n# Configure the executor LLM (GPT-4o-mini)\nopenai_config = OpenAILLMConfig(\n    model=\"gpt-4o-mini\", \n    openai_key=OPENAI_API_KEY\n)\nexecutor_llm = OpenAILLM(config=openai_config)\n</code></pre>"},{"location":"tutorial/aflow_optimizer/#3-setting-up-the-components","title":"3. Setting Up the Components","text":""},{"location":"tutorial/aflow_optimizer/#step-1-define-task-configuration","title":"Step 1: Define Task Configuration","text":"<p>The AFlow optimizer requires a configuration that specifies the task type and available operators. Here's an example configuration for different task types:</p> <pre><code>EXPERIMENTAL_CONFIG = {\n    \"humaneval\": {\n        \"question_type\": \"code\", \n        \"operators\": [\"Custom\", \"CustomCodeGenerate\", \"Test\", \"ScEnsemble\"] \n    }, \n    \"mbpp\": {\n        \"question_type\": \"code\", \n        \"operators\": [\"Custom\", \"CustomCodeGenerate\", \"Test\", \"ScEnsemble\"] \n    },\n    \"hotpotqa\": {\n        \"question_type\": \"qa\", \n        \"operators\": [\"Custom\", \"AnswerGenerate\", \"QAScEnsemble\"]\n    },\n    \"gsm8k\": {\n        \"question_type\": \"math\", \n        \"operators\": [\"Custom\", \"ScEnsemble\", \"Programmer\"]\n    },\n    \"math\": {\n        \"question_type\": \"math\", \n        \"operators\": [\"Custom\", \"ScEnsemble\", \"Programmer\"]\n    }\n}\n</code></pre>"},{"location":"tutorial/aflow_optimizer/#step-2-define-initial-workflow","title":"Step 2: Define initial workflow","text":"<p>The AFlow optimizer requires two files:  - <code>graph.py</code>: which defines the initial workflow graph in python code.  - <code>prompt.py</code>: which defines the prompts used in the workflow. </p> <p>Below is an example of the <code>graph.py</code> file for the HumanEval benchmark:</p> <pre><code>import evoagentx.workflow.operators as operator\nimport examples.aflow.code_generation.prompt as prompt_custom # noqa: F401\nfrom evoagentx.models.model_configs import LLMConfig\nfrom evoagentx.benchmark.benchmark import Benchmark\nfrom evoagentx.models.model_utils import create_llm_instance\n\nclass Workflow:\n\n    def __init__(\n        self,\n        name: str,\n        llm_config: LLMConfig,\n        benchmark: Benchmark\n    ):\n        self.name = name\n        self.llm = create_llm_instance(llm_config)\n        self.benchmark = benchmark \n        self.custom = operator.Custom(self.llm)\n        self.custom_code_generate = operator.CustomCodeGenerate(self.llm)\n\n    async def __call__(self, problem: str, entry_point: str):\n        \"\"\"\n        Implementation of the workflow\n        Custom operator to generate anything you want.\n        But when you want to get standard code, you should use custom_code_generate operator.\n        \"\"\"\n        # await self.custom(input=, instruction=\"\")\n        solution = await self.custom_code_generate(problem=problem, entry_point=entry_point, instruction=prompt_custom.GENERATE_PYTHON_CODE_PROMPT) # But When you want to get standard code ,you should use customcodegenerator.\n        return solution['response']\n</code></pre> <p>Note</p> <p>When defining your workflow, please pay attention to the following key points:</p> <ol> <li> <p>Prompt Import Path: Ensure the import path for <code>prompt.py</code> is correctly specified (e.g., <code>examples.aflow.code_generation.prompt</code>). This path should match your project structure to enable proper prompt loading.</p> </li> <li> <p>Operator Initialization: In the <code>__init__</code> function, you must initialize all operators that will be used in the workflow. Each operator should be instantiated with the appropriate LLM instance. </p> </li> <li> <p>Workflow Execution: The <code>__call__</code> function serves as the main entry point for workflow execution. It should define the complete execution logic of your workflow and return the final output that will be used for evaluation.</p> </li> </ol> <p>Below is an example of the <code>prompt.py</code> file for the HumanEval benchmark:</p> <pre><code>GENERATE_PYTHON_CODE_PROMPT = \"\"\"\nGenerate a functional and correct Python code for the given problem.\n\nProblem: \"\"\"\n</code></pre> <p>Note</p> <p>If the workflow does not require any prompts, the <code>prompt.py</code> file can be empty. </p>"},{"location":"tutorial/aflow_optimizer/#step-3-prepare-the-benchmark","title":"Step 3: Prepare the Benchmark","text":"<p>For this tutorial, we'll use the AFlowHumanEval benchmark. It follows the exact same data split and format as used in the original AFlow implementation.</p> <pre><code># Initialize the benchmark\nhumaneval = AFlowHumanEval()\n</code></pre>"},{"location":"tutorial/aflow_optimizer/#4-configuring-and-running-the-aflow-optimizer","title":"4. Configuring and Running the AFlow Optimizer","text":"<p>The AFlow optimizer can be configured with various parameters to control the optimization process:</p> <pre><code>optimizer = AFlowOptimizer(\n    graph_path=\"examples/aflow/code_generation\",  # Path to the initial workflow graph\n    optimized_path=\"examples/aflow/humaneval/optimized\",  # Path to save optimized workflows\n    optimizer_llm=optimizer_llm,  # LLM for optimization\n    executor_llm=executor_llm,    # LLM for execution\n    validation_rounds=3,          # Number of times to run validation on the development set during optimization\n    eval_rounds=3,               # Number of times to run evaluation on the test set during testing\n    max_rounds=20,               # Maximum optimization rounds\n    **EXPERIMENTAL_CONFIG[\"humaneval\"]  # Task-specific configuration, used to specify the task type and available operators\n)\n</code></pre>"},{"location":"tutorial/aflow_optimizer/#running-the-optimization","title":"Running the Optimization","text":"<p>To start the optimization process:</p> <pre><code># Optimize the workflow\noptimizer.optimize(humaneval)\n</code></pre> <p>Note</p> <p>During optimization, the workflow will be validated on the development set for <code>validation_rounds</code> times at each step. Make sure the benchmark <code>humaneval</code> contains a development set (i.e., <code>self._dev_data</code> is not empty).</p>"},{"location":"tutorial/aflow_optimizer/#test-the-optimized-workflow","title":"Test the Optimized Workflow","text":"<p>To test the optimized workflow:</p> <p><pre><code>optimizer.test(humaneval)\n</code></pre> By default, the optimizer will choose the workflow with the highest validation performance to test. You can also specify the test rounds using the <code>test_rounds: List[int]</code> parameter. For example, to evaluate the second round and the third round, you can use <code>optimizer.test(humaneval, test_rounds=[2, 3])</code>.</p> <p>Note</p> <p>During testing, the workflow will be evaluated on the test set for <code>eval_rounds</code> times. Make sure the benchmark <code>humaneval</code> contains a test set (i.e., <code>self._test_data</code> is not empty).</p> <p>For a complete working example, please refer to aflow_humaneval.py.</p>"},{"location":"tutorial/benchmark_and_evaluation/","title":"Benchmark and Evaluation Tutorial","text":"<p>This tutorial will guide you through the process of setting up and running benchmark evaluations using EvoAgentX. We'll use the HotpotQA dataset as an example to demonstrate how to set up and run the evaluation process.</p>"},{"location":"tutorial/benchmark_and_evaluation/#1-overview","title":"1. Overview","text":"<p>EvoAgentX provides a flexible and modular evaluation framework that enables you to:</p> <ul> <li>Load and use predefined benchmark datasets</li> <li>Customize data loading, processing, and post-processing logic</li> <li>Evaluate the performance of your multi-agent workflows</li> <li>Process multiple evaluation tasks in parallel</li> </ul>"},{"location":"tutorial/benchmark_and_evaluation/#2-setting-up-the-benchmark","title":"2. Setting Up the Benchmark","text":"<p>To get started, you need to import the relevant modules and set up the language model (LLM) that your agent will use during evaluation.</p> <pre><code>from evoagentx.config import Config\nfrom evoagentx.models import OpenAIConfig, OpenAI \nfrom evoagentx.benchmark import HotpotQA\nfrom evoagentx.workflow import QAActionGraph \nfrom evoagentx.evaluators import Evaluator \nfrom evoagentx.core.callbacks import suppress_logger_info\n</code></pre>"},{"location":"tutorial/benchmark_and_evaluation/#configure-the-llm-model","title":"Configure the LLM Model","text":"<p>You'll need a valid OpenAI API key to initialize the LLM. It is recommended to save your API key in the <code>.env</code> file and load it using the <code>load_dotenv</code> function:  <pre><code>import os \nfrom dotenv import load_dotenv\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nllm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=OPENAI_API_KEY)\nllm = OpenAILLM(config=llm_config)\n</code></pre></p>"},{"location":"tutorial/benchmark_and_evaluation/#3-initialize-the-benchmark","title":"3. Initialize the Benchmark","text":"<p>EvoAgentX includes several predefined benchmarks for tasks like Question Answering, Math, and Coding. Please refer to the Benchmark README for more details about existing benchmarks. You can also define your own benchmark class by extending the base <code>Benchmark</code> interface, and we provide an example in the Custom Benchmark section.</p> <p>In this example, we will use the <code>HotpotQA</code> benchmark.  <pre><code>benchmark = HotPotQA(mode=\"dev\")\n</code></pre> where <code>mode</code> parameter determines which split of the dataset is loaded. Options include:</p> <ul> <li><code>\"train\"</code>: Training data</li> <li><code>\"dev\"</code>: Development/validation data</li> <li><code>\"test\"</code>: Test data</li> <li><code>\"all\"</code> (default): Loads the entire dataset</li> </ul> <p>The data will be automatically downloaded to a default cache folder, but you can change this location by specifying the <code>path</code> parameter.</p>"},{"location":"tutorial/benchmark_and_evaluation/#4-running-the-evaluation","title":"4. Running the Evaluation","text":"<p>Once you have your benchmark and LLM ready, the next step is to define your agent workflow and evaluation logic. EvoAgentX supports full customization of how benchmark examples are processed and how outputs are interpreted.</p> <p>Here's how to run an evaluation using the <code>HotpotQA</code> benchmark and a QA workflow.</p>"},{"location":"tutorial/benchmark_and_evaluation/#step-1-define-the-agent-workflow","title":"Step 1: Define the Agent Workflow","text":"<p>You can use one of the predefined workflows or implement your own. In this example, we use the <code>QAActionGraph</code> designed for question answering, which simply use self-consistency to generate the final answer:</p> <pre><code>workflow = QAActionGraph(\n    llm_config=llm_config,\n    description=\"This workflow aims to address multi-hop QA tasks.\"\n)\n</code></pre>"},{"location":"tutorial/benchmark_and_evaluation/#step-2-customize-data-preprocessing-and-post-processing","title":"Step 2: Customize Data Preprocessing and Post-processing","text":"<p>The next key aspect of evaluation is properly transforming data between your benchmark, workflow, and evaluation metrics.</p>"},{"location":"tutorial/benchmark_and_evaluation/#why-preprocessing-and-postprocessing-are-needed","title":"Why Preprocessing and Postprocessing Are Needed","text":"<p>In EvoAgentX, preprocessing and postprocessing are essential steps to ensure smooth interaction between benchmark data, workflows, and evaluation logic:</p> <ul> <li> <p>Preprocessing (<code>collate_func</code>):  </p> <p>The raw examples from a benchmark like HotpotQA typically consist of structured fields such as questions, answer, and context. However, your agent workflow usually expects a single prompt string or other structured input. The <code>collate_func</code> is used to convert each raw example into a format that can be consumed by your (custom) workflow.</p> </li> <li> <p>Postprocessing (<code>output_postprocess_func</code>):</p> <p>The workflow output might include reasoning steps or additional formatting beyond just the final answer. Since the <code>Evaluator</code> internally calls the benchmark's <code>evaluate</code> method to compute metrics (e.g., exact match or F1), it's often necessary to extract the final answer in a clean format. The <code>output_postprocess_func</code> handles this and ensures the output is in the right form for evaluation.</p> </li> </ul> <p>In short, preprocessing prepares benchmark examples for the workflow, while postprocessing prepares workflow outputs for evaluation.</p> <p>In the following example, we define a <code>collate_func</code> to format the raw examples into a prompt for the workflow, and a <code>output_postprocess_func</code> to extract the final answer from the workflow output.</p> <p>Each example in the benchmark can be formatted using a <code>collate_func</code>, which transforms raw examples into a prompt or structured input for the agent.</p> <pre><code>def collate_func(example: dict) -&gt; dict:\n    \"\"\"\n    Args:\n        example (dict): A dictionary containing the raw example data.\n\n    Returns: \n        The expected input for the (custom) workflow.\n    \"\"\"\n    problem = \"Question: {}\\n\\n\".format(example[\"question\"])\n    context_list = []\n    for item in example[\"context\"]:\n        context = \"Title: {}\\nText: {}\".format(item[0], \" \".join([t.strip() for t in item[1]]))\n        context_list.append(context)\n    context = \"\\n\\n\".join(context_list)\n    problem += \"Context: {}\\n\\n\".format(context)\n    problem += \"Answer:\" \n    return {\"problem\": problem}\n</code></pre> <p>After the agent generates an output, you can define how to extract the final answer using <code>output_postprocess_func</code>.  <pre><code>def output_postprocess_func(output: dict) -&gt; dict:\n    \"\"\"\n    Args:\n        output (dict): The output from the workflow.\n\n    Returns: \n        The processed output that can be used to compute the metrics. The output will be directly passed to the benchmark's `evaluate` method. \n    \"\"\"\n    return output[\"answer\"]\n</code></pre></p>"},{"location":"tutorial/benchmark_and_evaluation/#step-3-initialize-the-evaluator","title":"Step 3: Initialize the Evaluator","text":"<p>The Evaluator ties everything together \u2014 it runs the workflow over the benchmark and calculates performance metrics.</p> <p><pre><code>evaluator = Evaluator(\n    llm=llm, \n    collate_func=collate_func,\n    output_postprocess_func=output_postprocess_func,\n    verbose=True, \n    num_workers=3 \n)\n</code></pre> If <code>num_workers</code> is greater than 1, the evaluation will be parallelized across multiple threads.  </p>"},{"location":"tutorial/benchmark_and_evaluation/#step-4-run-the-evaluation","title":"Step 4: Run the Evaluation","text":"<p>You can now run the evaluation by providing the workflow and benchmark to the evaluator:</p> <p><pre><code>with suppress_logger_info():\n    results = evaluator.evaluate(\n        graph=workflow, \n        benchmark=benchmark, \n        eval_mode=\"dev\", # Evaluation split: train / dev / test \n        sample_k=10 # If set, randomly sample k examples from the benchmark for evaluation  \n    )\n\nprint(\"Evaluation metrics: \", results)\n</code></pre> where <code>suppress_logger_info</code> is used to suppress the logger info.</p> <p>Please refer to the benchmark and evaluation example for a complete example.</p>"},{"location":"tutorial/benchmark_and_evaluation/#custom-benchmark","title":"Custom Benchmark","text":"<p>To define a custom benchmark, you need to extend the <code>Benchmark</code> class and implement the following methods:</p> <ul> <li> <p><code>_load_data(self)</code>: </p> <p>Load the benchmark data, and set the <code>self._train_data</code>, <code>self._dev_data</code> and <code>self._test_data</code> attributes.</p> </li> <li> <p><code>_get_id(self, example: Any) -&gt; Any</code>: </p> <p>Return the unique identifier of an example.</p> </li> <li> <p><code>_get_label(self, example: Any) -&gt; Any</code>:</p> <p>Return the label or ground truth associated with a given example.</p> <p>This is used to compare predictions against the correct answer during evaluation. The output will be directly passed to the <code>evaluate</code> method. </p> </li> <li> <p><code>evaluate(self, prediction: Any, label: Any) -&gt; dict</code>: </p> <p>Compute the evaluation metrics for a single example, based on its prediction and ground-truth label (obtained from <code>_get_label</code>). This method should return a dictionary of metric name(s) and value(s).</p> </li> </ul> <p>For a complete example of a benchmark implementation, please refer to the HotPotQA class.</p>"},{"location":"tutorial/first_agent/","title":"Build Your First Agent","text":"<p>In EvoAgentX, agents are intelligent components designed to complete specific tasks autonomously. This tutorial will walk you through the essential concepts of creating and using agents in EvoAgentX:</p> <ol> <li>Creating a Simple Agent with CustomizeAgent: Learn how to create a basic agent with custom prompts </li> <li>Working with Multiple Actions: Create more complex agents that can perform multiple tasks</li> <li>Saving and Loading Agents: Learn how to save and load your agents</li> </ol> <p>By the end of this tutorial, you'll be able to create both simple and complex agents, understand how they process inputs and outputs, and know how to save and reuse them in your projects.</p>"},{"location":"tutorial/first_agent/#1-creating-a-simple-agent-with-customizeagent","title":"1. Creating a Simple Agent with CustomizeAgent","text":"<p>The easiest way to create an agent is using <code>CustomizeAgent</code>, which allows you to quickly define an agent with a specific prompt.  </p> <p>First, let's import the necessary components and setup the LLM:</p> <pre><code>import os \nfrom dotenv import load_dotenv\nfrom evoagentx.models import OpenAILLMConfig\nfrom evoagentx.agents import CustomizeAgent\n\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\n# Configure LLM\nopenai_config = OpenAILLMConfig(\n    model=\"gpt-4o-mini\", \n    openai_key=OPENAI_API_KEY, \n    stream=True\n)\n</code></pre> <p>Now, let's create a simple agent that prints hello world. There are two ways to create a CustomizeAgent:</p>"},{"location":"tutorial/first_agent/#method-1-direct-initialization","title":"Method 1: Direct Initialization","text":"<p>You can directly initialize the agent with the <code>CustomizeAgent</code> class:  <pre><code>first_agent = CustomizeAgent(\n    name=\"FirstAgent\",\n    description=\"A simple agent that prints hello world\",\n    prompt=\"Print 'hello world'\", \n    llm_config=openai_config # specify the LLM configuration \n)\n</code></pre></p>"},{"location":"tutorial/first_agent/#method-2-creating-from-dictionary","title":"Method 2: Creating from Dictionary","text":"<p>You can also create an agent by defining its configuration in a dictionary:</p> <pre><code>agent_data = {\n    \"name\": \"FirstAgent\",\n    \"description\": \"A simple agent that prints hello world\",\n    \"prompt\": \"Print 'hello world'\",\n    \"llm_config\": openai_config\n}\nfirst_agent = CustomizeAgent.from_dict(agent_data) # use .from_dict() to create an agent. \n</code></pre>"},{"location":"tutorial/first_agent/#using-the-agent","title":"Using the Agent","text":"<p>Once created, you can use the agent to print hello world. </p> <pre><code># Execute the agent without input. The agent will return a Message object containing the results. \nmessage = first_agent()\n\nprint(f\"Response from {first_agent.name}:\")\nprint(message.content.content) # the content of a Message object is a LLMOutputParser object, where the `content` attribute is the raw LLM output. \n</code></pre> <p>For a complete example, please refer to the CustomizeAgent example. </p> <p>CustomizeAgent also offers other features including structured inputs/outputs and multiple parsing strategies. For detailed information, see the CustomizeAgent documentation.</p>"},{"location":"tutorial/first_agent/#2-creating-an-agent-with-multiple-actions","title":"2. Creating an Agent with Multiple Actions","text":"<p>In EvoAgentX, you can create an agent with multiple predefined actions. This allows you to build more complex agents that can perform multiple tasks. Here's an example showing how to create an agent with <code>TestCodeGeneration</code> and <code>TestCodeReview</code> actions:</p>"},{"location":"tutorial/first_agent/#defining-actions","title":"Defining Actions","text":"<p>First, we need to define the actions, which are subclasses of <code>Action</code>:  <pre><code>from evoagentx.agents import Agent\nfrom evoagentx.actions import Action, ActionInput, ActionOutput\n\n# Define the CodeGeneration action inputs\nclass TestCodeGenerationInput(ActionInput):\n    requirement: str = Field(description=\"The requirement for the code generation\")\n\n# Define the CodeGeneration action outputs\nclass TestCodeGenerationOutput(ActionOutput):\n    code: str = Field(description=\"The generated code\")\n\n# Define the CodeGeneration action\nclass TestCodeGeneration(Action): \n\n    def __init__(\n        self, \n        name: str=\"TestCodeGeneration\", \n        description: str=\"Generate code based on requirements\", \n        prompt: str=\"Generate code based on requirements: {requirement}\",\n        inputs_format: ActionInput=None, \n        outputs_format: ActionOutput=None, \n        **kwargs\n    ):\n        inputs_format = inputs_format or TestCodeGenerationInput\n        outputs_format = outputs_format or TestCodeGenerationOutput\n        super().__init__(\n            name=name, \n            description=description, \n            prompt=prompt, \n            inputs_format=inputs_format, \n            outputs_format=outputs_format, \n            **kwargs\n        )\n\n    def execute(self, llm: Optional[BaseLLM] = None, inputs: Optional[dict] = None, sys_msg: Optional[str]=None, return_prompt: bool = False, **kwargs) -&gt; TestCodeGenerationOutput:\n        action_input_attrs = self.inputs_format.get_attrs() # obtain the attributes of the action input \n        action_input_data = {attr: inputs.get(attr, \"undefined\") for attr in action_input_attrs}\n        prompt = self.prompt.format(**action_input_data) # format the prompt with the action input data \n        output = llm.generate(\n            prompt=prompt, \n            system_message=sys_msg, \n            parser=self.outputs_format, \n            parse_mode=\"str\" # specify how to parse the output \n        )\n        if return_prompt:\n            return output, prompt\n        return output\n\n\n# Define the CodeReview action inputs\nclass TestCodeReviewInput(ActionInput):\n    code: str = Field(description=\"The code to be reviewed\")\n    requirements: str = Field(description=\"The requirements for the code review\")\n\n# Define the CodeReview action outputs\nclass TestCodeReviewOutput(ActionOutput):\n    review: str = Field(description=\"The review of the code\")\n\n# Define the CodeReview action\nclass TestCodeReview(Action):\n    def __init__(\n        self, \n        name: str=\"TestCodeReview\", \n        description: str=\"Review the code based on requirements\", \n        prompt: str=\"Review the following code based on the requirements:\\n\\nRequirements: {requirements}\\n\\nCode:\\n{code}. You should output a JSON object with the following fields: 'review'.\", \n        inputs_format: ActionInput=None, \n        outputs_format: ActionOutput=None, \n        **kwargs\n    ):\n        inputs_format = inputs_format or TestCodeReviewInput\n        outputs_format = outputs_format or TestCodeReviewOutput\n        super().__init__(\n            name=name, \n            description=description, \n            prompt=prompt, \n            inputs_format=inputs_format, \n            outputs_format=outputs_format, \n            **kwargs\n        )\n\n    def execute(self, llm: Optional[BaseLLM] = None, inputs: Optional[dict] = None, sys_msg: Optional[str]=None, return_prompt: bool = False, **kwargs) -&gt; TestCodeReviewOutput:\n        action_input_attrs = self.inputs_format.get_attrs()\n        action_input_data = {attr: inputs.get(attr, \"undefined\") for attr in action_input_attrs}\n        prompt = self.prompt.format(**action_input_data)\n        output = llm.generate(\n            prompt=prompt, \n            system_message=sys_msg,\n            parser=self.outputs_format, \n            parse_mode=\"json\" # specify how to parse the output \n        ) \n        if return_prompt:\n            return output, prompt\n        return output\n</code></pre></p> <p>From the above example, we can see that in order to define an action, we need to:</p> <ol> <li>Define the action inputs and outputs using <code>ActionInput</code> and <code>ActionOutput</code> classes</li> <li>Create an action class that inherits from <code>Action</code></li> <li>Implement the <code>execute</code> method which formulates the prompt with the action input data and uses the LLM to generate output, and specify how to parse the output using <code>parse_mode</code>.</li> </ol>"},{"location":"tutorial/first_agent/#defining-an-agent","title":"Defining an Agent","text":"<p>Once we have defined the actions, we can create an agent by adding the actions to it:</p> <pre><code># Initialize the LLM\nopenai_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=os.getenv(\"OPENAI_API_KEY\"))\n\n# Define the agent \ndeveloper = Agent(\n    name=\"Developer\", \n    description=\"A developer who can write code and review code\",\n    actions=[TestCodeGeneration(), TestCodeReview()], \n    llm_config=openai_config\n)\n</code></pre>"},{"location":"tutorial/first_agent/#executing-different-actions","title":"Executing Different Actions","text":"<p>Once you've created an agent with multiple actions, you can execute specific actions:</p> <pre><code># List all available actions on the agent\nactions = developer.get_all_actions()\nprint(f\"Available actions of agent {developer.name}:\")\nfor action in actions:\n    print(f\"- {action.name}: {action.description}\")\n\n# Generate some code using the CodeGeneration action\ngeneration_result = developer.execute(\n    action_name=\"TestCodeGeneration\", # specify the action name\n    action_input_data={ \n        \"requirement\": \"Write a function that returns the sum of two numbers\"\n    }\n)\n\n# Access the generated code\ngenerated_code = generation_result.content.code\nprint(\"Generated code:\")\nprint(generated_code)\n\n# Review the generated code using the CodeReview action\nreview_result = developer.execute(\n    action_name=\"TestCodeReview\",\n    action_input_data={\n        \"requirements\": \"Write a function that returns the sum of two numbers\",\n        \"code\": generated_code\n    }\n)\n\n# Access the review results\nreview = review_result.content.review\nprint(\"\\nReview:\")\nprint(review)\n</code></pre> <p>This example demonstrates how to: 1. List all available actions on an agent 2. Generate code using the TestCodeGeneration action 3. Review the generated code using the TestCodeReview action 4. Access the results from each action execution</p> <p>For a complete working example, please refer to the Agent example. </p>"},{"location":"tutorial/first_agent/#3-saving-and-loading-agents","title":"3. Saving and Loading Agents","text":"<p>You can save an agent to a file and load it later:</p> <pre><code># Save the agent to a file\ndeveloper.save_module(\"examples/output/developer.json\") # ignore the LLM config to avoid saving the API key \n\n# Load the agent from a file\ndeveloper = Agent.from_file(\"examples/output/developer.json\", llm_config=openai_config)\n</code></pre>"},{"location":"tutorial/first_workflow/","title":"Build Your First Workflow","text":"<p>In EvoAgentX, workflows allow multiple agents to collaborate sequentially on complex tasks. This tutorial will guide you through creating and using workflows:</p> <ol> <li>Understanding Sequential Workflows: Learn how workflows connect multiple tasks together</li> <li>Building a Sequential Workflow: Create a workflow with planning and coding steps</li> <li>Executing and Managing Workflows: Run workflows with specific inputs</li> </ol> <p>By the end of this tutorial, you'll be able to create sequential workflows that coordinate multiple agents to solve complex problems.</p>"},{"location":"tutorial/first_workflow/#1-understanding-sequential-workflows","title":"1. Understanding Sequential Workflows","text":"<p>A workflow in EvoAgentX represents a sequence of tasks that can be executed by different agents. The simplest workflow is a sequential workflow, where tasks are executed one after another with outputs from previous tasks feeding into subsequent ones.</p> <p>Let's start by importing the necessary components:</p> <pre><code>import os \nfrom dotenv import load_dotenv\nfrom evoagentx.workflow import SequentialWorkFlowGraph, WorkFlow\nfrom evoagentx.agents import AgentManager\nfrom evoagentx.models import OpenAILLMConfig, OpenAILLM\n\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n</code></pre>"},{"location":"tutorial/first_workflow/#2-building-a-sequential-workflow","title":"2. Building a Sequential Workflow","text":"<p>A sequential workflow consists of a series of tasks where each task has:</p> <ul> <li>A name and description</li> <li>Input and output definitions</li> <li>A prompt template</li> <li>Parsing mode and function (optional) </li> </ul> <p>Here's how to build a sequential workflow with planning and coding tasks:</p> <pre><code># Configure the LLM \nllm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=OPENAI_API_KEY, stream=True, output_response=True)\nllm = OpenAILLM(llm_config)\n\n# Define a custom parsing function (if needed)\nfrom evoagentx.core.registry import register_parse_function\nfrom evoagentx.core.module_utils import extract_code_blocks\n\n# [optional] Define a custom parsing function (if needed)\n# It is suggested to use the `@register_parse_function` decorator to register a custom parsing function, so the workflow can be saved and loaded correctly.  \n\n@register_parse_function\ndef custom_parse_func(content: str) -&gt; str:\n    return {\"code\": extract_code_blocks(content)[0]}\n\n# Define sequential tasks\ntasks = [\n    {\n        \"name\": \"Planning\",\n        \"description\": \"Create a detailed plan for code generation\",\n        \"inputs\": [\n            {\"name\": \"problem\", \"type\": \"str\", \"required\": True, \"description\": \"Description of the problem to be solved\"},\n        ],\n        \"outputs\": [\n            {\"name\": \"plan\", \"type\": \"str\", \"required\": True, \"description\": \"Detailed plan with steps, components, and architecture\"}\n        ],\n        \"prompt\": \"You are a software architect. Your task is to create a detailed implementation plan for the given problem.\\n\\nProblem: {problem}\\n\\nPlease provide a comprehensive implementation plan including:\\n1. Problem breakdown\\n2. Algorithm or approach selection\\n3. Implementation steps\\n4. Potential edge cases and solutions\",\n        \"parse_mode\": \"str\",\n        # \"llm_config\": specific_llm_config # if you want to use a specific LLM for a task, you can add a key `llm_config` in the task dict.  \n    },\n    {\n        \"name\": \"Coding\",\n        \"description\": \"Implement the code based on the implementation plan\",\n        \"inputs\": [\n            {\"name\": \"problem\", \"type\": \"str\", \"required\": True, \"description\": \"Description of the problem to be solved\"},\n            {\"name\": \"plan\", \"type\": \"str\", \"required\": True, \"description\": \"Detailed implementation plan from the Planning phase\"},\n        ],\n        \"outputs\": [\n            {\"name\": \"code\", \"type\": \"str\", \"required\": True, \"description\": \"Implemented code with explanations\"}\n        ],\n        \"prompt\": \"You are a software developer. Your task is to implement the code based on the provided problem and implementation plan.\\n\\nProblem: {problem}\\nImplementation Plan: {plan}\\n\\nPlease provide the implementation code with appropriate comments.\",\n        \"parse_mode\": \"custom\",\n        \"parse_func\": custom_parse_func\n    }\n]\n\n# Create the sequential workflow graph\ngraph = SequentialWorkFlowGraph(\n    goal=\"Generate code to solve programming problems\",\n    tasks=tasks\n)\n</code></pre> <p>Note</p> <p>When you create a <code>SequentialWorkFlowGraph</code> with a list of tasks, the framework will create a <code>CustomizeAgent</code> for each task. Each task in the workflow becomes a specialized agent configured with the specific prompt, input/output formats, and parsing mode you defined. These agents are connected in sequence, with outputs from one agent becoming inputs to the next. </p> <p>The <code>parse_mode</code> controls how the output from an LLM is parsed into a structured format. Available options are: [<code>'str'</code> (default), <code>'json'</code>, <code>'title'</code>, <code>'xml'</code>, <code>'custom'</code>]. For detailed information about parsing modes and examples, please refer to the CustomizeAgent documentation.</p>"},{"location":"tutorial/first_workflow/#3-executing-and-managing-workflows","title":"3. Executing and Managing Workflows","text":"<p>Once you've created a workflow graph, you can create an instance of the workflow and execute it:</p> <pre><code># Create agent manager and add agents from the workflow. It will create a `CustomizeAgent` for each task in the workflow. \nagent_manager = AgentManager()\nagent_manager.add_agents_from_workflow(\n    graph, \n    llm_config=llm_config  # This config will be used for all tasks without `llm_config`. \n)\n\n# Create workflow instance\nworkflow = WorkFlow(graph=graph, agent_manager=agent_manager, llm=llm)\n\n# Execute the workflow with inputs\noutput = workflow.execute(\n    inputs = {\n        \"problem\": \"Write a function to find the longest palindromic substring in a given string.\"\n    }\n)\n\nprint(\"Workflow completed!\")\nprint(\"Workflow output:\\n\", output)\n</code></pre> <p>You should specify all the required inputs for the workflow in the <code>inputs</code> argument of the <code>execute</code> method. </p> <p>For a complete working example, please refer to the Sequential Workflow example. </p>"},{"location":"tutorial/first_workflow/#4-saving-and-loading-workflows","title":"4. Saving and Loading Workflows","text":"<p>You can save a workflow graph for future use:</p> <pre><code># Save the workflow graph to a file\ngraph.save_module(\"examples/output/saved_sequential_workflow.json\")\n\n# Load the workflow graph from a file\nloaded_graph = SequentialWorkFlowGraph.from_file(\"examples/output/saved_sequential_workflow.json\")\n\n# Create a new workflow with the loaded graph\nnew_workflow = WorkFlow(graph=loaded_graph, agent_manager=agent_manager, llm=llm)\n</code></pre> <p>For more complex workflows or different types of workflow graphs, please refer to the Workflow Graphs documentation and the Action Graphs documentation. </p>"},{"location":"tutorial/sew_optimizer/","title":"SEW Optimizer Tutorial","text":"<p>This tutorial will guide you through the process of setting up and running the SEW (Self-Evolving Workflow) optimizer in EvoAgentX. We'll use the HumanEval benchmark as an example to demonstrate how to optimize a multi-agent workflow.</p>"},{"location":"tutorial/sew_optimizer/#1-overview","title":"1. Overview","text":"<p>The SEW optimizer is a powerful tool in EvoAgentX that enables you to:</p> <ul> <li>Automatically optimize multi-agent workflows (prompts and workflow structure)</li> <li>Evaluate optimization results on benchmark datasets</li> <li>Support different workflow representation scheme (Python, Yaml, BPMN, etc.)</li> </ul>"},{"location":"tutorial/sew_optimizer/#2-setting-up-the-environment","title":"2. Setting Up the Environment","text":"<p>First, let's import the necessary modules for setting up the SEW optimizer:</p> <pre><code>from evoagentx.config import Config\nfrom evoagentx.models import OpenAILLMConfig, OpenAILLM\nfrom evoagentx.workflow import SEWWorkFlowGraph \nfrom evoagentx.agents import AgentManager\nfrom evoagentx.benchmark import HumanEval \nfrom evoagentx.evaluators import Evaluator \nfrom evoagentx.optimizers import SEWOptimizer \nfrom evoagentx.core.callbacks import suppress_logger_info\n</code></pre>"},{"location":"tutorial/sew_optimizer/#configure-the-llm-model","title":"Configure the LLM Model","text":"<p>Similar to other components in EvoAgentX, you'll need a valid OpenAI API key to initialize the LLM. </p> <pre><code>llm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=OPENAI_API_KEY)\nllm = OpenAILLM(config=llm_config)\n</code></pre>"},{"location":"tutorial/sew_optimizer/#3-setting-up-the-components","title":"3. Setting Up the Components","text":""},{"location":"tutorial/sew_optimizer/#step-1-initialize-the-sew-workflow","title":"Step 1: Initialize the SEW Workflow","text":"<p>The SEW workflow is the core component that will be optimized. It represents a sequential workflow that aims to solve the code generation task. </p> <pre><code>sew_graph = SEWWorkFlowGraph(llm_config=llm_config)\nagent_manager = AgentManager()\nagent_manager.add_agents_from_workflow(sew_graph)\n</code></pre>"},{"location":"tutorial/sew_optimizer/#step-2-prepare-the-benchmark","title":"Step 2: Prepare the Benchmark","text":"<p>For this tutorial, we'll use a modified version of the HumanEval benchmark that splits the test data into development and test sets:</p> <pre><code>class HumanEvalSplits(HumanEval):\n    def _load_data(self):\n        # load the original test data \n        super()._load_data()\n        # split the data into dev and test\n        import numpy as np \n        np.random.seed(42)\n        num_dev_samples = int(len(self._test_data) * 0.2)\n        random_indices = np.random.permutation(len(self._test_data))\n        self._dev_data = [self._test_data[i] for i in random_indices[:num_dev_samples]]\n        self._test_data = [self._test_data[i] for i in random_indices[num_dev_samples:]]\n\n# Initialize the benchmark\nhumaneval = HumanEvalSplits()\n</code></pre> <p>The SEWOptimizer will evaluate the performance on the development set by default. Please make sure the benchmark has a development set properly set up. You can either:    - Use a benchmark that already provides a development set (like HotPotQA)    - Split your dataset into development and test sets (like in the HumanEvalSplits example above)    - Implement a custom benchmark with development set support</p>"},{"location":"tutorial/sew_optimizer/#step-3-set-up-the-evaluator","title":"Step 3: Set Up the Evaluator","text":"<p>The evaluator is responsible for assessing the performance of the workflow during optimization. For more detailed information about how to set up and use the evaluator, please refer to the Benchmark and Evaluation Tutorial.</p> <pre><code>def collate_func(example: dict) -&gt; dict:\n    # convert raw example to the expected input for the SEW workflow\n    return {\"question\": example[\"prompt\"]}\n\nevaluator = Evaluator(\n    llm=llm, \n    agent_manager=agent_manager, \n    collate_func=collate_func, \n    num_workers=5, \n    verbose=True\n)\n</code></pre>"},{"location":"tutorial/sew_optimizer/#4-configuring-and-running-the-sew-optimizer","title":"4. Configuring and Running the SEW Optimizer","text":"<p>The SEW optimizer can be configured with various parameters to control the optimization process:</p> <pre><code>optimizer = SEWOptimizer(\n    graph=sew_graph,           # The workflow graph to optimize\n    evaluator=evaluator,       # The evaluator for performance assessment\n    llm=llm,                   # The language model\n    max_steps=10,             # Maximum optimization steps\n    eval_rounds=1,            # Number of evaluation rounds per step\n    repr_scheme=\"python\",     # Representation scheme for the workflow\n    optimize_mode=\"prompt\",   # What aspect to optimize (prompt/structure/all)\n    order=\"zero-order\"        # Optimization algorithm order (zero-order/first-order)\n)\n</code></pre>"},{"location":"tutorial/sew_optimizer/#running-the-optimization","title":"Running the Optimization","text":"<p>To start the optimization process:</p> <pre><code># Optimize the SEW workflow\noptimizer.optimize(dataset=humaneval)\n\n# Evaluate the optimized workflow\nwith suppress_logger_info():\n    metrics = optimizer.evaluate(dataset=humaneval, eval_mode=\"test\")\nprint(\"Evaluation metrics: \", metrics)\n\n# Save the optimized SEW workflow\noptimizer.save(\"debug/optimized_sew_workflow.json\")\n</code></pre> <p>For a complete working example, please refer to sew_optimizer.py.</p>"},{"location":"tutorial/textgrad_optimizer/","title":"TextGrad Optimizer Tutorial","text":"<p>This tutorial will guide you through the process of setting up and running the TextGrad optimizer in EvoAgentX. We'll use the MATH dataset as an example to demonstrate how to optimize the prompts and system prompts in a workflow.</p>"},{"location":"tutorial/textgrad_optimizer/#1-textgrad","title":"1. TextGrad","text":"<p>TextGrad uses textual feedback from LLM to improve text variables. In EvoAgentX, we use TextGrad to optimize  agents' prompts and system prompts. For more information on TextGrad, see their paper and GitHub.</p>"},{"location":"tutorial/textgrad_optimizer/#2-textgrad-optimizer","title":"2. TextGrad Optimizer","text":"<p>The TextGrad optimizer in EvoAgentX enables you to:</p> <ul> <li>Automatically optimize multi-agent workflows (prompts and/or system prompts)</li> <li>Evaluate optimization results on datasets</li> </ul>"},{"location":"tutorial/textgrad_optimizer/#3-setting-up-the-environment","title":"3. Setting Up the Environment","text":"<p>First, let's import the necessary modules for setting up the TextGrad optimizer:</p> <pre><code>from evoagentx.benchmark import MATH\nfrom evoagentx.optimizers import TextGradOptimizer\nfrom evoagentx.models import OpenAILLMConfig, OpenAILLM\nfrom evoagentx.workflow import SequentialWorkFlowGraph\nfrom evoagentx.core.callbacks import suppress_logger_info\n</code></pre>"},{"location":"tutorial/textgrad_optimizer/#configure-the-llm-model","title":"Configure the LLM Model","text":"<p>You'll need a valid API key to initialize the LLM. See Quickstart for more details on how to set up your API key.</p> <p><code>TextGradOptimizer</code> allows the use of different LLMs for workflow execution and optimization. For example, we can use GPT 4o-mini for workflow execution and GPT 4o for optimization.</p> <pre><code>executor_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=\"your_openai_api_key\")\nexecutor_llm = OpenAILLM(config=executor_config)\n\noptimizer_config = OpenAILLMConfig(model=\"gpt-4o\", openai_key=\"your_openai_api_key\")\noptimizer_llm = OpenAILLM(config=optimizer_config)\n</code></pre>"},{"location":"tutorial/textgrad_optimizer/#3-setting-up-the-components","title":"3. Setting Up the Components","text":""},{"location":"tutorial/textgrad_optimizer/#step-1-initialize-the-workflow","title":"Step 1: Initialize the Workflow","text":"<p>Currently, <code>TextGradOptimizer</code> only supports <code>SequentialWorkFlowGraph</code>. See Workflow Graph for more information on <code>SequentialWorkFlowGraph</code>. For this example, let us create the simplest workflow with only a single node.</p> <pre><code>math_graph_data = {\n    \"goal\": r\"Answer the math question. The answer should be in box format, e.g., \\boxed{123}\",\n    \"tasks\": [\n        {\n            \"name\": \"answer_generate\",\n            \"description\": \"Answer generation for Math.\",\n            \"inputs\": [\n                {\"name\": \"problem\", \"type\": \"str\", \"required\": True, \"description\": \"The problem to solve.\"}\n            ],\n            \"outputs\": [\n                {\"name\": \"answer\", \"type\": \"str\", \"required\": True, \"description\": \"The generated answer.\"}\n            ],\n            \"prompt\": \"Answer the math question. The answer should be in box format, e.g., \\\\boxed{{123}}\\n\\nProblem: {problem}\",\n            \"parse_mode\": \"str\"\n        }\n    ] \n}\n\nworkflow_graph = SequentialWorkFlowGraph.from_dict(math_graph_data)\n</code></pre>"},{"location":"tutorial/textgrad_optimizer/#step-2-prepare-the-dataset","title":"Step 2: Prepare the dataset","text":"<p>For this tutorial, we will use the MATH dataset which consists of challenging competition mathematics problems, spanning various difficulty levels and subject areas. The dataset is split into 7.5K training problems and 5K test problems. For demonstration purpose, let's take a smaller subset of the dataset to speed up the validation and evaluation process.</p> <pre><code>class MathSplits(MATH):\n    def _load_data(self):\n        super()._load_data()\n        import numpy as np \n        np.random.seed(42)\n        permutation = np.random.permutation(len(self._test_data))\n        full_test_data = self._test_data\n        # randomly select 10 samples for train, 40 for dev and 100 for test\n        self._train_data = [full_test_data[idx] for idx in permutation[:10]]\n        self._dev_data = [full_test_data[idx] for idx in permutation[10:50]]\n        self._test_data = [full_test_data[idx] for idx in permutation[50:150]]\n\nmath_splits = MathSplits()\n</code></pre> <p>During optimization, the <code>TextGradOptimizer</code> will evaluate the performance on the development set by default. Please make sure the dataset has a development set properly set up (i.e., <code>benchmark._dev_data</code> is not None). You can either:    - Use a dataset that already provides a development set    - Split your dataset to create a development set (like in the example above)    - Implement a custom dataset (inherits from <code>evoagentx.benchmark.Benchmark</code>) that properly sets up the development set. </p>"},{"location":"tutorial/textgrad_optimizer/#step-3-set-up-the-evaluator","title":"Step 3: Set Up the Evaluator","text":"<p>The evaluator is responsible for assessing the performance of the workflow during optimization. For more detailed information about how to set up and use the evaluator, please refer to the Benchmark and Evaluation Tutorial.</p> <pre><code>def collate_func(example: dict) -&gt; dict:\n    return {\"problem\": example[\"problem\"]}\n\nevaluator = Evaluator(\n    llm=llm, \n    agent_manager=agent_manager, \n    collate_func=collate_func, \n    num_workers=5, \n    verbose=True\n)\n</code></pre>"},{"location":"tutorial/textgrad_optimizer/#4-configuring-and-running-the-textgrad-optimizer","title":"4. Configuring and Running the TextGrad Optimizer","text":"<p>The TextGradOptimizer can be configured with various parameters to control the optimization process:</p> <ul> <li><code>graph</code>: The workflow graph to optimize</li> <li><code>optimize_mode</code>: The mode of optimization:<ul> <li>\"all\": optimize prompts and system prompts</li> <li>\"prompt\": optimize only the prompts</li> <li>\"system_prompt\": optimize only the system prompts</li> </ul> </li> <li><code>executor_llm</code>: The LLM used to execute the workflow</li> <li><code>optimizer_llm</code>: The LLM used to optimize the workflow</li> <li><code>batch_size</code>: The batch size for optimization</li> <li><code>max_steps</code>: The maximum number of optimization steps</li> <li><code>evaluator</code>: The evaluator to perform evaluation during optimization.</li> <li><code>eval_interval</code>: The number of steps between evaluations</li> <li><code>eval_rounds</code>: The number of evaluation rounds</li> <li><code>eval_config</code>: The evaluation configuration during optimization (passed to <code>TextGradOptimizer.evaluate()</code>). For example, if we don't want to evaluate on the entire development set, we can set  <code>eval_config = {\"sample_k\": 100}</code> to only evaluate on 100 random samples from the development set.</li> <li><code>save_interval</code>: The number of steps between saving the workflow graph</li> <li><code>save_path</code>: The path to save the workflow graph</li> <li><code>rollback</code>: Whether to rollback to the best workflow graph during optimization</li> </ul> <pre><code>textgrad_optimizer = TextGradOptimizer(\n    graph=workflow_graph, \n    optimize_mode=\"all\",\n    executor_llm=executor_llm, \n    optimizer_llm=optimizer_llm,\n    batch_size=3,\n    max_steps=20,\n    evaluator=evaluator,\n    eval_interval=1,\n    eval_rounds=1,\n    save_interval=None,\n    save_path=\"./\",\n    rollback=True\n)\n</code></pre>"},{"location":"tutorial/textgrad_optimizer/#running-the-optimization","title":"Running the Optimization","text":"<p>To start the optimization process: <pre><code>textgrad_optimizer.optimize(dataset=math_splits, seed=8)\n</code></pre> The <code>seed</code> is used for shuffling the training data. The training data is automatically re-shuffled every epoch. If <code>seed</code> is provided, the effective seed for shuffling the training data is <code>seed + epoch</code>.</p> <p>The final graph at the end of the optimization is not necessarily the best graph. If you wish to restore the graph that performed best on the development set, simply call <pre><code>textgrad_optimizer.restore_best_graph()\n</code></pre></p> <p>We can evaluate the workflow again to see the improvement after optimization. <pre><code>with suppress_logger_info():\n    result = textgrad_optimizer.evaluate(dataset=math_splits, eval_mode=\"test\")\nprint(f\"Evaluation result (after optimization):\\n{result}\")\n</code></pre></p> <p><code>TextGradOptimizer</code> always saves the final workflow graph and the best workflow graph to <code>save_path</code>. It also saves graphs during optimization if <code>save_interval</code> is not <code>None</code>. You can also save the workflow graph manually by calling <code>textgrad_optimizer.save()</code>.</p> <p>Note that <code>TextGradOptimizer</code> does not change the workflow structure but saving the workflow graph also saves the prompts and system prompts which will be different after optimization. Below is an example of a saved workflow graph after optimization using <code>TextGradOptimizer</code>.</p> <pre><code>{\n    \"class_name\": \"SequentialWorkFlowGraph\",\n    \"goal\": \"Answer the math question. The answer should be in box format, e.g., \\\\boxed{123}\",\n    \"tasks\": [\n        {\n            \"name\": \"answer_generate\",\n            \"description\": \"Answer generation for Math.\",\n            \"inputs\": [\n                {\n                    \"name\": \"problem\",\n                    \"type\": \"str\",\n                    \"description\": \"The problem to solve.\",\n                    \"required\": true\n                }\n            ],\n            \"outputs\": [\n                {\n                    \"name\": \"answer\",\n                    \"type\": \"str\",\n                    \"description\": \"The generated answer.\",\n                    \"required\": true\n                }\n            ],\n            \"prompt\": \"To solve the math problem, follow these steps:\\n\\n1. **Contextual Overview**: Begin with a brief overview of the problem-solving strategy, using logical reasoning and mathematical principles to derive the solution. Include any relevant geometric or algebraic insights.\\n\\n2. **Key Steps Identification**: Break down the problem-solving process into distinct parts:\\n   - Identify the relevant mathematical operations and properties, such as symmetry, roots of unity, or trigonometric identities.\\n   - Perform the necessary calculations, ensuring each step logically follows from the previous one.\\n   - Present the final answer.\\n\\n3. **Conciseness and Clarity**: Provide a clear and concise explanation of your solution, avoiding unnecessary repetition. Use consistent formatting and notation throughout.\\n\\n4. **Mathematical Justification**: Explain the reasoning behind each step to ensure the solution is well-justified. Include explanations of reference angles, geometric interpretations, and any special conditions or edge cases.\\n\\n5. **Verification Step**: Include a quick verification step to confirm the accuracy of your calculations. Consider recalculating key values if initial assumptions were incorrect.\\n\\n6. **Visual Aids**: Where applicable, include diagrams or sketches to visually represent the problem and solution, enhancing understanding.\\n\\n7. **Final Answer Presentation**: Present the final answer clearly and ensure it is boxed, reflecting the correct solution. Verify that it aligns with the problem's requirements and any known correct solutions.\\n\\nProblem: &lt;input&gt;{problem}&lt;/input&gt;\",\n            \"system_prompt\": \"You are a math-focused assistant dedicated to providing clear, concise, and educational solutions to mathematical problems. Your goal is to deliver structured and pedagogically sound explanations, ensuring mathematical accuracy and logical reasoning. Begin with a brief overview of the problem-solving approach, followed by detailed calculations, and conclude with a verification step. Use precise mathematical notation and consider potential edge cases. Present the final answer clearly, using the specified format, and incorporate visual aids or analogies where appropriate to enhance understanding and engagement. \\n\\nExplicitly include geometric explanations when applicable, describing the geometric context and relationships. Emphasize the importance of visual aids, such as diagrams or sketches, to enhance understanding. Ensure consistency in formatting and mathematical notation. Provide a brief explanation of the reference angle concept and its significance. Include contextual explanations of trigonometric identities and their applications. Critically evaluate initial assumptions and verify geometric properties before proceeding. Highlight the use of symmetry and conjugate pairs in complex numbers. Encourage re-evaluation and verification of steps, ensuring logical flow and clarity. Focus on deriving the correct answer and consider problem-specific strategies or known techniques.\",\n            \"parse_mode\": \"str\",\n            \"parse_func\": null,\n            \"parse_title\": null\n        }\n    ]\n}\n</code></pre> <p>For a complete working example, please refer to examples/textgrad/math_textgrad.py. You can also find TextGrad optimization scripts for other datasets in examples/textgrad.</p>"},{"location":"zh/","title":"EvoAgentX","text":"<p> \u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u548c\u6f14\u8fdb\u4ee3\u7406\u5de5\u4f5c\u6d41\u7684\u81ea\u52a8\u5316\u6846\u67b6\u3002 </p> <p> </p>"},{"location":"zh/#-\u7b80\u4ecb","title":"\ud83d\ude80 \u7b80\u4ecb","text":"<p>EvoAgentX \u662f\u4e00\u4e2a\u5f00\u6e90\u6846\u67b6\uff0c\u65e8\u5728\u81ea\u52a8\u5316\u4ee3\u7406\u5de5\u4f5c\u6d41\u7684\u751f\u6210\u3001\u6267\u884c\u3001\u8bc4\u4f30\u548c\u4f18\u5316\u3002\u901a\u8fc7\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0cEvoAgentX \u4f7f\u5f00\u53d1\u8005\u548c\u7814\u7a76\u4eba\u5458\u80fd\u591f\u5feb\u901f\u6784\u5efa\u3001\u6d4b\u8bd5\u548c\u90e8\u7f72\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u53ef\u4ee5\u968f\u7740\u65f6\u95f4\u63a8\u79fb\u5728\u590d\u6742\u6027\u548c\u80fd\u529b\u4e0a\u4e0d\u65ad\u589e\u957f\u3002</p>"},{"location":"zh/#-\u4e3b\u8981\u7279\u6027","title":"\u2728 \u4e3b\u8981\u7279\u6027","text":"<ul> <li>\u7b80\u5355\u7684\u4ee3\u7406\u548c\u5de5\u4f5c\u6d41\u5b9a\u5236\uff1a\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u8f7b\u677e\u521b\u5efa\u81ea\u5b9a\u4e49\u4ee3\u7406\u548c\u5de5\u4f5c\u6d41\u3002EvoAgentX \u8ba9\u4f60\u80fd\u591f\u8f7b\u677e\u5730\u5c06\u9ad8\u5c42\u6b21\u60f3\u6cd5\u8f6c\u5316\u4e3a\u53ef\u5de5\u4f5c\u7684\u7cfb\u7edf\u3002</li> <li>\u81ea\u52a8\u5de5\u4f5c\u6d41\u751f\u6210\u4e0e\u6267\u884c\uff1a\u4ece\u7b80\u5355\u7684\u76ee\u6807\u63cf\u8ff0\u81ea\u52a8\u751f\u6210\u548c\u6267\u884c\u4ee3\u7406\u5de5\u4f5c\u6d41\uff0c\u51cf\u5c11\u591a\u4ee3\u7406\u7cfb\u7edf\u8bbe\u8ba1\u4e2d\u7684\u624b\u52a8\u5de5\u4f5c\u91cf\u3002</li> <li>\u5de5\u4f5c\u6d41\u4f18\u5316\uff1a\u96c6\u6210\u73b0\u6709\u5de5\u4f5c\u6d41\u4f18\u5316\u6280\u672f\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u63d0\u5347\u5de5\u4f5c\u6d41\u6027\u80fd\u3002</li> <li>\u57fa\u51c6\u6d4b\u8bd5\u4e0e\u8bc4\u4f30\uff1a\u5305\u542b\u5185\u7f6e\u57fa\u51c6\u6d4b\u8bd5\u548c\u6807\u51c6\u5316\u8bc4\u4f30\u6307\u6807\uff0c\u7528\u4e8e\u8861\u91cf\u4e0d\u540c\u4efb\u52a1\u548c\u4ee3\u7406\u914d\u7f6e\u4e0b\u7684\u5de5\u4f5c\u6d41\u6548\u679c\u3002</li> <li>\u5de5\u4f5c\u6d41\u6267\u884c\u5de5\u5177\u5305\uff1a\u63d0\u4f9b\u6267\u884c\u590d\u6742\u5de5\u4f5c\u6d41\u6240\u9700\u7684\u4e00\u7cfb\u5217\u5de5\u5177\uff0c\u5982\u641c\u7d22\u7ec4\u4ef6\u548c\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCP\uff09\u3002</li> </ul>"},{"location":"zh/#-\u5de5\u4f5c\u539f\u7406","title":"\ud83d\udd0d \u5de5\u4f5c\u539f\u7406","text":"<p>EvoAgentX \u4f7f\u7528\u6a21\u5757\u5316\u67b6\u6784\uff0c\u5305\u542b\u4ee5\u4e0b\u6838\u5fc3\u7ec4\u4ef6\uff1a</p> <ol> <li>\u5de5\u4f5c\u6d41\u751f\u6210\u5668\uff1a\u6839\u636e\u4f60\u7684\u76ee\u6807\u521b\u5efa\u4ee3\u7406\u5de5\u4f5c\u6d41</li> <li>\u4ee3\u7406\u7ba1\u7406\u5668\uff1a\u5904\u7406\u4ee3\u7406\u7684\u521b\u5efa\u3001\u914d\u7f6e\u548c\u90e8\u7f72</li> <li>\u5de5\u4f5c\u6d41\u6267\u884c\u5668\uff1a\u9ad8\u6548\u8fd0\u884c\u5de5\u4f5c\u6d41\uff0c\u786e\u4fdd\u4ee3\u7406\u95f4\u6b63\u786e\u901a\u4fe1</li> <li>\u8bc4\u4f30\u5668\uff1a\u63d0\u4f9b\u6027\u80fd\u6307\u6807\u548c\u6539\u8fdb\u5efa\u8bae</li> <li>\u4f18\u5316\u5668\uff1a\u901a\u8fc7\u4e0d\u65ad\u6f14\u8fdb\u63d0\u5347\u5de5\u4f5c\u6d41\u6027\u80fd</li> </ol>"},{"location":"zh/#-\u793e\u533a","title":"\ud83d\udc65 \u793e\u533a","text":"<ul> <li>Discord\uff1a\u52a0\u5165\u6211\u4eec\u7684 Discord \u9891\u9053 \u8fdb\u884c\u8ba8\u8bba\u548c\u83b7\u53d6\u652f\u6301</li> <li>GitHub\uff1a\u5728 GitHub \u4e0a\u4e3a\u9879\u76ee\u505a\u51fa\u8d21\u732e</li> <li>Email\uff1a\u901a\u8fc7 evoagentx.ai@gmail.com \u8054\u7cfb\u6211\u4eec</li> <li>\u5fae\u4fe1\uff1a\u901a\u8fc7 \u5fae\u4fe1 \u83b7\u53d6\u66f4\u65b0\u548c\u652f\u6301\u3002</li> </ul>"},{"location":"zh/#-\u8d21\u732e","title":"\ud83e\udd1d \u8d21\u732e","text":"<p>\u6211\u4eec\u6b22\u8fce\u793e\u533a\u8d21\u732e\uff01\u8bf7\u53c2\u9605\u6211\u4eec\u7684\u8d21\u732e\u6307\u5357\u4e86\u89e3\u66f4\u591a\u8be6\u60c5\u3002</p>"},{"location":"zh/installation/","title":"EvoAgentX \u5b89\u88c5\u6307\u5357","text":"<p>\u672c\u6307\u5357\u5c06\u5f15\u5bfc\u60a8\u5b8c\u6210\u5728\u7cfb\u7edf\u4e0a\u5b89\u88c5 EvoAgentX\u3001\u8bbe\u7f6e\u6240\u9700\u4f9d\u8d56\u9879\u4ee5\u53ca\u4e3a\u60a8\u7684\u9879\u76ee\u914d\u7f6e\u6846\u67b6\u7684\u5168\u8fc7\u7a0b\u3002</p>"},{"location":"zh/installation/#\u524d\u7f6e\u6761\u4ef6","title":"\u524d\u7f6e\u6761\u4ef6","text":"<p>\u5728\u5b89\u88c5 EvoAgentX \u4e4b\u524d\uff0c\u8bf7\u786e\u4fdd\u60a8\u5177\u5907\u4ee5\u4e0b\u524d\u7f6e\u6761\u4ef6\uff1a</p> <ul> <li>Python 3.10 \u6216\u66f4\u9ad8\u7248\u672c  </li> <li>pip\uff08Python \u5305\u5b89\u88c5\u5668\uff09  </li> <li>Git\uff08\u7528\u4e8e\u514b\u9686\u4ed3\u5e93\uff09  </li> <li>Conda\uff08\u63a8\u8350\u7528\u4e8e\u73af\u5883\u7ba1\u7406\uff0c\u4f46\u53ef\u9009\uff09  </li> </ul>"},{"location":"zh/installation/#\u5b89\u88c5\u65b9\u6cd5","title":"\u5b89\u88c5\u65b9\u6cd5","text":"<p>\u5b89\u88c5 EvoAgentX \u6709\u591a\u79cd\u65b9\u5f0f\u3002\u8bf7\u9009\u62e9\u6700\u7b26\u5408\u60a8\u9700\u6c42\u7684\u65b9\u6cd5\u3002</p>"},{"location":"zh/installation/#\u65b9\u6cd5-1\u4f7f\u7528-pip\u63a8\u8350","title":"\u65b9\u6cd5 1\uff1a\u4f7f\u7528 pip\uff08\u63a8\u8350\uff09","text":"<p>\u5b89\u88c5 EvoAgentX \u6700\u7b80\u5355\u7684\u65b9\u6cd5\u662f\u4f7f\u7528 pip\uff1a</p> <pre><code>pip install git+https://github.com/EvoAgentX/EvoAgentX.git\n</code></pre>"},{"location":"zh/installation/#\u65b9\u6cd5-2\u4ece\u6e90\u4ee3\u7801\u5b89\u88c5\u5f00\u53d1\u8005\u4e13\u7528","title":"\u65b9\u6cd5 2\uff1a\u4ece\u6e90\u4ee3\u7801\u5b89\u88c5\uff08\u5f00\u53d1\u8005\u4e13\u7528\uff09","text":"<p>\u5982\u679c\u60a8\u5e0c\u671b\u4e3a EvoAgentX \u505a\u8d21\u732e\uff0c\u6216\u9700\u8981\u83b7\u53d6\u6700\u65b0\u7684\u5f00\u53d1\u7248\u672c\uff0c\u53ef\u4ee5\u76f4\u63a5\u4ece\u6e90\u4ee3\u7801\u5b89\u88c5\uff1a</p> <pre><code># \u514b\u9686\u4ed3\u5e93\ngit clone https://github.com/EvoAgentX/EvoAgentX/\n\n# \u8fdb\u5165\u9879\u76ee\u76ee\u5f55\ncd EvoAgentX\n\n# \u4ee5\u5f00\u53d1\u6a21\u5f0f\u5b89\u88c5\u5305\npip install -e .\n</code></pre>"},{"location":"zh/installation/#\u65b9\u6cd5-3\u4f7f\u7528-conda-\u73af\u5883\u63a8\u8350\u7528\u4e8e\u9694\u79bb","title":"\u65b9\u6cd5 3\uff1a\u4f7f\u7528 Conda \u73af\u5883\uff08\u63a8\u8350\u7528\u4e8e\u9694\u79bb\uff09","text":"<p>\u5982\u679c\u60a8\u504f\u597d\u4f7f\u7528 Conda \u6765\u7ba1\u7406 Python \u73af\u5883\uff0c\u8bf7\u6309\u4ee5\u4e0b\u6b65\u9aa4\u64cd\u4f5c\uff1a</p> <pre><code># \u521b\u5efa\u65b0\u7684conda\u73af\u5883\nconda create -n evoagentx python=3.10\n\n# \u6fc0\u6d3b\u73af\u5883\nconda activate evoagentx\n\n# \u5b89\u88c5\u5305\npip install -r requirements.txt\n# \u6216\u8005\u4ee5\u5f00\u53d1\u6a21\u5f0f\u5b89\u88c5\npip install -e .\n</code></pre>"},{"location":"zh/installation/#\u9a8c\u8bc1\u5b89\u88c5","title":"\u9a8c\u8bc1\u5b89\u88c5","text":"<p>\u8981\u9a8c\u8bc1 EvoAgentX \u662f\u5426\u5df2\u6b63\u786e\u5b89\u88c5\uff0c\u8bf7\u8fd0\u884c\u4ee5\u4e0b Python \u4ee3\u7801\uff1a</p> <pre><code>import evoagentx\n\n# \u6253\u5370\u7248\u672c\nprint(evoagentx.__version__)\n</code></pre> <p>\u60a8\u5e94\u8be5\u80fd\u5728\u63a7\u5236\u53f0\u770b\u5230 EvoAgentX \u5f53\u524d\u7684\u7248\u672c\u53f7\u3002</p>"},{"location":"zh/quickstart/","title":"EvoAgentX \u5feb\u901f\u5f00\u59cb\u6307\u5357","text":"<p>\u672c\u5feb\u901f\u5f00\u59cb\u6307\u5357\u5c06\u5f15\u5bfc\u4f60\u5b8c\u6210\u4f7f\u7528 EvoAgentX \u7684\u57fa\u7840\u6b65\u9aa4\u3002\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u4f60\u5c06\u5b66\u4e60\u5982\u4f55\uff1a 1. \u914d\u7f6e\u7528\u4e8e\u8bbf\u95ee LLM \u7684 API \u5bc6\u94a5 2. \u81ea\u52a8\u521b\u5efa\u5e76\u6267\u884c\u5de5\u4f5c\u6d41  </p>"},{"location":"zh/quickstart/#\u5b89\u88c5","title":"\u5b89\u88c5","text":"<p><pre><code>pip install evoagentx \n</code></pre> \u8bf7\u53c2\u9605 \u5b89\u88c5\u6307\u5357 \u83b7\u53d6\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\u3002</p>"},{"location":"zh/quickstart/#api-\u5bc6\u94a5-\u548c-llm-\u8bbe\u7f6e","title":"API \u5bc6\u94a5 \u548c LLM \u8bbe\u7f6e","text":"<p>\u8981\u5728 EvoAgentX \u4e2d\u6267\u884c\u5de5\u4f5c\u6d41\uff0c\u9996\u5148\u9700\u8981\u914d\u7f6e\u7528\u4e8e\u8bbf\u95ee\u5927\u6a21\u578b\uff08LLM\uff09\u7684 API \u5bc6\u94a5\u3002\u63a8\u8350\u4ee5\u4e0b\u4e24\u79cd\u65b9\u5f0f\uff1a</p>"},{"location":"zh/quickstart/#\u65b9\u6cd5\u4e00\u5728\u7ec8\u7aef\u8bbe\u7f6e\u73af\u5883\u53d8\u91cf","title":"\u65b9\u6cd5\u4e00\uff1a\u5728\u7ec8\u7aef\u8bbe\u7f6e\u73af\u5883\u53d8\u91cf","text":"<p>\u6b64\u65b9\u6cd5\u76f4\u63a5\u5728\u7cfb\u7edf\u73af\u5883\u4e2d\u8bbe\u7f6e API \u5bc6\u94a5\u3002</p> <p>\u5bf9\u4e8e Linux/macOS\uff1a <pre><code>export OPENAI_API_KEY=&lt;\u4f60\u7684-openai-api-key&gt;\n</code></pre></p> <p>\u5bf9\u4e8e Windows \u547d\u4ee4\u63d0\u793a\u7b26\uff1a <pre><code>set OPENAI_API_KEY=&lt;\u4f60\u7684-openai-api-key&gt;\n</code></pre></p> <p>\u5bf9\u4e8e Windows PowerShell\uff1a <pre><code>$env:OPENAI_API_KEY=\"&lt;\u4f60\u7684-openai-api-key&gt;\"  # \u5f15\u53f7\u662f\u5fc5\u9700\u7684\n</code></pre></p> <p>\u8bbe\u7f6e\u5b8c\u6210\u540e\uff0c\u53ef\u5728 Python \u4e2d\u8fd9\u6837\u83b7\u53d6\uff1a <pre><code>import os\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n</code></pre></p>"},{"location":"zh/quickstart/#\u65b9\u6cd5\u4e8c\u4f7f\u7528-env-\u6587\u4ef6","title":"\u65b9\u6cd5\u4e8c\uff1a\u4f7f\u7528 <code>.env</code> \u6587\u4ef6","text":"<p>\u4e5f\u53ef\u4ee5\u5728\u9879\u76ee\u6839\u76ee\u5f55\u4e0b\u521b\u5efa <code>.env</code> \u6587\u4ef6\u6765\u5b58\u50a8 API \u5bc6\u94a5\u3002</p> <p>\u6587\u4ef6\u5185\u5bb9\u793a\u4f8b\uff1a <pre><code>OPENAI_API_KEY=&lt;\u4f60\u7684-openai-api-key&gt;\n</code></pre></p> <p>\u7136\u540e\u5728 Python \u4e2d\u4f7f\u7528 <code>python-dotenv</code> \u52a0\u8f7d\uff1a <pre><code>from dotenv import load_dotenv \nimport os \n\nload_dotenv()  # \u4ece .env \u6587\u4ef6\u52a0\u8f7d\u73af\u5883\u53d8\u91cf\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n</code></pre></p> <p>\ud83d\udd10 \u63d0\u793a\uff1a\u5207\u52ff\u5c06 <code>.env</code> \u6587\u4ef6\u63d0\u4ea4\u5230\u516c\u5171\u5e73\u53f0\uff08\u5982 GitHub\uff09\uff0c\u8bf7\u5c06\u5176\u6dfb\u52a0\u5230 <code>.gitignore</code>\u3002</p>"},{"location":"zh/quickstart/#\u5728-evoagentx-\u4e2d\u914d\u7f6e\u5e76\u4f7f\u7528-llm","title":"\u5728 EvoAgentX \u4e2d\u914d\u7f6e\u5e76\u4f7f\u7528 LLM","text":"<p>\u914d\u7f6e\u597d API \u5bc6\u94a5\u540e\uff0c\u53ef\u6309\u5982\u4e0b\u65b9\u5f0f\u521d\u59cb\u5316\u5e76\u4f7f\u7528 LLM\uff1a <pre><code>from evoagentx.models import OpenAILLMConfig, OpenAILLM\n\n# \u4ece\u73af\u5883\u52a0\u8f7d API \u5bc6\u94a5\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\n# \u5b9a\u4e49 LLM \u914d\u7f6e\nopenai_config = OpenAILLMConfig(\n    model=\"gpt-4o-mini\",       # \u6307\u5b9a\u6a21\u578b\u540d\u79f0\n    openai_key=OPENAI_API_KEY, # \u76f4\u63a5\u4f20\u5165\u5bc6\u94a5\n    stream=True,               # \u542f\u7528\u6d41\u5f0f\u54cd\u5e94\n    output_response=True       # \u6253\u5370\u54cd\u5e94\u5230\u6807\u51c6\u8f93\u51fa\n)\n\n# \u521d\u59cb\u5316\u8bed\u8a00\u6a21\u578b\nllm = OpenAILLM(config=openai_config)\n\n# \u4ece LLM \u751f\u6210\u54cd\u5e94\nresponse = llm.generate(prompt=\"What is Agentic Workflow?\")\n</code></pre></p> <p>\u4f60\u53ef\u4ee5\u5728 LLM \u6a21\u5757\u6307\u5357 \u4e2d\u627e\u5230\u66f4\u591a\u5173\u4e8e\u652f\u6301\u7684 LLM \u7c7b\u578b\u53ca\u5176\u53c2\u6570\u7684\u8be6\u7ec6\u4fe1\u606f\u3002</p>"},{"location":"zh/quickstart/#\u81ea\u52a8\u5de5\u4f5c\u6d41\u751f\u6210\u4e0e\u6267\u884c","title":"\u81ea\u52a8\u5de5\u4f5c\u6d41\u751f\u6210\u4e0e\u6267\u884c","text":"<p>\u914d\u7f6e\u5b8c\u6210\u540e\uff0c\u5373\u53ef\u5728 EvoAgentX \u4e2d\u81ea\u52a8\u751f\u6210\u5e76\u6267\u884c\u667a\u80fd\u5de5\u4f5c\u6d41\u3002\u672c\u8282\u5c55\u793a\u751f\u6210\u5de5\u4f5c\u6d41\u3001\u5b9e\u4f8b\u5316\u4ee3\u7406\u5e76\u8fd0\u884c\u7684\u6838\u5fc3\u6b65\u9aa4\u3002</p> <p>\u9996\u5148\uff0c\u5bfc\u5165\u5fc5\u8981\u7684\u6a21\u5757\uff1a</p> <pre><code>from evoagentx.workflow import WorkFlowGenerator, WorkFlowGraph, WorkFlow\nfrom evoagentx.agents import AgentManager\n</code></pre>"},{"location":"zh/quickstart/#\u7b2c\u4e00\u6b65\u751f\u6210\u5de5\u4f5c\u6d41\u4e0e\u4efb\u52a1\u56fe","title":"\u7b2c\u4e00\u6b65\uff1a\u751f\u6210\u5de5\u4f5c\u6d41\u4e0e\u4efb\u52a1\u56fe","text":"<p>\u4f7f\u7528 <code>WorkFlowGenerator</code> \u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u76ee\u6807\u81ea\u52a8\u521b\u5efa\u5de5\u4f5c\u6d41\uff1a <pre><code>goal = \"Generate html code for the Tetris game that can be played in the browser.\"\nwf_generator = WorkFlowGenerator(llm=llm)\nworkflow_graph: WorkFlowGraph = wf_generator.generate_workflow(goal=goal)\n</code></pre> <code>WorkFlowGraph</code> \u662f\u4e00\u4e2a\u6570\u636e\u7ed3\u6784\uff0c\u7528\u4e8e\u5b58\u50a8\u6574\u4f53\u5de5\u4f5c\u6d41\u8ba1\u5212\uff0c\u5305\u62ec\u4efb\u52a1\u8282\u70b9\u53ca\u5176\u5173\u7cfb\uff0c\u4f46\u5c1a\u672a\u5305\u542b\u53ef\u6267\u884c\u7684\u4ee3\u7406\u3002</p> <p>\u53ef\u9009\uff1a\u53ef\u89c6\u5316\u6216\u4fdd\u5b58\u751f\u6210\u7684\u5de5\u4f5c\u6d41\uff1a <pre><code># \u53ef\u89c6\u5316\u5de5\u4f5c\u6d41\u7ed3\u6784\uff08\u53ef\u9009\uff09\nworkflow_graph.display()\n\n# \u5c06\u5de5\u4f5c\u6d41\u4fdd\u5b58\u4e3a JSON \u6587\u4ef6\uff08\u53ef\u9009\uff09\nworkflow_graph.save_module(\"/path/to/save/workflow_demo.json\")\n</code></pre> \u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e2a\u751f\u6210\u7684\u5de5\u4f5c\u6d41\u793a\u4f8b here\u3002\u4f60\u53ef\u4ee5\u91cd\u65b0\u52a0\u8f7d\u4fdd\u5b58\u7684\u5de5\u4f5c\u6d41\uff1a <pre><code>workflow_graph = WorkFlowGraph.from_file(\"/path/to/save/workflow_demo.json\")\n</code></pre></p>"},{"location":"zh/quickstart/#\u7b2c\u4e8c\u6b65\u521b\u5efa\u5e76\u7ba1\u7406\u6267\u884c\u4ee3\u7406","title":"\u7b2c\u4e8c\u6b65\uff1a\u521b\u5efa\u5e76\u7ba1\u7406\u6267\u884c\u4ee3\u7406","text":"<p>\u4f7f\u7528 <code>AgentManager</code> \u57fa\u4e8e\u5de5\u4f5c\u6d41\u56fe\u5b9e\u4f8b\u5316\u5e76\u7ba1\u7406\u4ee3\u7406\uff1a <pre><code>agent_manager = AgentManager()\nagent_manager.add_agents_from_workflow(workflow_graph, llm_config=openai_config)\n</code></pre></p>"},{"location":"zh/quickstart/#\u7b2c\u4e09\u6b65\u6267\u884c\u5de5\u4f5c\u6d41","title":"\u7b2c\u4e09\u6b65\uff1a\u6267\u884c\u5de5\u4f5c\u6d41","text":"<p>\u4ee3\u7406\u51c6\u5907\u5c31\u7eea\u540e\uff0c\u53ef\u4ee5\u521b\u5efa <code>WorkFlow</code> \u5b9e\u4f8b\u5e76\u8fd0\u884c\uff1a <pre><code>workflow = WorkFlow(graph=workflow_graph, agent_manager=agent_manager, llm=llm)\noutput = workflow.execute()\nprint(output)\n</code></pre></p> <p>\u66f4\u591a\u793a\u4f8b\u8bf7\u53c2\u89c1 \u5b8c\u6574\u5de5\u4f5c\u6d41\u6f14\u793a\u3002</p>"},{"location":"zh/api/actions/","title":"\ud83c\udfaf \u52a8\u4f5c\u63a5\u53e3","text":""},{"location":"zh/api/agents/","title":"\ud83e\udd16 Agent \u63a5\u53e3","text":""},{"location":"zh/api/benchmark/","title":"\ud83e\uddea \u57fa\u51c6\u6d4b\u8bd5\u63a5\u53e3","text":""},{"location":"zh/api/core/","title":"\ud83e\udde0 \u6838\u5fc3\u6a21\u5757","text":""},{"location":"zh/api/evaluators/","title":"\ud83e\uddd1\u200d\u2696\ufe0f \u8bc4\u4f30\u5668\u63a5\u53e3","text":""},{"location":"zh/api/memory/","title":"\ud83d\udd87\ufe0f \u5de5\u5177\u96c6\u63a5\u53e3","text":""},{"location":"zh/api/models/","title":"\ud83e\uddec \u6a21\u578b\u63a5\u53e3","text":""},{"location":"zh/api/optimizers/","title":"\ud83e\uddee \u4f18\u5316\u5668\u63a5\u53e3","text":""},{"location":"zh/api/storages/","title":"\ud83d\udcbe \u5b58\u50a8\u6a21\u5757","text":""},{"location":"zh/api/tools/","title":"\ud83d\udee0\ufe0f \u5de5\u5177\u96c6\u63a5\u53e3","text":""},{"location":"zh/api/workflow/","title":"\ud83d\udd01 \u5de5\u4f5c\u6d41\u63a5\u53e3","text":""},{"location":"zh/modules/action_graph/","title":"\u52a8\u4f5c\u56fe","text":""},{"location":"zh/modules/action_graph/#\u7b80\u4ecb","title":"\u7b80\u4ecb","text":"<p><code>ActionGraph</code> \u7c7b\u662f EvoAgentX \u6846\u67b6\u4e2d\u7684\u4e00\u4e2a\u57fa\u7840\u7ec4\u4ef6\uff0c\u7528\u4e8e\u5728\u5355\u4e2a\u4efb\u52a1\u4e2d\u521b\u5efa\u548c\u6267\u884c\u64cd\u4f5c\uff08\u52a8\u4f5c\uff09\u5e8f\u5217\u3002\u5b83\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ed3\u6784\u5316\u7684\u65b9\u5f0f\u6765\u5b9a\u4e49\u3001\u7ba1\u7406\u548c\u6267\u884c\u4e00\u7cfb\u5217\u9700\u8981\u6309\u7279\u5b9a\u987a\u5e8f\u6267\u884c\u7684\u64cd\u4f5c\uff0c\u4ee5\u5b8c\u6210\u4efb\u52a1\u3002</p> <p>\u52a8\u4f5c\u56fe\u8868\u793a\u4e00\u7ec4\u6309\u9884\u5b9a\u4e49\u987a\u5e8f\u6267\u884c\u7684\u8fd0\u7b97\u7b26\uff08\u52a8\u4f5c\uff09\uff0c\u7528\u4e8e\u5904\u7406\u8f93\u5165\u5e76\u4ea7\u751f\u8f93\u51fa\u3002\u4e0e\u5728\u66f4\u9ad8\u5c42\u6b21\u7ba1\u7406\u591a\u4e2a\u4efb\u52a1\u53ca\u5176\u4f9d\u8d56\u5173\u7cfb\u7684 <code>WorkFlowGraph</code> \u4e0d\u540c\uff0c<code>ActionGraph</code> \u4e13\u6ce8\u4e8e\u5355\u4e2a\u4efb\u52a1\u5185\u7684\u8be6\u7ec6\u6267\u884c\u6b65\u9aa4\u3002</p>"},{"location":"zh/modules/action_graph/#\u67b6\u6784","title":"\u67b6\u6784","text":""},{"location":"zh/modules/action_graph/#\u52a8\u4f5c\u56fe\u67b6\u6784","title":"\u52a8\u4f5c\u56fe\u67b6\u6784","text":"<p><code>ActionGraph</code> \u7531\u51e0\u4e2a\u5173\u952e\u7ec4\u4ef6\u7ec4\u6210\uff1a</p> <ol> <li> <p>\u8fd0\u7b97\u7b26\uff1a</p> <p>\u6bcf\u4e2a\u8fd0\u7b97\u7b26\u4ee3\u8868\u53ef\u4ee5\u4f5c\u4e3a\u4efb\u52a1\u4e00\u90e8\u5206\u6267\u884c\u7684\u7279\u5b9a\u64cd\u4f5c\u6216\u52a8\u4f5c\uff0c\u5177\u6709\u4ee5\u4e0b\u5c5e\u6027\uff1a</p> <ul> <li><code>name</code>\uff1a\u8fd0\u7b97\u7b26\u7684\u552f\u4e00\u6807\u8bc6\u7b26</li> <li><code>description</code>\uff1a\u8fd0\u7b97\u7b26\u529f\u80fd\u7684\u8be6\u7ec6\u63cf\u8ff0</li> <li><code>llm</code>\uff1a\u7528\u4e8e\u6267\u884c\u8fd0\u7b97\u7b26\u7684 LLM</li> <li><code>outputs_format</code>\uff1a\u8fd0\u7b97\u7b26\u8f93\u51fa\u7684\u7ed3\u6784\u5316\u683c\u5f0f</li> <li><code>interface</code>\uff1a\u8c03\u7528\u8fd0\u7b97\u7b26\u7684\u63a5\u53e3</li> <li><code>prompt</code>\uff1a\u6267\u884c\u6b64\u8fd0\u7b97\u7b26\u65f6\u7528\u4e8e\u6307\u5bfc LLM \u7684\u6a21\u677f</li> </ul> </li> <li> <p>LLM\uff1a</p> <p>ActionGraph \u4f7f\u7528\u8bed\u8a00\u5b66\u4e60\u6a21\u578b\uff08LLM\uff09\u6765\u6267\u884c\u8fd0\u7b97\u7b26\u3002\u5b83\u63a5\u6536 <code>llm_config</code> \u4f5c\u4e3a\u8f93\u5165\u5e76\u521b\u5efa LLM \u5b9e\u4f8b\uff0c\u8be5\u5b9e\u4f8b\u5c06\u88ab\u4f20\u9012\u7ed9\u8fd0\u7b97\u7b26\u6267\u884c\u3002LLM \u63d0\u4f9b\u4e86\u6267\u884c\u6bcf\u4e2a\u52a8\u4f5c\u6240\u9700\u7684\u63a8\u7406\u548c\u751f\u6210\u80fd\u529b\u3002</p> </li> <li> <p>\u6267\u884c\u6d41\u7a0b\uff1a</p> <p>ActionGraph \u5b9a\u4e49\u4e86\u7279\u5b9a\u7684\u6267\u884c\u987a\u5e8f\uff1a</p> <ul> <li>\u52a8\u4f5c\u6309\u9884\u5b9a\u987a\u5e8f\u6267\u884c\uff08\u5728 <code>execute</code> \u6216 <code>async_execute</code> \u65b9\u6cd5\u4e2d\u4f7f\u7528\u4ee3\u7801\u6307\u5b9a\uff09</li> <li>\u6bcf\u4e2a\u52a8\u4f5c\u53ef\u4ee5\u4f7f\u7528\u4e4b\u524d\u52a8\u4f5c\u7684\u7ed3\u679c</li> <li>\u5728\u6240\u6709\u52a8\u4f5c\u6267\u884c\u5b8c\u6210\u540e\u4ea7\u751f\u6700\u7ec8\u8f93\u51fa</li> </ul> </li> </ol>"},{"location":"zh/modules/action_graph/#\u4e0e\u5de5\u4f5c\u6d41\u56fe\u7684\u6bd4\u8f83","title":"\u4e0e\u5de5\u4f5c\u6d41\u56fe\u7684\u6bd4\u8f83","text":"<p>\u867d\u7136 <code>ActionGraph</code> \u548c <code>WorkFlowGraph</code> \u90fd\u7ba1\u7406\u6267\u884c\u6d41\u7a0b\uff0c\u4f46\u5b83\u4eec\u5728\u62bd\u8c61\u5c42\u6b21\u4e0a\u6709\u6240\u4e0d\u540c\uff1a</p> \u7279\u6027 \u52a8\u4f5c\u56fe \u5de5\u4f5c\u6d41\u56fe \u8303\u56f4 \u5355\u4e2a\u4efb\u52a1\u6267\u884c \u591a\u4efb\u52a1\u5de5\u4f5c\u6d41\u7f16\u6392 \u7ec4\u4ef6 \u8fd0\u7b97\u7b26\uff08\u52a8\u4f5c\uff09 \u8282\u70b9\uff08\u4efb\u52a1\uff09\u548c\u8fb9\uff08\u4f9d\u8d56\u5173\u7cfb\uff09 \u91cd\u70b9 \u4efb\u52a1\u5185\u7684\u8be6\u7ec6\u6b65\u9aa4 \u4e0d\u540c\u4efb\u52a1\u4e4b\u95f4\u7684\u5173\u7cfb \u7075\u6d3b\u6027 \u56fa\u5b9a\u6267\u884c\u987a\u5e8f \u57fa\u4e8e\u4f9d\u8d56\u5173\u7cfb\u7684\u52a8\u6001\u6267\u884c \u4e3b\u8981\u7528\u9014 \u5b9a\u4e49\u53ef\u91cd\u7528\u7684\u4efb\u52a1\u6267\u884c\u6a21\u5f0f \u7f16\u6392\u590d\u6742\u7684\u591a\u6b65\u9aa4\u5de5\u4f5c\u6d41 \u7c92\u5ea6 \u7ec6\u7c92\u5ea6\u64cd\u4f5c \u7c97\u7c92\u5ea6\u4efb\u52a1"},{"location":"zh/modules/action_graph/#\u4f7f\u7528\u65b9\u6cd5","title":"\u4f7f\u7528\u65b9\u6cd5","text":""},{"location":"zh/modules/action_graph/#\u57fa\u672c\u52a8\u4f5c\u56fe\u521b\u5efa","title":"\u57fa\u672c\u52a8\u4f5c\u56fe\u521b\u5efa","text":"<pre><code>from evoagentx.workflow import ActionGraph\nfrom evoagentx.workflow.operators import Custom\nfrom evoagentx.models import OpenAILLMConfig \n\n# \u521b\u5efa LLM \u914d\u7f6e\nllm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=\"xxx\")\n\n# \u521b\u5efa\u81ea\u5b9a\u4e49 ActionGraph\nclass MyActionGraph(ActionGraph):\n    def __init__(self, llm_config, **kwargs):\n\n        name = kwargs.pop(\"name\") if \"name\" in kwargs else \"Custom Action Graph\"\n        description = kwargs.pop(\"description\") if \"description\" in kwargs else \"A custom action graph for text processing\"\n        # \u57fa\u4e8e `llm_config` \u521b\u5efa LLM \u5b9e\u4f8b `self._llm` \u5e76\u5c06\u5176\u4f20\u9012\u7ed9\u8fd0\u7b97\u7b26\n        super().__init__(name=name, description=description, llm_config=llm_config, **kwargs)\n        # \u5b9a\u4e49\u8fd0\u7b97\u7b26\n        self.extract_entities = Custom(self._llm) # , prompt=\"Extract key entities from the following text: {input}\")\n        self.analyze_sentiment = Custom(self._llm) # , prompt=\"Analyze the sentiment of the following text: {input}\")\n        self.summarize = Custom(self._llm) # , prompt=\"Summarize the following text in one paragraph: {input}\")\n\n    def execute(self, text: str) -&gt; dict:\n        # \u6309\u987a\u5e8f\u6267\u884c\u8fd0\u7b97\u7b26\uff08\u6307\u5b9a\u8fd0\u7b97\u7b26\u7684\u6267\u884c\u987a\u5e8f\uff09\n        entities = self.extract_entities(input=text, instruction=\"Extract key entities from the provided text\")[\"response\"]\n        sentiment = self.analyze_sentiment(input=text, instruction=\"Analyze the sentiment of the provided text\")[\"response\"]\n        summary = self.summarize(input=text, instruction=\"Summarize the provided text in one paragraph\")[\"response\"]\n\n        # \u8fd4\u56de\u7ec4\u5408\u7ed3\u679c\n        return {\n            \"entities\": entities,\n            \"sentiment\": sentiment,\n            \"summary\": summary\n        }\n\n# \u521b\u5efa\u52a8\u4f5c\u56fe\naction_graph = MyActionGraph(llm_config=llm_config)\n\n# \u6267\u884c\u52a8\u4f5c\u56fe\nresult = action_graph.execute(text=\"This is a test text\")\nprint(result)\n</code></pre>"},{"location":"zh/modules/action_graph/#\u5728\u5de5\u4f5c\u6d41\u56fe\u4e2d\u4f7f\u7528\u52a8\u4f5c\u56fe","title":"\u5728\u5de5\u4f5c\u6d41\u56fe\u4e2d\u4f7f\u7528\u52a8\u4f5c\u56fe","text":"<p>\u60a8\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 <code>ActionGraph</code> \u6216\u5728 <code>WorkFlowGraph</code> \u4e2d\u5c06\u5176\u4f5c\u4e3a\u8282\u70b9\u4f7f\u7528\u3002</p> <pre><code>from evoagentx.workflow.workflow_graph import WorkFlowNode, WorkFlowGraph\nfrom evoagentx.workflow.action_graph import QAActionGraph\nfrom evoagentx.core.base_config import Parameter\nfrom evoagentx.models import OpenAILLMConfig, OpenAILLM\nfrom evoagentx.workflow import WorkFlow\n\n# \u521b\u5efa LLM \u914d\u7f6e\nllm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=\"xxx\", stream=True, output_response=True)\nllm = OpenAILLM(llm_config)\n\n# \u521b\u5efa\u52a8\u4f5c\u56fe\nqa_graph = QAActionGraph(llm_config=llm_config)\n\n# \u521b\u5efa\u4f7f\u7528\u52a8\u4f5c\u56fe\u7684\u5de5\u4f5c\u6d41\u8282\u70b9\nqa_node = WorkFlowNode(\n    name=\"QATask\",\n    description=\"Answer questions using a QA system\",\n    # \u8f93\u5165\u540d\u79f0\u5e94\u4e0e\u52a8\u4f5c\u56fe\u7684 `execute` \u65b9\u6cd5\u4e2d\u7684\u53c2\u6570\u5339\u914d\n    inputs=[Parameter(name=\"problem\", type=\"string\", description=\"The problem to answer\")],\n    outputs=[Parameter(name=\"answer\", type=\"string\", description=\"The answer to the problem\")],\n    action_graph=qa_graph  # \u4f7f\u7528 action_graph \u800c\u4e0d\u662f agents\n)\n\n# \u521b\u5efa\u5de5\u4f5c\u6d41\u56fe\nworkflow_graph = WorkFlowGraph(goal=\"Answer a question\", nodes=[qa_node])\n\n# \u5b9a\u4e49\u5de5\u4f5c\u6d41\nworkflow = WorkFlow(graph=workflow_graph, llm=llm)\n\n# \u6267\u884c\u5de5\u4f5c\u6d41\nresult = workflow.execute(inputs={\"problem\": \"What is the capital of France?\"})\nprint(result)\n</code></pre> <p>Warning</p> <p>\u5728 <code>WorkFlowNode</code> \u4e2d\u4f7f\u7528 <code>ActionGraph</code> \u65f6\uff0c<code>WorkFlowNode</code> \u7684 <code>inputs</code> \u53c2\u6570\u5e94\u4e0e <code>ActionGraph</code> \u7684 <code>execute</code> \u65b9\u6cd5\u4e2d\u6240\u9700\u7684\u53c2\u6570\u5339\u914d\u3002<code>execute</code> \u65b9\u6cd5\u5e94\u8fd4\u56de\u4e00\u4e2a\u5b57\u5178\u6216 <code>LLMOutputParser</code> \u5b9e\u4f8b\uff0c\u5176\u952e\u4e0e <code>WorkFlowNode</code> \u4e2d <code>outputs</code> \u7684\u540d\u79f0\u5339\u914d\u3002</p>"},{"location":"zh/modules/action_graph/#\u4fdd\u5b58\u548c\u52a0\u8f7d\u52a8\u4f5c\u56fe","title":"\u4fdd\u5b58\u548c\u52a0\u8f7d\u52a8\u4f5c\u56fe","text":"<pre><code># \u4fdd\u5b58\u52a8\u4f5c\u56fe\naction_graph.save_module(\"examples/output/my_action_graph.json\")\n\n# \u52a0\u8f7d\u52a8\u4f5c\u56fe\nfrom evoagentx.workflow.action_graph import ActionGraph\nloaded_graph = ActionGraph.from_file(\"examples/output/my_action_graph.json\", llm_config=llm_config)\n</code></pre> <p><code>ActionGraph</code> \u7c7b\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u5927\u7684\u65b9\u5f0f\u6765\u5b9a\u4e49\u5355\u4e2a\u4efb\u52a1\u5185\u7684\u590d\u6742\u64cd\u4f5c\u5e8f\u5217\uff0c\u8865\u5145\u4e86 EvoAgentX \u6846\u67b6\u4e2d <code>WorkFlowGraph</code> \u7684\u9ad8\u7ea7\u7f16\u6392\u529f\u80fd\u3002</p>"},{"location":"zh/modules/agent/#\u7b80\u4ecb","title":"\u7b80\u4ecb","text":"<p><code>Agent</code> \u7c7b\u662f EvoAgentX \u6846\u67b6\u4e2d\u521b\u5efa\u667a\u80fd AI \u4ee3\u7406\u7684\u57fa\u7840\u6784\u5efa\u5757\u3002\u5b83\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ed3\u6784\u5316\u7684\u65b9\u5f0f\u6765\u7ec4\u5408\u8bed\u8a00\u6a21\u578b\u3001\u52a8\u4f5c\u548c\u5185\u5b58\u7ba1\u7406\u3002</p>"},{"location":"zh/modules/agent/#\u67b6\u6784","title":"\u67b6\u6784","text":"<p>Agent \u7531\u51e0\u4e2a\u5173\u952e\u7ec4\u4ef6\u7ec4\u6210\uff1a</p> <ol> <li> <p>\u5927\u8bed\u8a00\u6a21\u578b (LLM)\uff1a</p> <p>LLM \u901a\u8fc7 <code>llm</code> \u6216 <code>llm_config</code> \u53c2\u6570\u6307\u5b9a\uff0c\u4f5c\u4e3a\u4ee3\u7406\u7684\u57fa\u7840\u6784\u5efa\u5757\u3002\u5b83\u8d1f\u8d23\u89e3\u91ca\u4e0a\u4e0b\u6587\u3001\u751f\u6210\u54cd\u5e94\u548c\u505a\u51fa\u9ad8\u7ea7\u51b3\u7b56\u3002LLM \u5c06\u88ab\u4f20\u9012\u7ed9\u52a8\u4f5c\u4ee5\u6267\u884c\u7279\u5b9a\u4efb\u52a1\u3002</p> </li> <li> <p>\u52a8\u4f5c (Actions)\uff1a</p> <p>\u52a8\u4f5c\u662f\u4ee3\u7406\u7684\u57fa\u672c\u64cd\u4f5c\u5355\u5143\u3002\u6bcf\u4e2a\u52a8\u4f5c\u5c01\u88c5\u4e86\u4e00\u4e2a\u7279\u5b9a\u4efb\u52a1\uff0c\u662f\u5b9e\u9645\u8c03\u7528 LLM \u8fdb\u884c\u63a8\u7406\u3001\u751f\u6210\u6216\u51b3\u7b56\u7684\u5730\u65b9\u3002\u867d\u7136\u4ee3\u7406\u63d0\u4f9b\u6574\u4f53\u7f16\u6392\uff0c\u4f46 LLM \u901a\u8fc7\u52a8\u4f5c\u6267\u884c\u5176\u6838\u5fc3\u529f\u80fd\u3002\u6bcf\u4e2a\u52a8\u4f5c\u90fd\u88ab\u8bbe\u8ba1\u4e3a\u53ea\u505a\u4e00\u4ef6\u4e8b\u2014\u2014\u6bd4\u5982\u68c0\u7d22\u77e5\u8bc6\u3001\u603b\u7ed3\u8f93\u5165\u6216\u8c03\u7528 API\u2014\u2014\u5e76\u4e14\u53ef\u4ee5\u5305\u542b\u4ee5\u4e0b\u7ec4\u4ef6\uff1a</p> <ul> <li>prompt\uff1a\u7528\u4e8e\u6307\u5bfc LLM \u6267\u884c\u6b64\u7279\u5b9a\u4efb\u52a1\u7684\u63d0\u793a\u6a21\u677f\u3002</li> <li>inputs_format\uff1a\u4f20\u5165\u52a8\u4f5c\u7684\u8f93\u5165\u7684\u9884\u671f\u7ed3\u6784\u548c\u952e\u3002</li> <li>outputs_format\uff1a\u7528\u4e8e\u89e3\u91ca\u548c\u89e3\u6790 LLM \u8f93\u51fa\u7684\u683c\u5f0f\u3002</li> <li>tools\uff1a\u53ef\u4ee5\u5728\u52a8\u4f5c\u4e2d\u96c6\u6210\u548c\u4f7f\u7528\u7684\u53ef\u9009\u5de5\u5177\u3002</li> </ul> </li> <li> <p>\u5185\u5b58\u7ec4\u4ef6\uff1a</p> <p>\u5185\u5b58\u5141\u8bb8\u4ee3\u7406\u5728\u4ea4\u4e92\u8fc7\u7a0b\u4e2d\u4fdd\u7559\u548c\u56de\u5fc6\u76f8\u5173\u4fe1\u606f\uff0c\u589e\u5f3a\u4e0a\u4e0b\u6587\u611f\u77e5\u80fd\u529b\u3002EvoAgentX \u6846\u67b6\u4e2d\u6709\u4e24\u79cd\u7c7b\u578b\u7684\u5185\u5b58\uff1a</p> <ul> <li>\u77ed\u671f\u5185\u5b58\uff1a\u7ef4\u62a4\u5f53\u524d\u4efb\u52a1\u7684\u4e2d\u95f4\u5bf9\u8bdd\u6216\u4e0a\u4e0b\u6587\u3002</li> <li>\u957f\u671f\u5185\u5b58\uff08\u53ef\u9009\uff09\uff1a\u5b58\u50a8\u53ef\u4ee5\u8de8\u8d8a\u4f1a\u8bdd\u6216\u4efb\u52a1\u7684\u6301\u4e45\u77e5\u8bc6\u3002\u8fd9\u4f7f\u4ee3\u7406\u80fd\u591f\u4ece\u8fc7\u53bb\u7684\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u3001\u7ef4\u62a4\u7528\u6237\u504f\u597d\u6216\u968f\u65f6\u95f4\u6784\u5efa\u77e5\u8bc6\u5e93\u3002</li> </ul> </li> </ol>"},{"location":"zh/modules/agent/#\u4f7f\u7528\u65b9\u6cd5","title":"\u4f7f\u7528\u65b9\u6cd5","text":""},{"location":"zh/modules/agent/#\u57fa\u672c-agent-\u521b\u5efa","title":"\u57fa\u672c Agent \u521b\u5efa","text":"<p>\u8981\u521b\u5efa\u4ee3\u7406\uff0c\u60a8\u9700\u8981\u5b9a\u4e49\u4ee3\u7406\u5c06\u6267\u884c\u7684\u52a8\u4f5c\u3002\u6bcf\u4e2a\u52a8\u4f5c\u90fd\u88ab\u5b9a\u4e49\u4e3a\u4e00\u4e2a\u7ee7\u627f\u81ea <code>Action</code> \u7c7b\u7684\u7c7b\u3002\u52a8\u4f5c\u7c7b\u5e94\u8be5\u5b9a\u4e49\u4ee5\u4e0b\u7ec4\u4ef6\uff1a<code>name</code>\u3001<code>description</code>\u3001<code>prompt</code>\u3001<code>inputs_format</code> \u548c <code>outputs_format</code>\uff0c\u5e76\u5b9e\u73b0 <code>execute</code> \u65b9\u6cd5\uff08\u5982\u679c\u60a8\u60f3\u5f02\u6b65\u4f7f\u7528\u4ee3\u7406\uff0c\u8fd8\u9700\u8981\u5b9e\u73b0 <code>async_exectue</code>\uff09\u3002</p> <pre><code>from evoagentx.agents import Agent\nfrom evoagentx.models import OpenAILLMConfig\nfrom evoagentx.actions import Action, ActionInput, ActionOutput\n\n# \u5b9a\u4e49\u4e00\u4e2a\u4f7f\u7528 LLM \u56de\u7b54\u95ee\u9898\u7684\u7b80\u5355\u52a8\u4f5c\n\nclass AnswerQuestionInput(ActionInput):\n    question: str\n\nclass AnswerQuestionOutput(ActionOutput):\n    answer: str\n\nclass AnswerQuestionAction(Action):\n\n    def __init__(\n        self, \n        name = \"answer_question\",\n        description = \"Answers a factual question using the LLM\",   \n        prompt = \"Answer the following question as accurately as possible:\\n\\n{question}\",\n        inputs_format = AnswerQuestionInput,\n        outputs_format = AnswerQuestionOutput,\n        **kwargs\n    ):\n        super().__init__(\n            name=name, \n            description=description, \n            prompt=prompt, \n            inputs_format=inputs_format, \n            outputs_format=outputs_format, \n            **kwargs\n        )\n\n    def execute(self, llm, inputs, sys_msg = None, return_prompt = False, **kwargs) -&gt; AnswerQuestionOutput:\n        question = inputs.get(\"question\")\n        prompt = self.prompt.format(question=question)\n        response = llm.generate(\n            prompt=prompt, \n            system_message=sys_msg,\n            parser=self.outputs_format, \n            parse_mode=\"str\"\n        )\n\n        if return_prompt:\n            return response, prompt\n        return response \n\n    async def async_execute(self, llm, inputs, sys_msg = None, return_prompt = False, **kwargs) -&gt; AnswerQuestionOutput:\n        question = inputs.get(\"question\")\n        prompt = self.prompt.format(question=question)\n        response = await llm.async_generate(\n            prompt=prompt, \n            system_message=sys_msg,\n            parser=self.outputs_format, \n            parse_mode=\"str\"\n        )   \n        if return_prompt:\n            return response, prompt\n        return response \n\n# \u914d\u7f6e LLM\nllm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=\"your-api-key\")\n\n# \u521b\u5efa\u4ee3\u7406\nagent = Agent(\n    name=\"AssistantAgent\",\n    description=\"Answers a factual question using the LLM\",\n    llm_config=llm_config,\n    system_prompt=\"You are a helpful assistant.\",\n    actions=[AnswerQuestionAction()]\n)\n</code></pre>"},{"location":"zh/modules/agent/#\u6267\u884c\u52a8\u4f5c","title":"\u6267\u884c\u52a8\u4f5c","text":"<p>\u60a8\u53ef\u4ee5\u76f4\u63a5\u50cf\u51fd\u6570\u4e00\u6837\u8c03\u7528 <code>Agent</code> \u5b9e\u4f8b\u3002\u8fd9\u5c06\u5185\u90e8\u4f7f\u7528\u6307\u5b9a\u7684 <code>action_name</code> \u548c <code>action_input_data</code> \u8c03\u7528\u5339\u914d\u52a8\u4f5c\u7684 <code>execute()</code> \u65b9\u6cd5\u3002</p> <pre><code># \u4f7f\u7528\u8f93\u5165\u6570\u636e\u6267\u884c\u52a8\u4f5c\nmessage = agent(\n    action_name=\"answer_question\",\n    action_input_data={\"question\": \"What is the capital of France?\"}\n)\n\n# \u8bbf\u95ee\u8f93\u51fa\nresult = message.content.answer \n</code></pre>"},{"location":"zh/modules/agent/#\u5f02\u6b65\u6267\u884c","title":"\u5f02\u6b65\u6267\u884c","text":"<p>\u60a8\u4e5f\u53ef\u4ee5\u5728\u5f02\u6b65\u4e0a\u4e0b\u6587\u4e2d\u8c03\u7528 <code>Agent</code> \u5b9e\u4f8b\u3002\u5982\u679c\u52a8\u4f5c\u5b9a\u4e49\u4e86 <code>async_execute</code> \u65b9\u6cd5\uff0c\u5f53\u60a8 <code>await</code> \u4ee3\u7406\u65f6\uff0c\u5b83\u5c06\u81ea\u52a8\u4f7f\u7528\u3002</p> <pre><code># \u5f02\u6b65\u6267\u884c\u52a8\u4f5c\nimport asyncio \n\nasync def main():\n    message = await agent(\n        action_name=\"answer_question\",\n        action_input_data={\"question\": \"What is the capital of France?\"}\n    )\n    return message.content.answer \n\nresult = asyncio.run(main())\nprint(result)\n</code></pre>"},{"location":"zh/modules/agent/#\u5185\u5b58\u7ba1\u7406","title":"\u5185\u5b58\u7ba1\u7406","text":"<p>\u4ee3\u7406\u7ef4\u62a4\u77ed\u671f\u5185\u5b58\u4ee5\u8ddf\u8e2a\u5bf9\u8bdd\u4e0a\u4e0b\u6587\uff1a</p> <pre><code># \u8bbf\u95ee\u4ee3\u7406\u7684\u5185\u5b58\nmessages = agent.short_term_memory.get(n=5)  # \u83b7\u53d6\u6700\u540e 5 \u6761\u6d88\u606f\n\n# \u6e05\u9664\u5185\u5b58\nagent.clear_short_term_memory()\n</code></pre>"},{"location":"zh/modules/agent/#\u4ee3\u7406\u914d\u7f6e\u6587\u4ef6","title":"\u4ee3\u7406\u914d\u7f6e\u6587\u4ef6","text":"<p>\u60a8\u53ef\u4ee5\u83b7\u53d6\u4ee3\u7406\u53ca\u5176\u529f\u80fd\u7684\u4eba\u7c7b\u53ef\u8bfb\u63cf\u8ff0\uff1a</p> <pre><code># \u83b7\u53d6\u6240\u6709\u52a8\u4f5c\u7684\u63cf\u8ff0\nprofile = agent.get_agent_profile()\nprint(profile)\n\n# \u83b7\u53d6\u7279\u5b9a\u52a8\u4f5c\u7684\u63cf\u8ff0\nprofile = agent.get_agent_profile(action_names=[\"answer_question\"])\nprint(profile)\n</code></pre>"},{"location":"zh/modules/agent/#\u63d0\u793a\u7ba1\u7406","title":"\u63d0\u793a\u7ba1\u7406","text":"<p>\u8bbf\u95ee\u548c\u4fee\u6539\u4ee3\u7406\u4f7f\u7528\u7684\u63d0\u793a\uff1a</p> <pre><code># \u83b7\u53d6\u6240\u6709\u63d0\u793a\nprompts = agent.get_prompts()\n# prompts \u662f\u4e00\u4e2a\u5b57\u5178\uff0c\u7ed3\u6784\u5982\u4e0b\uff1a\n# {'answer_question': {'system_prompt': 'You are a helpful assistant.', 'prompt': 'Answer the following question as accurately as possible:\\n\\n{question}'}}\n\n# \u8bbe\u7f6e\u7279\u5b9a\u63d0\u793a\nagent.set_prompt(\n    action_name=\"answer_question\",\n    prompt=\"Please provide a clear and concise answer to the following query:\\n\\n{question}\",\n    system_prompt=\"You are a helpful assistant.\" # \u53ef\u9009\uff0c\u5982\u679c\u672a\u63d0\u4f9b\uff0c\u7cfb\u7edf\u63d0\u793a\u5c06\u4fdd\u6301\u4e0d\u53d8\n)\n\n# \u66f4\u65b0\u6240\u6709\u63d0\u793a\nprompts_dict = {\n    \"answer_question\": {\n        \"system_prompt\": \"You are an expert in providing concise, accurate information.\",\n        \"prompt\": \"Please answer this question with precision and clarity:\\n\\n{question}\"\n    }\n}\nagent.set_prompts(prompts_dict)\n</code></pre>"},{"location":"zh/modules/agent/#\u4fdd\u5b58\u548c\u52a0\u8f7d\u4ee3\u7406","title":"\u4fdd\u5b58\u548c\u52a0\u8f7d\u4ee3\u7406","text":"<p>\u4ee3\u7406\u53ef\u4ee5\u88ab\u6301\u4e45\u5316\u548c\u91cd\u65b0\u52a0\u8f7d\uff1a</p> <pre><code># \u4fdd\u5b58\u4ee3\u7406\nagent.save_module(\"./agents/my_agent.json\")\n\n# \u52a0\u8f7d\u4ee3\u7406\uff08\u9700\u8981\u518d\u6b21\u63d0\u4f9b llm_config\uff09\nloaded_agent = Agent.from_file(\n    \"./agents/my_agent.json\", \n    llm_config=llm_config\n)\n</code></pre>"},{"location":"zh/modules/agent/#\u4e0a\u4e0b\u6587\u63d0\u53d6","title":"\u4e0a\u4e0b\u6587\u63d0\u53d6","text":"<p>\u4ee3\u7406\u5305\u542b\u5185\u7f6e\u7684\u4e0a\u4e0b\u6587\u63d0\u53d6\u673a\u5236\uff0c\u53ef\u4ee5\u81ea\u52a8\u4ece\u5bf9\u8bdd\u5386\u53f2\u4e2d\u6d3e\u751f\u52a8\u52a8\u4f5c\u7684\u9002\u5f53\u8f93\u5165\uff1a</p> <pre><code># \u5728\u6ca1\u6709\u663e\u5f0f\u8f93\u5165\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u6267\u884c\u65f6\uff0c\u4e0a\u4e0b\u6587\u4f1a\u81ea\u52a8\u63d0\u53d6\nresponse = agent.execute(\n    action_name=\"action_name\",\n    msgs=conversation_history\n)\n\n# \u624b\u52a8\u83b7\u53d6\u52a8\u4f5c\u8f93\u5165\naction = agent.get_action(\"action_name\")\ninputs = agent.get_action_inputs(action)\n</code></pre>"},{"location":"zh/modules/benchmark/","title":"\u57fa\u51c6\u6d4b\u8bd5","text":""},{"location":"zh/modules/benchmark/#\u57fa\u51c6\u6d4b\u8bd5\u6982\u8ff0","title":"\u57fa\u51c6\u6d4b\u8bd5\u6982\u8ff0","text":"<p>EvoAgentX \u63d0\u4f9b\u4e86\u4e00\u5957\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e0d\u540c\u7684\u57fa\u4e8e\u4ee3\u7406\u7684\u7cfb\u7edf\u3002\u4ee5\u4e0b\u662f\u5f53\u524d\u5305\u542b\u7684\u57fa\u51c6\u6d4b\u8bd5\u53ca\u5176\u57fa\u672c\u6570\u636e\u96c6\u7edf\u8ba1\u4fe1\u606f\uff1a</p> \u4efb\u52a1 \u6570\u636e\u96c6\u540d\u79f0 \u8bad\u7ec3\u96c6\u6570\u91cf \u9a8c\u8bc1\u96c6\u6570\u91cf \u6d4b\u8bd5\u96c6\u6570\u91cf \u95ee\u7b54 NQ 79,168 8,757 3,610 \u591a\u8df3\u95ee\u7b54 HotPotQA 90,447 7,405 / \u6570\u5b66 GSM8K 7,473 / 1,319 \u6570\u5b66 MATH 7,500 / 5,000 \u4ee3\u7801\u751f\u6210 HumanEval / / 164 \u4ee3\u7801\u751f\u6210 MBPP / / 427 \u4ee3\u7801\u751f\u6210 LiveCodeBench(v1~v5) / / 400~880 \u4ee3\u7801\u6267\u884c LiveCodeBench / / 479 \u6d4b\u8bd5\u8f93\u51fa\u9884\u6d4b LiveCodeBench / / 442 <p>\u6211\u4eec\u7684\u6846\u67b6\u63d0\u4f9b\u4e86\u81ea\u52a8\u6570\u636e\u96c6\u4e0b\u8f7d\u529f\u80fd\uff0c\u6240\u6709\u57fa\u51c6\u6d4b\u8bd5\u90fd\u5185\u7f6e\u4e86\u8bc4\u4f30\u65b9\u6cd5\u3002\u8be5\u6846\u67b6\u8bbe\u8ba1\u5141\u8bb8\u7528\u6237\u8f7b\u677e\u52a0\u8f7d\u3001\u4f7f\u7528\u548c\u8bc4\u4f30\u5404\u79cd\u4efb\u52a1\u7684\u6570\u636e\u96c6\uff0c\u800c\u65e0\u9700\u624b\u52a8\u5904\u7406\u6570\u636e\u4e0b\u8f7d\u548c\u8bc4\u4f30\u903b\u8f91\u3002 \u6240\u6709\u6570\u636e\u96c6\u5728\u9996\u6b21\u4f7f\u7528\u65f6\u90fd\u4f1a\u81ea\u52a8\u4e0b\u8f7d\u5230\u9ed8\u8ba4\u8def\u5f84\uff08~/.evoagentx/data/\uff09\uff0c\u6216\u8005\u7528\u6237\u53ef\u4ee5\u901a\u8fc7 <code>path</code> \u53c2\u6570\u6307\u5b9a\u81ea\u5b9a\u4e49\u8def\u5f84\u3002\u6bcf\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u7c7b\u90fd\u5b9e\u73b0\u4e86\u6807\u51c6\u5316\u63a5\u53e3\uff0c\u5305\u62ec\u6570\u636e\u52a0\u8f7d\u3001\u6807\u7b7e\u68c0\u7d22\u548c\u9884\u6d4b\u8bc4\u4f30\u7684\u65b9\u6cd5\u3002</p> <p>\u4e0b\u9762\uff0c\u6211\u4eec\u4ecb\u7ecd\u6bcf\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u7684\u9884\u5904\u7406\u6b65\u9aa4\u548c\u8bc4\u4f30\u6307\u6807\u3002</p> <ul> <li>\u95ee\u7b54<ul> <li>NQ</li> <li>HotPotQA</li> </ul> </li> <li>\u6570\u5b66<ul> <li>GSM8K</li> <li>MATH</li> </ul> </li> <li>\u4ee3\u7801\u751f\u6210<ul> <li>HumanEval</li> <li>MBPP</li> <li>LiveCodeBench</li> </ul> </li> </ul>"},{"location":"zh/modules/benchmark/#\u9884\u5904\u7406\u548c\u8bc4\u4f30\u6307\u6807","title":"\u9884\u5904\u7406\u548c\u8bc4\u4f30\u6307\u6807","text":""},{"location":"zh/modules/benchmark/#\u95ee\u7b54","title":"\u95ee\u7b54","text":"<p>\u5bf9\u4e8e\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u6211\u4eec\u9ed8\u8ba4\u4f7f\u7528\u7cbe\u786e\u5339\u914d\uff08EM\uff09\u3001F1 \u5206\u6570\u548c\u51c6\u786e\u7387\uff08ACC\uff09\u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807\u3002EM \u8981\u6c42\u9884\u6d4b\u7b54\u6848\u4e0e\u771f\u5b9e\u7b54\u6848\u5b8c\u5168\u4e00\u81f4\u3002ACC \u8981\u6c42\u9884\u6d4b\u7b54\u6848\u5305\u542b\u771f\u5b9e\u7b54\u6848\uff0c\u8fd9\u5728 LLM \u7528\u4e8e\u751f\u6210\u7b54\u6848\u65f6\u7279\u522b\u6709\u7528\u3002</p>"},{"location":"zh/modules/benchmark/#nq","title":"NQ","text":"<p>Natural Questions (NQ) \u5305\u542b\u6765\u81ea\u8c37\u6b4c\u641c\u7d22\u5f15\u64ce\u7684\u95ee\u9898\uff0c\u7b54\u6848\u7531\u4eba\u5de5\u6807\u6ce8\u8005\u6807\u6ce8\uff0c\u662f\u7ef4\u57fa\u767e\u79d1\u524d 5 \u4e2a\u641c\u7d22\u7ed3\u679c\u9875\u9762\u4e2d\u7684\u6bb5\u843d\u6216\u5b9e\u4f53\u3002\u6211\u4eec\u4f7f\u7528 DPR \u4ed3\u5e93\u63d0\u4f9b\u7684\u6570\u636e\u96c6\u5212\u5206\uff0c\u5305\u542b 79,168 \u4e2a\u8bad\u7ec3\u6837\u672c\u30018,757 \u4e2a\u5f00\u53d1\u6837\u672c\u548c 3,610 \u4e2a\u6d4b\u8bd5\u6837\u672c\u3002</p> <p>\u60a8\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u4ee3\u7801\u52a0\u8f7d\u6570\u636e\u96c6\uff1a <pre><code>from evoagentx.benchmark import NQ\nnq_dataset = NQ() # \u53ef\u9009\uff1apath=\"/path/to/save_data\"\ntest_data = nq_dataset.get_test_data()\n</code></pre> \u6570\u636e\u96c6\u4e2d\u7684\u6bcf\u4e2a\u6837\u672c\u683c\u5f0f\u5982\u4e0b\uff1a <pre><code>{\n    \"id\": \"test-1\", \n    \"question\": \"\u95ee\u9898\", \n    \"answers\": [\"\u53ef\u80fd\u7684\u7b54\u6848\"]\n}\n</code></pre></p>"},{"location":"zh/modules/benchmark/#hotpotqa","title":"HotPotQA","text":"<p>HotPotQA \u662f\u4e00\u4e2a\u591a\u8df3\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u9700\u8981\u591a\u6b65\u63a8\u7406\u6765\u56de\u7b54\u95ee\u9898\u3002\u6211\u4eec\u4f7f\u7528\u6570\u636e\u96c6\u7684\u5e72\u6270\u9879\u8bbe\u7f6e\u3002\u6bcf\u4e2a\u6837\u672c\u5305\u542b\u4e00\u4e2a\u95ee\u9898\u3001\u4e00\u4e2a\u7b54\u6848\u3001\u4e00\u4e9b\u5305\u542b\u652f\u6301\u4fe1\u606f\u548c\u5e72\u6270\u4fe1\u606f\u7684\u4e0a\u4e0b\u6587\uff0c\u4ee5\u53ca\u652f\u6301\u4e8b\u5b9e\u3002\u6211\u4eec\u53ea\u5305\u542b\u8bad\u7ec3\u96c6\u548c\u5f00\u53d1\u96c6\uff0c\u56e0\u4e3a\u6d4b\u8bd5\u96c6\u4e0d\u516c\u5f00\u3002</p> <p>\u60a8\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u4ee3\u7801\u52a0\u8f7d\u6570\u636e\u96c6\uff1a <pre><code>from evoagentx.benchmark import HotPotQA\nhotpotqa_dataset = HotPotQA() # \u53ef\u9009\uff1apath=\"/path/to/save_data\"\ntest_data = hotpotqa_dataset.get_test_data()\n</code></pre> \u6570\u636e\u96c6\u4e2d\u7684\u6bcf\u4e2a\u6837\u672c\u683c\u5f0f\u5982\u4e0b\uff0c\u5176\u4e2d supporting_fact \u7684\u7b2c\u4e8c\u4e2a\u5143\u7d20\uff08\u6574\u6570\uff09\u662f\u652f\u6301\u7b54\u6848\u7684\u4e0a\u4e0b\u6587\u53e5\u5b50\u7d22\u5f15\uff1a <pre><code>{\n        \"_id\": \"\u6837\u672cID\", \n        \"question\": \"\u95ee\u9898\", \n        \"answer\": \"\u7b54\u6848\", \n        \"context\": [[\"\u4e0a\u4e0b\u6587\u6807\u9898\", [\"\u4e0a\u4e0b\u6587\u53e5\u5b50\", \"\u53e6\u4e00\u4e2a\u53e5\u5b50\"]]],\n        \"supporting_facts\": [[\"\u652f\u6301\u6807\u9898\", 0]]\n    }\n</code></pre></p>"},{"location":"zh/modules/benchmark/#\u6570\u5b66","title":"\u6570\u5b66","text":"<p>\u5bf9\u4e8e\u6570\u5b66\u6570\u636e\u96c6\uff0c\u6211\u4eec\u4f7f\u7528\u89e3\u51b3\u7387\u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807\u3002\u89e3\u51b3\u7387\u662f\u6b63\u786e\u89e3\u51b3\u7684\u6837\u672c\u6570\u91cf\u4e0e\u603b\u6837\u672c\u6570\u91cf\u7684\u6bd4\u7387\u3002</p>"},{"location":"zh/modules/benchmark/#gsm8k","title":"GSM8K","text":"<p>GSM8K \u7531\u4eba\u5de5\u95ee\u9898\u7f16\u5199\u8005\u521b\u5efa\u7684\u9ad8\u8d28\u91cf\u5c0f\u5b66\u6570\u5b66\u95ee\u9898\u7ec4\u6210\u3002\u8fd9\u4e9b\u95ee\u9898\u9700\u8981\u591a\u6b65\u6570\u5b66\u63a8\u7406\u6765\u89e3\u51b3\u3002\u6211\u4eec\u4f7f\u7528\u539f\u59cb\u4ed3\u5e93\u63d0\u4f9b\u7684\u6570\u636e\u96c6\u5212\u5206\uff0c\u5305\u542b 7.5K \u4e2a\u8bad\u7ec3\u95ee\u9898\u548c 1K \u4e2a\u6d4b\u8bd5\u95ee\u9898\u3002</p> <p>\u60a8\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u4ee3\u7801\u52a0\u8f7d\u6570\u636e\u96c6\uff1a <pre><code>from evoagentx.benchmark import GSM8K\ngsm8k_dataset = GSM8K() # \u53ef\u9009\uff1apath=\"/path/to/save_data\"\ntest_data = gsm8k_dataset.get_test_data()\n</code></pre> \u6570\u636e\u96c6\u4e2d\u7684\u6bcf\u4e2a\u6837\u672c\u683c\u5f0f\u5982\u4e0b\uff1a <pre><code>{\n    \"id\": \"test-1\", \n    \"question\": \"\u95ee\u9898\", \n    \"answer\": \"\u7b54\u6848\"\n}\n</code></pre></p>"},{"location":"zh/modules/benchmark/#math","title":"MATH","text":"<p>Mathematics Aptitude Test of Heuristics (MATH) \u6570\u636e\u96c6\u5305\u542b\u6765\u81ea\u6570\u5b66\u7ade\u8d5b\u7684\u95ee\u9898\uff0c\u5305\u62ec AMC 10\u3001AMC 12\u3001AIME \u7b49\u3002MATH \u4e2d\u7684\u6bcf\u4e2a\u95ee\u9898\u90fd\u6709\u9010\u6b65\u89e3\u51b3\u65b9\u6848\u3002\u6211\u4eec\u4f7f\u7528\u539f\u59cb\u4ed3\u5e93\u63d0\u4f9b\u7684\u6570\u636e\u96c6\u5212\u5206\uff0c\u5305\u542b 7.5K \u4e2a\u8bad\u7ec3\u95ee\u9898\u548c 5K \u4e2a\u6d4b\u8bd5\u95ee\u9898\u3002</p> <p>\u60a8\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u4ee3\u7801\u52a0\u8f7d\u6570\u636e\u96c6\uff1a <pre><code>from evoagentx.benchmark import MATH\nmath_dataset = MATH() # \u53ef\u9009\uff1apath=\"/path/to/save_data\"\ntest_data = math_dataset.get_test_data()\n</code></pre> \u6570\u636e\u96c6\u4e2d\u7684\u6bcf\u4e2a\u6837\u672c\u683c\u5f0f\u5982\u4e0b\u3002\u5bf9\u4e8e <code>level</code> \u5b57\u6bb5\uff0c\u6709\u6548\u503c\u4e3a\uff1a\"Level 1\"\u3001\"Level 2\"\u3001\"Level 3\"\u3001\"Level 4\"\u3001\"Level 5\" \u548c \"Level ?\"\u3002<code>type</code> \u5b57\u6bb5\u53ef\u4ee5\u662f\u4ee5\u4e0b\u4e4b\u4e00\uff1a\"Geometry\"\u3001\"Algebra\"\u3001\"Intermediate Algebra\"\u3001\"Counting &amp; Probability\"\u3001\"Precalculus\"\u3001\"Number Theory\" \u6216 \"Prealgebra\"\u3002</p> <pre><code>{\n    \"id\": \"test-1\", \n    \"problem\": \"\u95ee\u9898\", \n    \"solution\": \"\u89e3\u51b3\u65b9\u6848\",\n    \"level\": \"Level 1\",\n    \"type\": \"Algebra\"\n}\n</code></pre>"},{"location":"zh/modules/benchmark/#\u4ee3\u7801\u751f\u6210","title":"\u4ee3\u7801\u751f\u6210","text":"<p>\u5bf9\u4e8e\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6211\u4eec\u4f7f\u7528 pass@k \u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807\uff0c\u5176\u4e2d k \u662f\u6bcf\u4e2a\u95ee\u9898\u7684\u89e3\u51b3\u65b9\u6848\u6570\u91cf\u3002\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0ck \u8bbe\u7f6e\u4e3a 1\u3002</p>"},{"location":"zh/modules/benchmark/#humaneval","title":"HumanEval","text":"<p>HumanEval \u662f\u4e00\u4e2a\u5305\u542b 164 \u4e2a\u7f16\u7a0b\u95ee\u9898\u7684\u6570\u636e\u96c6\uff0c\u6765\u81ea HumanEval \u57fa\u51c6\u6d4b\u8bd5\u3002\u6bcf\u4e2a\u95ee\u9898\u5305\u542b\u4e00\u4e2a\u51fd\u6570\u7b7e\u540d\u3001\u4e00\u4e2a\u89c4\u8303\u89e3\u51b3\u65b9\u6848\u548c\u4e00\u7ec4\u5355\u5143\u6d4b\u8bd5\u3002</p> <p>\u60a8\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u4ee3\u7801\u52a0\u8f7d\u6570\u636e\u96c6\uff1a <pre><code>from evoagentx.benchmark import HumanEval\nhumaneval_dataset = HumanEval() # \u53ef\u9009\uff1apath=\"/path/to/save_data\"\ntest_data = humaneval_dataset.get_test_data()\n</code></pre> \u6570\u636e\u96c6\u4e2d\u7684\u6bcf\u4e2a\u6837\u672c\u683c\u5f0f\u5982\u4e0b\uff1a <pre><code>{\n    \"task_id\": \"HumanEval/0\", \n    \"prompt\": \"\u95ee\u9898\u63d0\u793a\", \n    \"entry_point\": \"\u8981\u6d4b\u8bd5\u7684\u51fd\u6570\u540d\u79f0\",\n    \"canonical_solution\": \"\u95ee\u9898\u7684\u89c4\u8303\u89e3\u51b3\u65b9\u6848\", \n    \"test\": \"\u95ee\u9898\u7684\u5355\u5143\u6d4b\u8bd5\"\n}\n</code></pre></p>"},{"location":"zh/modules/benchmark/#mbpp","title":"MBPP","text":"<p>Mostly Basic Python Problems (MBPP) \u5305\u542b\u6570\u767e\u4e2a\u5165\u95e8\u7ea7 Python \u7f16\u7a0b\u95ee\u9898\u3002\u6bcf\u4e2a\u95ee\u9898\u5305\u542b\u4efb\u52a1\u63cf\u8ff0\u3001\u4ee3\u7801\u89e3\u51b3\u65b9\u6848\u548c 3 \u4e2a\u81ea\u52a8\u5316\u6d4b\u8bd5\u7528\u4f8b\u3002\u6211\u4eec\u4f7f\u7528 MBPP \u6570\u636e\u96c6\u7684\u6e05\u7406\u5b50\u96c6\uff0c\u5305\u542b 427 \u4e2a\u7ecf\u8fc7\u4f5c\u8005\u624b\u52a8\u9a8c\u8bc1\u7684\u95ee\u9898\u3002\u4e3a\u4e86\u4fbf\u4e8e\u8bc4\u4f30\uff0c\u6211\u4eec\u5c06 MBPP \u6570\u636e\u96c6\u8f6c\u6362\u4e3a HumanEval \u683c\u5f0f\u3002</p> <p>\u60a8\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u4ee3\u7801\u52a0\u8f7d\u6570\u636e\u96c6\uff1a <pre><code>from evoagentx.benchmark import MBPP\nmbpp_dataset = MBPP() # \u53ef\u9009\uff1apath=\"/path/to/save_data\"\ntest_data = mbpp_dataset.get_test_data()\n</code></pre> \u6570\u636e\u96c6\u4e2d\u7684\u6bcf\u4e2a\u6837\u672c\u683c\u5f0f\u5982\u4e0b\uff0c\u6211\u4eec\u4fdd\u7559\u539f\u59cb\u7684 MBPP <code>task_id</code>\uff1a <pre><code>{\n    \"task_id\": 2, \n    \"prompt\": \"\u95ee\u9898\u63d0\u793a\", \n    \"entry_point\": \"\u8981\u6d4b\u8bd5\u7684\u51fd\u6570\u540d\u79f0\",\n    \"canonical_solution\": \"\u95ee\u9898\u7684\u89c4\u8303\u89e3\u51b3\u65b9\u6848\", \n    \"test\": \"\u95ee\u9898\u7684\u5355\u5143\u6d4b\u8bd5\"\n}\n</code></pre> \u60a8\u8fd8\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 <code>example[\"code\"]</code> \u8bbf\u95ee\u539f\u59cb MBPP \u5c5e\u6027\uff0c\u5982 \"code\"\u3001\"test_list\" \u7b49\u3002</p>"},{"location":"zh/modules/benchmark/#livecodebench","title":"LiveCodeBench","text":"<p>LiveCodeBench \u662f\u4e00\u4e2a\u65e0\u6c61\u67d3\u7684 LLM \u4ee3\u7801\u8bc4\u4f30\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5b83\u968f\u65f6\u95f4\u4e0d\u65ad\u6536\u96c6\u65b0\u95ee\u9898\u3002\u7279\u522b\u662f\uff0cLiveCodeBench \u8fd8\u5173\u6ce8\u66f4\u5e7f\u6cdb\u7684\u4ee3\u7801\u76f8\u5173\u80fd\u529b\uff0c\u5982\u4ee3\u7801\u6267\u884c\u548c\u6d4b\u8bd5\u8f93\u51fa\u9884\u6d4b\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u4ee3\u7801\u751f\u6210\u3002\u76ee\u524d\uff0cLiveCodeBench \u6258\u7ba1\u4e86 300 \u591a\u4e2a\u9ad8\u8d28\u91cf\u7f16\u7a0b\u95ee\u9898\uff0c\u53d1\u5e03\u65f6\u95f4\u5728 2023 \u5e74 5 \u6708\u81f3 2024 \u5e74 2 \u6708\u4e4b\u95f4\u3002</p> <p>\u60a8\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u4ee3\u7801\u52a0\u8f7d\u6570\u636e\u96c6\uff0c\u5176\u4e2d <code>scenario</code> \u53ef\u4ee5\u662f [<code>code_generation</code>\u3001<code>test_output_prediction</code>\u3001<code>code_execution</code>] \u4e4b\u4e00\uff0c\u8868\u793a\u4e0d\u540c\u7684\u4efb\u52a1\u3002<code>version</code> \u8868\u793a\u4ee3\u7801\u751f\u6210\u6570\u636e\u96c6\u7684\u4e0d\u540c\u7248\u672c\uff0c\u4ec5\u9002\u7528\u4e8e <code>code_generation</code> \u573a\u666f\uff0c\u53ef\u4ee5\u662f <code>[\"release_v1\"\u3001\"release_v2\"\u3001\"release_v3\"\u3001\"release_v4\"\u3001\"release_v5\"\u3001\"release_latest\"]</code> \u4e4b\u4e00\u3002\u6709\u5173\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 LiveCodeBench \u4ed3\u5e93\u3002</p> <pre><code>from evoagentx.benchmark import LiveCodeBench\nlivecodebench_dataset = LiveCodeBench(scenario=\"code_generation\", version=\"release_v1\") # \u53ef\u9009\uff1apath=\"/path/to/save_data\"\ntest_data = livecodebench_dataset.get_test_data()\n</code></pre>"},{"location":"zh/modules/customize_agent/","title":"\u81ea\u5b9a\u4e49\u4ee3\u7406","text":""},{"location":"zh/modules/customize_agent/#\u7b80\u4ecb","title":"\u7b80\u4ecb","text":"<p><code>CustomizeAgent</code> \u7c7b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u521b\u5efa\u4e13\u95e8\u7684 LLM \u9a71\u52a8\u7684\u4ee3\u7406\u3002\u5b83\u5141\u8bb8\u5b9a\u4e49\u5177\u6709\u660e\u786e\u5b9a\u4e49\u7684\u8f93\u5165\u3001\u8f93\u51fa\u3001\u81ea\u5b9a\u4e49\u63d0\u793a\u6a21\u677f\u548c\u53ef\u914d\u7f6e\u89e3\u6790\u7b56\u7565\u7684\u4ee3\u7406\uff0c\u4f7f\u5176\u9002\u5408\u5feb\u901f\u539f\u578b\u8bbe\u8ba1\u548c\u90e8\u7f72\u7279\u5b9a\u9886\u57df\u7684\u4ee3\u7406\u3002</p>"},{"location":"zh/modules/customize_agent/#\u4e3b\u8981\u7279\u6027","title":"\u4e3b\u8981\u7279\u6027","text":"<ul> <li>\u65e0\u9700\u81ea\u5b9a\u4e49\u4ee3\u7801\uff1a\u901a\u8fc7\u914d\u7f6e\u800c\u4e0d\u662f\u7f16\u5199\u81ea\u5b9a\u4e49\u4ee3\u7406\u7c7b\u6765\u521b\u5efa\u4e13\u95e8\u7684\u4ee3\u7406</li> <li>\u7075\u6d3b\u7684\u8f93\u5165/\u8f93\u51fa\u5b9a\u4e49\uff1a\u660e\u786e\u5b9a\u4e49\u4ee3\u7406\u63a5\u53d7\u7684\u8f93\u5165\u548c\u4ea7\u751f\u7684\u8f93\u51fa</li> <li>\u53ef\u81ea\u5b9a\u4e49\u7684\u89e3\u6790\u7b56\u7565\uff1a\u591a\u79cd\u89e3\u6790\u6a21\u5f0f\uff0c\u7528\u4e8e\u4ece LLM \u54cd\u5e94\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u6570\u636e</li> <li>\u53ef\u91cd\u7528\u7ec4\u4ef6\uff1a\u4fdd\u5b58\u548c\u52a0\u8f7d\u4ee3\u7406\u5b9a\u4e49\uff0c\u4ee5\u4fbf\u5728\u9879\u76ee\u4e4b\u95f4\u91cd\u7528</li> </ul>"},{"location":"zh/modules/customize_agent/#\u57fa\u672c\u7528\u6cd5","title":"\u57fa\u672c\u7528\u6cd5","text":""},{"location":"zh/modules/customize_agent/#\u7b80\u5355\u4ee3\u7406","title":"\u7b80\u5355\u4ee3\u7406","text":"<p>\u521b\u5efa <code>CustomizeAgent</code> \u7684\u6700\u7b80\u5355\u65b9\u6cd5\u662f\u53ea\u4f7f\u7528\u540d\u79f0\u3001\u63cf\u8ff0\u548c\u63d0\u793a\uff1a</p> <pre><code>import os \nfrom dotenv import load_dotenv\nfrom evoagentx.models import OpenAILLMConfig\nfrom evoagentx.agents import CustomizeAgent\n\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\n# \u914d\u7f6e LLM\nopenai_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=OPENAI_API_KEY)\n\n# \u521b\u5efa\u4e00\u4e2a\u7b80\u5355\u4ee3\u7406\nsimple_agent = CustomizeAgent(\n    name=\"SimpleAgent\",\n    description=\"A basic agent that responds to queries\",\n    prompt=\"Answer the following question: {question}\",\n    llm_config=openai_config,\n    inputs=[\n        {\"name\": \"question\", \"type\": \"string\", \"description\": \"The question to answer\"}\n    ]\n)\n\n# \u6267\u884c\u4ee3\u7406\nresponse = simple_agent(inputs={\"question\": \"What is a language model?\"})\nprint(response.content.content)  # \u8bbf\u95ee\u539f\u59cb\u54cd\u5e94\u5185\u5bb9\n</code></pre> <p>\u5728\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\uff1a 1. \u7531\u4e8e\u63d0\u793a\u9700\u8981\u8f93\u5165\uff0c\u6211\u4eec\u5728 <code>inputs</code> \u53c2\u6570\u4e2d\u6307\u5b9a\u4e86\u8f93\u5165\u4fe1\u606f\uff08\u5305\u62ec\u5176\u540d\u79f0\u3001\u7c7b\u578b\u548c\u63cf\u8ff0\uff09\u3002 2. \u6b64\u5916\uff0c\u5f53\u4f7f\u7528 <code>simple_agent(...)</code> \u6267\u884c\u4ee3\u7406\u65f6\uff0c\u60a8\u5e94\u8be5\u5728 <code>inputs</code> \u53c2\u6570\u4e2d\u63d0\u4f9b\u6240\u6709\u8f93\u5165\u3002</p> <p>\u6267\u884c\u4ee3\u7406\u540e\u7684\u8f93\u51fa\u662f\u4e00\u4e2a <code>Message</code> \u5bf9\u8c61\uff0c\u5176\u4e2d\u5305\u542b <code>message.content.content</code> \u4e2d\u7684\u539f\u59cb LLM \u54cd\u5e94\u3002</p> <p>Note</p> <p>\u5728 <code>CustomizeAgent(inputs=[...])</code> \u4e2d\u6307\u5b9a\u7684\u6240\u6709\u8f93\u5165\u540d\u79f0\u90fd\u5e94\u8be5\u51fa\u73b0\u5728 <code>prompt</code> \u4e2d\u3002\u5426\u5219\uff0c\u5c06\u5f15\u53d1\u9519\u8bef\u3002</p>"},{"location":"zh/modules/customize_agent/#\u7ed3\u6784\u5316\u8f93\u51fa","title":"\u7ed3\u6784\u5316\u8f93\u51fa","text":"<p><code>CustomizeAgent</code> \u6700\u5f3a\u5927\u7684\u529f\u80fd\u4e4b\u4e00\u662f\u80fd\u591f\u5b9a\u4e49\u7ed3\u6784\u5316\u8f93\u51fa\u3002\u8fd9\u5141\u8bb8\u60a8\u5c06\u975e\u7ed3\u6784\u5316\u7684 LLM \u54cd\u5e94\u8f6c\u6362\u4e3a\u66f4\u5bb9\u6613\u4ee5\u7f16\u7a0b\u65b9\u5f0f\u5904\u7406\u7684\u660e\u786e\u5b9a\u4e49\u7684\u6570\u636e\u7ed3\u6784\u3002</p>"},{"location":"zh/modules/customize_agent/#\u57fa\u672c\u7ed3\u6784\u5316\u8f93\u51fa","title":"\u57fa\u672c\u7ed3\u6784\u5316\u8f93\u51fa","text":"<p>\u4ee5\u4e0b\u662f\u5b9a\u4e49\u7ed3\u6784\u5316\u8f93\u51fa\u7684\u7b80\u5355\u793a\u4f8b\uff1a</p> <pre><code>from evoagentx.core.module_utils import extract_code_blocks\n\n# \u521b\u5efa\u4e00\u4e2a\u5177\u6709\u7ed3\u6784\u5316\u8f93\u51fa\u7684\u4ee3\u7801\u7f16\u5199\u4ee3\u7406\ncode_writer = CustomizeAgent(\n    name=\"CodeWriter\",\n    description=\"Writes Python code based on requirements\",\n    prompt=\"Write Python code that implements the following requirement: {requirement}\",\n    llm_config=openai_config,\n    inputs=[\n        {\"name\": \"requirement\", \"type\": \"string\", \"description\": \"The coding requirement\"}\n    ],\n    outputs=[\n        {\"name\": \"code\", \"type\": \"string\", \"description\": \"The generated Python code\"}\n    ],\n    parse_mode=\"custom\",  # \u4f7f\u7528\u81ea\u5b9a\u4e49\u89e3\u6790\u51fd\u6570\n    parse_func=lambda content: {\"code\": extract_code_blocks(content)[0]}  # \u63d0\u53d6\u7b2c\u4e00\u4e2a\u4ee3\u7801\u5757\n)\n\n# \u6267\u884c\u4ee3\u7406\nmessage = code_writer(\n    inputs={\"requirement\": \"Write a function that returns the sum of two numbers\"}\n)\nprint(message.content.code)  # \u76f4\u63a5\u8bbf\u95ee\u89e3\u6790\u540e\u7684\u4ee3\u7801\n</code></pre> <p>\u5728\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\uff1a 1. \u6211\u4eec\u5728 <code>outputs</code> \u53c2\u6570\u4e2d\u5b9a\u4e49\u4e86\u4e00\u4e2a\u540d\u4e3a <code>code</code> \u7684\u8f93\u51fa\u5b57\u6bb5\u3002 2. \u6211\u4eec\u8bbe\u7f6e <code>parse_mode=\"custom\"</code> \u6765\u4f7f\u7528\u81ea\u5b9a\u4e49\u89e3\u6790\u51fd\u6570\u3002 3. <code>parse_func</code> \u4ece LLM \u54cd\u5e94\u4e2d\u63d0\u53d6\u7b2c\u4e00\u4e2a\u4ee3\u7801\u5757\u3002 4. \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7 <code>message.content.code</code> \u76f4\u63a5\u8bbf\u95ee\u89e3\u6790\u540e\u7684\u4ee3\u7801\u3002</p> <p>\u60a8\u4e5f\u53ef\u4ee5\u901a\u8fc7 <code>message.content.content</code> \u8bbf\u95ee\u539f\u59cb LLM \u54cd\u5e94\u3002</p> <p>Note</p> <ol> <li> <p>\u5982\u679c\u5728 <code>CustomizeAgent</code> \u4e2d\u8bbe\u7f6e\u4e86 <code>outputs</code> \u53c2\u6570\uff0c\u4ee3\u7406\u5c06\u5c1d\u8bd5\u6839\u636e\u8f93\u51fa\u5b57\u6bb5\u540d\u79f0\u89e3\u6790 LLM \u54cd\u5e94\u3002\u5982\u679c\u60a8\u4e0d\u60f3\u89e3\u6790 LLM \u54cd\u5e94\uff0c\u5219\u4e0d\u5e94\u8bbe\u7f6e <code>outputs</code> \u53c2\u6570\u3002\u53ef\u4ee5\u901a\u8fc7 <code>message.content.content</code> \u8bbf\u95ee\u539f\u59cb LLM \u54cd\u5e94\u3002</p> </li> <li> <p>CustomizeAgent \u652f\u6301\u4e0d\u540c\u7684\u89e3\u6790\u6a21\u5f0f\uff0c\u5982 <code>['str', 'json', 'xml', 'title', 'custom']</code>\u3002\u6709\u5173\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u89e3\u6790\u6a21\u5f0f\u90e8\u5206\u3002</p> </li> </ol>"},{"location":"zh/modules/customize_agent/#\u591a\u4e2a\u7ed3\u6784\u5316\u8f93\u51fa","title":"\u591a\u4e2a\u7ed3\u6784\u5316\u8f93\u51fa","text":"<p>\u60a8\u53ef\u4ee5\u5b9a\u4e49\u591a\u4e2a\u8f93\u51fa\u5b57\u6bb5\u6765\u521b\u5efa\u66f4\u590d\u6742\u7684\u7ed3\u6784\u5316\u6570\u636e\uff1a</p> <pre><code># \u751f\u6210\u4ee3\u7801\u548c\u89e3\u91ca\u7684\u4ee3\u7406\nanalyzer = CustomizeAgent(\n    name=\"CodeAnalyzer\",\n    description=\"Generates and explains Python code\",\n    prompt=\"\"\"\n    Write Python code for: {requirement}\n\n    Provide your response in the following format:\n\n    ## code\n    [Your code implementation here]\n\n    ## explanation\n    [A brief explanation of how the code works]\n\n    ## complexity\n    [Time and space complexity analysis]\n    \"\"\",\n    llm_config=openai_config,\n    inputs=[\n        {\"name\": \"requirement\", \"type\": \"string\", \"description\": \"The coding requirement\"}\n    ],\n    outputs=[\n        {\"name\": \"code\", \"type\": \"string\", \"description\": \"The generated Python code\"},\n        {\"name\": \"explanation\", \"type\": \"string\", \"description\": \"Explanation of the code\"},\n        {\"name\": \"complexity\", \"type\": \"string\", \"description\": \"Complexity analysis\"}\n    ],\n    parse_mode=\"title\"  # \u4f7f\u7528\u9ed8\u8ba4\u7684\u6807\u9898\u89e3\u6790\u6a21\u5f0f\n)\n\n# \u6267\u884c\u4ee3\u7406\nresult = analyzer(inputs={\"requirement\": \"Write a binary search algorithm\"})\n\n# \u5206\u522b\u8bbf\u95ee\u6bcf\u4e2a\u7ed3\u6784\u5316\u8f93\u51fa\nprint(\"CODE:\")\nprint(result.content.code)\nprint(\"\\nEXPLANATION:\")\nprint(result.content.explanation)\nprint(\"\\nCOMPLEXITY:\")\nprint(result.content.complexity)\n</code></pre>"},{"location":"zh/modules/customize_agent/#\u89e3\u6790\u6a21\u5f0f","title":"\u89e3\u6790\u6a21\u5f0f","text":"<p>CustomizeAgent \u652f\u6301\u4e0d\u540c\u7684\u65b9\u5f0f\u6765\u89e3\u6790 LLM \u8f93\u51fa\uff1a</p>"},{"location":"zh/modules/customize_agent/#1-\u5b57\u7b26\u4e32\u6a21\u5f0f-parse_modestr","title":"1. \u5b57\u7b26\u4e32\u6a21\u5f0f (<code>parse_mode=\"str\"</code>)","text":"<p>\u4f7f\u7528\u539f\u59cb LLM \u8f93\u51fa\u4f5c\u4e3a\u6bcf\u4e2a\u8f93\u51fa\u5b57\u6bb5\u7684\u503c\u3002\u9002\u7528\u4e8e\u4e0d\u9700\u8981\u7ed3\u6784\u5316\u89e3\u6790\u7684\u7b80\u5355\u4ee3\u7406\u3002</p> <pre><code>agent = CustomizeAgent(\n    name=\"SimpleAgent\",\n    description=\"Returns raw output\",\n    prompt=\"Generate a greeting for {name}\",\n    inputs=[{\"name\": \"name\", \"type\": \"string\", \"description\": \"The name to greet\"}],\n    outputs=[{\"name\": \"greeting\", \"type\": \"string\", \"description\": \"The generated greeting\"}],\n    parse_mode=\"str\",\n    # \u5176\u4ed6\u53c2\u6570...\n)\n</code></pre> <p>\u6267\u884c\u4ee3\u7406\u540e\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7 <code>message.content.content</code> \u6216 <code>message.content.greeting</code> \u8bbf\u95ee\u539f\u59cb LLM \u54cd\u5e94\u3002</p>"},{"location":"zh/modules/customize_agent/#2-\u6807\u9898\u6a21\u5f0f-parse_modetitle\u9ed8\u8ba4","title":"2. \u6807\u9898\u6a21\u5f0f (<code>parse_mode=\"title\"</code>\uff0c\u9ed8\u8ba4)","text":"<p>\u63d0\u53d6\u4e0e\u8f93\u51fa\u5b57\u6bb5\u540d\u79f0\u5339\u914d\u7684\u6807\u9898\u4e4b\u95f4\u7684\u5185\u5bb9\u3002\u8fd9\u662f\u9ed8\u8ba4\u7684\u89e3\u6790\u6a21\u5f0f\u3002</p> <pre><code>agent = CustomizeAgent(\n    name=\"ReportGenerator\",\n    description=\"Generates a structured report\",\n    prompt=\"Create a report about {topic}\",\n    outputs=[\n        {\"name\": \"summary\", \"type\": \"string\", \"description\": \"Brief summary\"},\n        {\"name\": \"analysis\", \"type\": \"string\", \"description\": \"Detailed analysis\"}\n    ],\n    # \u9ed8\u8ba4\u6807\u9898\u6a21\u5f0f\u662f \"## {title}\"\n    title_format=\"### {title}\",  # \u53ef\u9009\uff1a\u81ea\u5b9a\u4e49\u6807\u9898\u683c\u5f0f\n    # \u5176\u4ed6\u53c2\u6570...\n)\n</code></pre> <p>\u4f7f\u7528\u6b64\u914d\u7f6e\uff0c\u5e94\u8be5\u6307\u793a LLM \u5c06\u5176\u54cd\u5e94\u683c\u5f0f\u5316\u4e3a\uff1a</p> <pre><code>### summary\nBrief summary of the topic here.\n\n### analysis\nDetailed analysis of the topic here.\n</code></pre> <p>Note</p> <p>LLM \u8f93\u51fa\u7684\u7ae0\u8282\u6807\u9898\u5e94\u8be5\u4e0e\u8f93\u51fa\u5b57\u6bb5\u540d\u79f0\u5b8c\u5168\u76f8\u540c\u3002\u5426\u5219\uff0c\u89e3\u6790\u5c06\u5931\u8d25\u3002\u4f8b\u5982\uff0c\u5728\u4e0a\u9762\u7684\u4f8b\u5b50\u4e2d\uff0c\u5982\u679c LLM \u8f93\u51fa <code>### Analysis</code>\uff0c\u8fd9\u4e0e\u8f93\u51fa\u5b57\u6bb5\u540d\u79f0 <code>analysis</code> \u4e0d\u540c\uff0c\u89e3\u6790\u5c06\u5931\u8d25\u3002</p>"},{"location":"zh/modules/customize_agent/#3-json-\u6a21\u5f0f-parse_modejson","title":"3. JSON \u6a21\u5f0f (<code>parse_mode=\"json\"</code>)","text":"<p>\u89e3\u6790 LLM \u8f93\u51fa\u7684 JSON \u5b57\u7b26\u4e32\u3002JSON \u5b57\u7b26\u4e32\u7684\u952e\u5e94\u8be5\u4e0e\u8f93\u51fa\u5b57\u6bb5\u540d\u79f0\u5b8c\u5168\u76f8\u540c\u3002</p> <pre><code>agent = CustomizeAgent(\n    name=\"DataExtractor\",\n    description=\"Extracts structured data\",\n    prompt=\"Extract key information from this text: {text}\",\n    inputs=[\n        {\"name\": \"text\", \"type\": \"string\", \"description\": \"The text to extract information from\"}\n    ],\n    outputs=[\n        {\"name\": \"people\", \"type\": \"string\", \"description\": \"Names of people mentioned\"},\n        {\"name\": \"places\", \"type\": \"string\", \"description\": \"Locations mentioned\"},\n        {\"name\": \"dates\", \"type\": \"string\", \"description\": \"Dates mentioned\"}\n    ],\n    parse_mode=\"json\",\n    # \u5176\u4ed6\u53c2\u6570...\n)\n</code></pre> <p>\u4f7f\u7528\u6b64\u6a21\u5f0f\u65f6\uff0cLLM \u5e94\u8be5\u8f93\u51fa\u4e00\u4e2a\u6709\u6548\u7684 JSON \u5b57\u7b26\u4e32\uff0c\u5176\u952e\u4e0e\u8f93\u51fa\u5b57\u6bb5\u540d\u79f0\u5339\u914d\u3002\u4f8b\u5982\uff0c\u60a8\u5e94\u8be5\u6307\u793a LLM \u8f93\u51fa\uff1a</p> <pre><code>{\n    \"people\": \"extracted people\",\n    \"places\": \"extracted places\",\n    \"dates\": \"extracted dates\"\n}\n</code></pre> <p>\u5982\u679c LLM \u54cd\u5e94\u4e2d\u6709\u591a\u4e2a JSON \u5b57\u7b26\u4e32\uff0c\u5c06\u53ea\u4f7f\u7528\u7b2c\u4e00\u4e2a\u3002</p>"},{"location":"zh/modules/customize_agent/#4-xml-\u6a21\u5f0f-parse_modexml","title":"4. XML \u6a21\u5f0f (<code>parse_mode=\"xml\"</code>)","text":"<p>\u89e3\u6790 LLM \u8f93\u51fa\u7684 XML \u5b57\u7b26\u4e32\u3002XML \u5b57\u7b26\u4e32\u7684\u952e\u5e94\u8be5\u4e0e\u8f93\u51fa\u5b57\u6bb5\u540d\u79f0\u5b8c\u5168\u76f8\u540c\u3002</p> <pre><code>agent = CustomizeAgent(\n    name=\"DataExtractor\",\n    description=\"Extracts structured data\",\n    prompt=\"Extract key information from this text: {text}\",\n    inputs=[\n        {\"name\": \"text\", \"type\": \"string\", \"description\": \"The text to extract information from\"}\n    ],\n    outputs=[\n        {\"name\": \"people\", \"type\": \"string\", \"description\": \"Names of people mentioned\"},\n    ],\n    parse_mode=\"xml\",\n    # \u5176\u4ed6\u53c2\u6570...\n)\n</code></pre> <p>\u4f7f\u7528\u6b64\u6a21\u5f0f\u65f6\uff0cLLM \u5e94\u8be5\u751f\u6210\u5305\u542b\u4e0e\u8f93\u51fa\u5b57\u6bb5\u540d\u79f0\u5339\u914d\u7684 XML \u6807\u7b7e\u7684\u6587\u672c\u3002\u4f8b\u5982\uff0c\u60a8\u5e94\u8be5\u6307\u793a LLM \u8f93\u51fa\uff1a</p> <pre><code>The people mentioned in the text are: &lt;people&gt;John Doe and Jane Smith&lt;/people&gt;.\n</code></pre> <p>\u5982\u679c LLM \u8f93\u51fa\u5305\u542b\u591a\u4e2a\u5177\u6709\u76f8\u540c\u540d\u79f0\u7684 XML \u6807\u7b7e\uff0c\u5c06\u53ea\u4f7f\u7528\u7b2c\u4e00\u4e2a\u3002</p>"},{"location":"zh/modules/customize_agent/#5-\u81ea\u5b9a\u4e49\u89e3\u6790-parse_modecustom","title":"5. \u81ea\u5b9a\u4e49\u89e3\u6790 (<code>parse_mode=\"custom\"</code>)","text":"<p>\u4e3a\u4e86\u83b7\u5f97\u6700\u5927\u7684\u7075\u6d3b\u6027\uff0c\u60a8\u53ef\u4ee5\u5b9a\u4e49\u81ea\u5b9a\u4e49\u89e3\u6790\u51fd\u6570\uff1a</p> <pre><code>from evoagentx.core.registry import register_parse_function\n\n@register_parse_function  # \u6ce8\u518c\u51fd\u6570\u4ee5\u4fbf\u5e8f\u5217\u5316\ndef extract_python_code(content: str) -&gt; dict:\n    \"\"\"\u4ece LLM \u54cd\u5e94\u4e2d\u63d0\u53d6 Python \u4ee3\u7801\"\"\"\n    code_blocks = extract_code_blocks(content)\n    return {\"code\": code_blocks[0] if code_blocks else \"\"}\n\nagent = CustomizeAgent(\n    name=\"CodeExplainer\",\n    description=\"Generates and explains code\",\n    prompt=\"Write a Python function that {requirement}\",\n    inputs=[\n        {\"name\": \"requirement\", \"type\": \"string\", \"description\": \"The requirement to generate code for\"}\n    ],\n    outputs=[\n        {\"name\": \"code\", \"type\": \"string\", \"description\": \"The generated code\"},\n    ],\n    parse_mode=\"custom\",\n    parse_func=extract_python_code,\n    # \u5176\u4ed6\u53c2\u6570...\n)\n</code></pre> <p>Note</p> <ol> <li> <p>\u89e3\u6790\u51fd\u6570\u5e94\u8be5\u6709\u4e00\u4e2a\u8f93\u5165\u53c2\u6570 <code>content</code>\uff0c\u5b83\u63a5\u6536\u539f\u59cb LLM \u54cd\u5e94\u4f5c\u4e3a\u8f93\u5165\uff0c\u5e76\u8fd4\u56de\u4e00\u4e2a\u5b57\u5178\uff0c\u5176\u952e\u4e0e\u8f93\u51fa\u5b57\u6bb5\u540d\u79f0\u5339\u914d\u3002</p> </li> <li> <p>\u5efa\u8bae\u4f7f\u7528 <code>@register_parse_function</code> \u88c5\u9970\u5668\u6765\u6ce8\u518c\u89e3\u6790\u51fd\u6570\u4ee5\u4fbf\u5e8f\u5217\u5316\uff0c\u8fd9\u6837\u60a8\u5c31\u53ef\u4ee5\u4fdd\u5b58\u4ee3\u7406\u5e76\u5728\u4ee5\u540e\u52a0\u8f7d\u5b83\u3002</p> </li> </ol>"},{"location":"zh/modules/customize_agent/#\u4fdd\u5b58\u548c\u52a0\u8f7d\u4ee3\u7406","title":"\u4fdd\u5b58\u548c\u52a0\u8f7d\u4ee3\u7406","text":"<p>\u60a8\u53ef\u4ee5\u4fdd\u5b58\u4ee3\u7406\u5b9a\u4e49\u4ee5\u4fbf\u4ee5\u540e\u91cd\u7528\uff1a</p> <pre><code># \u4fdd\u5b58\u4ee3\u7406\u914d\u7f6e\u3002\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c`llm_config` \u4e0d\u4f1a\u88ab\u4fdd\u5b58\u3002\ncode_writer.save_module(\"./agents/code_writer.json\")\n\n# \u4ece\u6587\u4ef6\u52a0\u8f7d\u4ee3\u7406\uff08\u9700\u8981\u518d\u6b21\u63d0\u4f9b llm_config\uff09\nloaded_agent = CustomizeAgent.from_file(\n    \"./agents/code_writer.json\", \n    llm_config=openai_config\n)\n</code></pre>"},{"location":"zh/modules/customize_agent/#\u9ad8\u7ea7\u793a\u4f8b\u591a\u6b65\u9aa4\u4ee3\u7801\u751f\u6210\u5668","title":"\u9ad8\u7ea7\u793a\u4f8b\uff1a\u591a\u6b65\u9aa4\u4ee3\u7801\u751f\u6210\u5668","text":"<p>\u4ee5\u4e0b\u662f\u4e00\u4e2a\u66f4\u9ad8\u7ea7\u7684\u793a\u4f8b\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u521b\u5efa\u4e00\u4e2a\u5177\u6709\u591a\u4e2a\u7ed3\u6784\u5316\u8f93\u51fa\u7684\u4e13\u95e8\u4ee3\u7801\u751f\u6210\u4ee3\u7406\uff1a</p> <pre><code>from pydantic import Field\nfrom evoagentx.actions import ActionOutput\nfrom evoagentx.core.registry import register_parse_function\n\nclass CodeGeneratorOutput(ActionOutput):\n    code: str = Field(description=\"The generated Python code\")\n    documentation: str = Field(description=\"Documentation for the code\")\n    tests: str = Field(description=\"Unit tests for the code\")\n\n@register_parse_function\ndef parse_code_documentation_tests(content: str) -&gt; dict:\n    \"\"\"\u5c06 LLM \u8f93\u51fa\u89e3\u6790\u4e3a\u4ee3\u7801\u3001\u6587\u6863\u548c\u6d4b\u8bd5\u90e8\u5206\"\"\"\n    sections = content.split(\"## \")\n    result = {\"code\": \"\", \"documentation\": \"\", \"tests\": \"\"}\n\n    for section in sections:\n        if not section.strip():\n            continue\n\n        lines = section.strip().split(\"\\n\")\n        section_name = lines[0].lower()\n        section_content = \"\\n\".join(lines[1:]).strip()\n\n        if \"code\" in section_name:\n            # \u4ece\u4ee3\u7801\u5757\u4e2d\u63d0\u53d6\u4ee3\u7801\n            code_blocks = extract_code_blocks(section_content)\n            result[\"code\"] = code_blocks[0] if code_blocks else section_content\n        elif \"documentation\" in section_name:\n            result[\"documentation\"] = section_content\n        elif \"test\" in section_name:\n            # \u5982\u679c\u5b58\u5728\uff0c\u4ece\u4ee3\u7801\u5757\u4e2d\u63d0\u53d6\u4ee3\u7801\n            code_blocks = extract_code_blocks(section_content)\n            result[\"tests\"] = code_blocks[0] if code_blocks else section_content\n\n    return result\n\n# \u521b\u5efa\u9ad8\u7ea7\u4ee3\u7801\u751f\u6210\u5668\u4ee3\u7406\nadvanced_generator = CustomizeAgent(\n    name=\"AdvancedCodeGenerator\",\n    description=\"Generates complete code packages with documentation and tests\",\n    prompt=\"\"\"\n    Create a complete implementation based on this requirement:\n    {requirement}\n\n    Provide your response in the following format:\n\n    ## Code\n    [Include the Python code implementation here]\n\n    ## Documentation\n    [Include clear documentation explaining the code]\n\n    ## Tests\n    [Include unit tests that verify the code works correctly]\n    \"\"\",\n    llm_config=openai_config,\n    inputs=[\n        {\"name\": \"requirement\", \"type\": \"string\", \"description\": \"The coding requirement\"}\n    ],\n    outputs=[\n        {\"name\": \"code\", \"type\": \"string\", \"description\": \"The generated Python code\"},\n        {\"name\": \"documentation\", \"type\": \"string\", \"description\": \"Documentation for the code\"},\n        {\"name\": \"tests\", \"type\": \"string\", \"description\": \"Unit tests for the code\"}\n    ],\n    output_parser=CodeGeneratorOutput,\n    parse_mode=\"custom\",\n    parse_func=parse_code_documentation_tests,\n    system_prompt=\"You are an expert Python developer specialized in writing clean, efficient code with comprehensive documentation and tests.\"\n)\n\n# \u6267\u884c\u4ee3\u7406\nresult = advanced_generator(\n    inputs={\n        \"requirement\": \"Create a function to validate if a string is a valid email address\"\n    }\n)\n\n# \u8bbf\u95ee\u7ed3\u6784\u5316\u8f93\u51fa\nprint(\"CODE:\")\nprint(result.content.code)\nprint(\"\\nDOCUMENTATION:\")\nprint(result.content.documentation)\nprint(\"\\nTESTS:\")\nprint(result.content.tests)\n</code></pre> <p>\u8fd9\u4e2a\u9ad8\u7ea7\u793a\u4f8b\u5c55\u793a\u4e86\u5982\u4f55\u521b\u5efa\u4e00\u4e2a\u4e13\u95e8\u7684\u4ee3\u7406\uff0c\u5b83\u53ef\u4ee5\u4ece\u5355\u4e2a LLM \u8c03\u7528\u4e2d\u4ea7\u751f\u591a\u4e2a\u7ed3\u6784\u5316\u8f93\u51fa\uff0c\u63d0\u4f9b\u5305\u542b\u5b9e\u73b0\u3001\u6587\u6863\u548c\u6d4b\u8bd5\u7684\u5b8c\u6574\u4ee3\u7801\u5305\u3002</p>"},{"location":"zh/modules/evaluator/","title":"\u8bc4\u4f30\u5668","text":""},{"location":"zh/modules/evaluator/#\u7b80\u4ecb","title":"\u7b80\u4ecb","text":"<p><code>Evaluator</code> \u7c7b\u662f EvoAgentX \u6846\u67b6\u4e2d\u7684\u4e00\u4e2a\u57fa\u7840\u7ec4\u4ef6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5de5\u4f5c\u6d41\u548c\u52a8\u4f5c\u56fe\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u6027\u80fd\u3002\u5b83\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ed3\u6784\u5316\u7684\u65b9\u5f0f\u6765\u8861\u91cf AI \u4ee3\u7406\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u8fd0\u884c\u6d4b\u8bd5\u6570\u636e\u5e76\u8ba1\u7b97\u6307\u6807\u3002</p>"},{"location":"zh/modules/evaluator/#\u67b6\u6784","title":"\u67b6\u6784","text":""},{"location":"zh/modules/evaluator/#\u8bc4\u4f30\u5668\u67b6\u6784","title":"\u8bc4\u4f30\u5668\u67b6\u6784","text":"<p><code>Evaluator</code> \u7531\u51e0\u4e2a\u5173\u952e\u7ec4\u4ef6\u7ec4\u6210\uff1a</p> <ol> <li> <p>LLM \u5b9e\u4f8b\uff1a</p> <p>\u7528\u4e8e\u5728\u8bc4\u4f30\u671f\u95f4\u6267\u884c\u5de5\u4f5c\u6d41\u7684\u8bed\u8a00\u6a21\u578b\uff1a</p> <ul> <li>\u63d0\u4f9b\u5de5\u4f5c\u6d41\u6267\u884c\u6240\u9700\u7684\u63a8\u7406\u548c\u751f\u6210\u80fd\u529b</li> <li>\u53ef\u4ee5\u662f\u4efb\u4f55\u9075\u5faa <code>BaseLLM</code> \u63a5\u53e3\u7684\u5b9e\u73b0</li> </ul> </li> <li> <p>\u4ee3\u7406\u7ba1\u7406\u5668\uff1a</p> <p>\u7ba1\u7406\u8bc4\u4f30\u671f\u95f4\u5de5\u4f5c\u6d41\u56fe\u4f7f\u7528\u7684\u4ee3\u7406\uff1a</p> <ul> <li>\u63d0\u4f9b\u5de5\u4f5c\u6d41\u6267\u884c\u6240\u9700\u7684\u4ee3\u7406\u8bbf\u95ee</li> <li>\u4ec5\u5728\u8bc4\u4f30 <code>WorkFlowGraph</code> \u5b9e\u4f8b\u65f6\u9700\u8981\uff0c\u8bc4\u4f30 <code>ActionGraph</code> \u5b9e\u4f8b\u65f6\u53ef\u4ee5\u5ffd\u7565</li> </ul> </li> <li> <p>\u6570\u636e\u5904\u7406\u51fd\u6570\uff1a</p> <p>\u5728\u8bc4\u4f30\u671f\u95f4\u51c6\u5907\u548c\u5904\u7406\u6570\u636e\u7684\u51fd\u6570\uff1a</p> <ul> <li><code>collate_func</code>\uff1a\u4e3a\u5de5\u4f5c\u6d41\u6267\u884c\u51c6\u5907\u57fa\u51c6\u6d4b\u8bd5\u793a\u4f8b</li> <li><code>output_postprocess_func</code>\uff1a\u5728\u8bc4\u4f30\u524d\u5904\u7406\u5de5\u4f5c\u6d41\u8f93\u51fa</li> </ul> </li> </ol>"},{"location":"zh/modules/evaluator/#\u8bc4\u4f30\u6d41\u7a0b","title":"\u8bc4\u4f30\u6d41\u7a0b","text":"<p>\u8bc4\u4f30\u6d41\u7a0b\u9075\u5faa\u4ee5\u4e0b\u6b65\u9aa4\uff1a</p> <ol> <li>\u6570\u636e\u5904\u7406\uff1a\u4ece\u57fa\u51c6\u6d4b\u8bd5\u6570\u636e\u96c6\u4e2d\u83b7\u53d6\u793a\u4f8b\uff0c\u5e76\u5c06\u5176\u5904\u7406\u6210\u5de5\u4f5c\u6d41\u56fe\u6216\u52a8\u4f5c\u56fe\u671f\u671b\u7684\u683c\u5f0f</li> <li>\u5de5\u4f5c\u6d41\u6267\u884c\uff1a\u901a\u8fc7\u5de5\u4f5c\u6d41\u56fe\u6216\u52a8\u4f5c\u56fe\u8fd0\u884c\u6bcf\u4e2a\u793a\u4f8b</li> <li>\u8f93\u51fa\u5904\u7406\uff1a\u5c06\u8f93\u51fa\u5904\u7406\u6210\u57fa\u51c6\u6d4b\u8bd5\u671f\u671b\u7684\u683c\u5f0f</li> <li>\u6307\u6807\u8ba1\u7b97\uff1a\u901a\u8fc7\u6bd4\u8f83\u8f93\u51fa\u4e0e\u771f\u5b9e\u503c\u6765\u8ba1\u7b97\u6027\u80fd\u6307\u6807</li> <li>\u7ed3\u679c\u805a\u5408\uff1a\u5c06\u5355\u4e2a\u6307\u6807\u805a\u5408\u6210\u6574\u4f53\u6027\u80fd\u5206\u6570</li> </ol>"},{"location":"zh/modules/evaluator/#\u4f7f\u7528\u65b9\u6cd5","title":"\u4f7f\u7528\u65b9\u6cd5","text":""},{"location":"zh/modules/evaluator/#\u57fa\u672c\u8bc4\u4f30\u5668\u521b\u5efa\u4e0e\u6267\u884c","title":"\u57fa\u672c\u8bc4\u4f30\u5668\u521b\u5efa\u4e0e\u6267\u884c","text":"<pre><code>from evoagentx.evaluators import Evaluator\nfrom evoagentx.models import OpenAILLMConfig, OpenAILLM\nfrom evoagentx.agents import AgentManager\nfrom evoagentx.workflow.workflow_graph import WorkFlowGraph\nfrom evoagentx.benchmark import SomeBenchmark\nfrom evoagentx.core.callbacks import suppress_logger_info\n\n# \u521d\u59cb\u5316 LLM\nllm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=\"xxx\")\nllm = OpenAILLM(llm_config)\n\n# \u521d\u59cb\u5316\u4ee3\u7406\u7ba1\u7406\u5668\nagent_manager = AgentManager()\n\n# \u52a0\u8f7d\u5de5\u4f5c\u6d41\u56fe\nworkflow_graph = WorkFlowGraph.from_file(\"path/to/workflow.json\")\n\n# \u5c06\u4ee3\u7406\u6dfb\u52a0\u5230\u4ee3\u7406\u7ba1\u7406\u5668\nagent_manager.add_agents_from_workflow(workflow_graph, llm_config=llm_config)\n\n# \u521b\u5efa\u57fa\u51c6\u6d4b\u8bd5\nbenchmark = SomeBenchmark()\n\n# \u521b\u5efa\u8bc4\u4f30\u5668\nevaluator = Evaluator(\n    llm=llm,\n    agent_manager=agent_manager,\n    num_workers=4,  # \u4f7f\u7528 4 \u4e2a\u5e76\u884c\u5de5\u4f5c\u5668\n    verbose=True    # \u663e\u793a\u8fdb\u5ea6\u6761\n)\n\n# \u8fd0\u884c\u8bc4\u4f30\u5e76\u6291\u5236\u65e5\u5fd7\nwith suppress_logger_info():\n    results = evaluator.evaluate(\n        graph=workflow_graph,\n        benchmark=benchmark,\n        eval_mode=\"test\",    # \u5728\u6d4b\u8bd5\u96c6\u4e0a\u8bc4\u4f30\uff08\u9ed8\u8ba4\uff09\n        sample_k=100         # \u4f7f\u7528 100 \u4e2a\u968f\u673a\u793a\u4f8b\n    )\n\nprint(f\"\u8bc4\u4f30\u7ed3\u679c: {results}\")\n</code></pre>"},{"location":"zh/modules/evaluator/#\u81ea\u5b9a\u4e49\u6570\u636e\u5904\u7406","title":"\u81ea\u5b9a\u4e49\u6570\u636e\u5904\u7406","text":"<pre><code>from evoagentx.evaluators import Evaluator\nfrom evoagentx.models import OpenAILLMConfig, OpenAILLM\nfrom evoagentx.agents import AgentManager\nfrom evoagentx.core.callbacks import suppress_logger_info\n\n# \u81ea\u5b9a\u4e49\u6574\u7406\u51fd\u6570\u6765\u51c6\u5907\u8f93\u5165\u3002\u952e\u5e94\u8be5\u5339\u914d\u5de5\u4f5c\u6d41\u56fe\u6216\u52a8\u4f5c\u56fe\u7684\u8f93\u5165\u53c2\u6570\u3002\u8fd4\u56de\u503c\u5c06\u76f4\u63a5\u4f20\u9012\u7ed9\u5de5\u4f5c\u6d41\u56fe\u6216\u52a8\u4f5c\u56fe\u7684 `execute` \u65b9\u6cd5\u3002\ndef custom_collate(example):\n    return {\n        \"input_text\": example[\"question\"],\n        \"context\": example.get(\"context\", \"\")\n    }\n\n# \u81ea\u5b9a\u4e49\u8f93\u51fa\u5904\u7406\uff0c`output` \u662f\u5de5\u4f5c\u6d41\u7684\u8f93\u51fa\uff0c\u8fd4\u56de\u503c\u5c06\u4f20\u9012\u7ed9\u57fa\u51c6\u6d4b\u8bd5\u7684 `evaluate` \u65b9\u6cd5\u3002\ndef custom_postprocess(output):\n    if isinstance(output, dict):\n        return output.get(\"answer\", \"\")\n    return output\n\n# \u4f7f\u7528\u81ea\u5b9a\u4e49\u51fd\u6570\u521b\u5efa\u8bc4\u4f30\u5668\nevaluator = Evaluator(\n    llm=llm,\n    agent_manager=agent_manager,\n    collate_func=custom_collate,\n    output_postprocess_func=custom_postprocess,\n    num_workers=4,  # \u4f7f\u7528 4 \u4e2a\u5e76\u884c\u5de5\u4f5c\u5668\n    verbose=True    # \u663e\u793a\u8fdb\u5ea6\u6761\n)\n</code></pre>"},{"location":"zh/modules/evaluator/#\u8bc4\u4f30\u52a8\u4f5c\u56fe","title":"\u8bc4\u4f30\u52a8\u4f5c\u56fe","text":"<pre><code>from evoagentx.workflow.action_graph import ActionGraph\nfrom evoagentx.evaluators import Evaluator\nfrom evoagentx.models import OpenAILLMConfig, OpenAILLM\nfrom evoagentx.core.callbacks import suppress_logger_info\n\n# \u521d\u59cb\u5316 LLM\nllm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=\"xxx\")\nllm = OpenAILLM(llm_config)\n\n# \u52a0\u8f7d\u52a8\u4f5c\u56fe\naction_graph = ActionGraph.from_file(\"path/to/action_graph.json\", llm_config=llm_config)\n\n# \u521b\u5efa\u8bc4\u4f30\u5668\uff08\u52a8\u4f5c\u56fe\u4e0d\u9700\u8981 agent_manager\uff09\nevaluator = Evaluator(llm=llm, num_workers=4, verbose=True)\n\n# \u8fd0\u884c\u8bc4\u4f30\u5e76\u6291\u5236\u65e5\u5fd7\nwith suppress_logger_info():\n    results = evaluator.evaluate(\n        graph=action_graph,\n        benchmark=benchmark\n    )\n</code></pre>"},{"location":"zh/modules/evaluator/#\u5f02\u6b65\u8bc4\u4f30","title":"\u5f02\u6b65\u8bc4\u4f30","text":"<pre><code>import asyncio\nfrom evoagentx.evaluators import Evaluator\nfrom evoagentx.models import OpenAILLMConfig, OpenAILLM\nfrom evoagentx.agents import AgentManager\nfrom evoagentx.workflow.workflow_graph import WorkFlowGraph\nfrom evoagentx.benchmark import SomeBenchmark\nfrom evoagentx.core.callbacks import suppress_logger_info\n\n# \u521d\u59cb\u5316 LLM \u548c\u7ec4\u4ef6\nllm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=\"xxx\")\nllm = OpenAILLM(llm_config)\nagent_manager = AgentManager()\nworkflow_graph = WorkFlowGraph.from_file(\"path/to/workflow.json\")\nbenchmark = SomeBenchmark()\n\n# \u521b\u5efa\u8bc4\u4f30\u5668\nevaluator = Evaluator(\n    llm=llm,\n    agent_manager=agent_manager,\n    num_workers=4\n)\n\n# \u8fd0\u884c\u5f02\u6b65\u8bc4\u4f30\nasync def run_async_eval():\n    with suppress_logger_info():\n        results = await evaluator.async_evaluate(\n            graph=workflow_graph,\n            benchmark=benchmark\n        )\n    return results\n\n# \u6267\u884c\u5f02\u6b65\u8bc4\u4f30\nresults = asyncio.run(run_async_eval())\n</code></pre>"},{"location":"zh/modules/evaluator/#\u8bbf\u95ee\u8bc4\u4f30\u8bb0\u5f55","title":"\u8bbf\u95ee\u8bc4\u4f30\u8bb0\u5f55","text":"<pre><code>from evoagentx.evaluators import Evaluator\nfrom evoagentx.models import OpenAILLMConfig, OpenAILLM\nfrom evoagentx.benchmark import SomeBenchmark\nfrom evoagentx.core.callbacks import suppress_logger_info\n\n# \u521d\u59cb\u5316\u7ec4\u4ef6\nllm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=\"xxx\")\nllm = OpenAILLM(llm_config)\nbenchmark = SomeBenchmark()\nevaluator = Evaluator(llm=llm)\n\n# \u8fd0\u884c\u8bc4\u4f30\u5e76\u6291\u5236\u65e5\u5fd7\nwith suppress_logger_info():\n    evaluator.evaluate(graph=graph, benchmark=benchmark)\n\n# \u83b7\u53d6\u6240\u6709\u8bc4\u4f30\u8bb0\u5f55\nall_records = evaluator.get_all_evaluation_records()\n\n# \u83b7\u53d6\u7279\u5b9a\u793a\u4f8b\u7684\u8bb0\u5f55\nexample = benchmark.get_test_data()[0]\nrecord = evaluator.get_example_evaluation_record(benchmark, example)\n\n# \u901a\u8fc7\u793a\u4f8b ID \u83b7\u53d6\u8bb0\u5f55\nrecord_by_id = evaluator.get_evaluation_record_by_id(\n    benchmark=benchmark,\n    example_id=\"example-123\",\n    eval_mode=\"test\"\n)\n\n# \u8bbf\u95ee\u5de5\u4f5c\u6d41\u56fe\u8bc4\u4f30\u7684\u8f68\u8ff9\nif \"trajectory\" in record:\n    for message in record[\"trajectory\"]:\n        print(f\"{message.role}: {message.content}\")\n</code></pre> <p><code>Evaluator</code> \u7c7b\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u5927\u7684\u65b9\u5f0f\u6765\u8bc4\u4f30\u5de5\u4f5c\u6d41\u548c\u52a8\u4f5c\u56fe\u7684\u6027\u80fd\uff0c\u4f7f EvoAgentX \u6846\u67b6\u4e2d\u7684\u5b9a\u91cf\u6bd4\u8f83\u548c\u6539\u8fdb\u8ddf\u8e2a\u6210\u4e3a\u53ef\u80fd\u3002</p>"},{"location":"zh/modules/llm/#\u7b80\u4ecb","title":"\u7b80\u4ecb","text":"<p>LLM\uff08\u5927\u8bed\u8a00\u6a21\u578b\uff09\u6a21\u5757\u4e3a EvoAgentX \u6846\u67b6\u63d0\u4f9b\u4e86\u4e0e\u5404\u79cd\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u5546\u4ea4\u4e92\u7684\u7edf\u4e00\u63a5\u53e3\u3002\u5b83\u62bd\u8c61\u4e86\u7279\u5b9a\u63d0\u4f9b\u5546\u7684\u5b9e\u73b0\u7ec6\u8282\uff0c\u4e3a\u751f\u6210\u6587\u672c\u3001\u7ba1\u7406\u6210\u672c\u548c\u5904\u7406\u54cd\u5e94\u63d0\u4f9b\u4e86\u4e00\u81f4\u7684 API\u3002</p>"},{"location":"zh/modules/llm/#\u652f\u6301\u7684-llm-\u63d0\u4f9b\u5546","title":"\u652f\u6301\u7684 LLM \u63d0\u4f9b\u5546","text":"<p>EvoAgentX \u76ee\u524d\u652f\u6301\u4ee5\u4e0b LLM \u63d0\u4f9b\u5546\uff1a</p>"},{"location":"zh/modules/llm/#openaillm","title":"OpenAILLM","text":"<p>\u8fd9\u662f\u8bbf\u95ee OpenAI \u8bed\u8a00\u6a21\u578b\u7684\u4e3b\u8981\u5b9e\u73b0\u3002\u5b83\u5904\u7406 GPT-4\u3001GPT-3.5-Turbo \u548c\u5176\u4ed6 OpenAI \u6a21\u578b\u7684\u8ba4\u8bc1\u3001\u8bf7\u6c42\u683c\u5f0f\u5316\u548c\u54cd\u5e94\u89e3\u6790\u3002</p> <p>\u57fa\u672c\u7528\u6cd5\uff1a</p> <pre><code>from evoagentx.models import OpenAILLMConfig, OpenAILLM\n\n# Configure the model\nconfig = OpenAILLMConfig(\n    model=\"gpt-4o-mini\",  \n    openai_key=\"your-api-key\",\n    temperature=0.7,\n    max_tokens=1000\n)\n\n# Initialize the model\nllm = OpenAILLM(config=config)\n\n# Generate text\nresponse = llm.generate(\n    prompt=\"Explain quantum computing in simple terms.\",\n    system_message=\"You are a helpful assistant that explains complex topics simply.\"\n)\n</code></pre>"},{"location":"zh/modules/llm/#litellm","title":"LiteLLM","text":"<p>LiteLLM \u662f LiteLLM \u9879\u76ee \u7684\u9002\u914d\u5668\uff0c\u8be5\u9879\u76ee\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684 Python SDK \u548c\u4ee3\u7406\u670d\u52a1\u5668\uff0c\u7528\u4e8e\u4f7f\u7528 OpenAI API \u683c\u5f0f\u8c03\u7528\u8d85\u8fc7 100 \u4e2a LLM API\u3002\u5b83\u652f\u6301 Bedrock\u3001Azure\u3001OpenAI\u3001VertexAI\u3001Cohere\u3001Anthropic\u3001Sagemaker\u3001HuggingFace\u3001Replicate \u548c Groq \u7b49\u63d0\u4f9b\u5546\u3002\u591a\u4e8f\u4e86\u8fd9\u4e2a\u9879\u76ee\uff0cEvoAgentX \u4e2d\u7684 <code>LiteLLM</code> \u6a21\u578b\u7c7b\u53ef\u4ee5\u901a\u8fc7\u5355\u4e00\u63a5\u53e3\u65e0\u7f1d\u8bbf\u95ee\u5404\u79cd LLM \u63d0\u4f9b\u5546\u3002</p> <p>\u57fa\u672c\u7528\u6cd5\uff1a</p> <p>\u4e3a\u4e86\u4e0e LiteLLM \u65e0\u7f1d\u96c6\u6210\uff0c\u60a8\u5e94\u8be5\u4f7f\u7528 LiteLLM \u5e73\u53f0\u5b9a\u4e49\u7684\u547d\u540d\u7ea6\u5b9a\u6765\u6307\u5b9a\u6a21\u578b\u540d\u79f0\u3002\u4f8b\u5982\uff0c\u5bf9\u4e8e Claude 3.0 Opus\uff0c\u60a8\u9700\u8981\u6307\u5b9a <code>anthropic/claude-3-opus-20240229</code>\u3002\u60a8\u53ef\u4ee5\u5728\u5176\u5b98\u65b9\u6587\u6863\u4e2d\u627e\u5230\u652f\u6301\u7684\u63d0\u4f9b\u5546\u548c\u6a21\u578b\u540d\u79f0\u7684\u5b8c\u6574\u5217\u8868\uff1ahttps://docs.litellm.ai/docs/providers\u3002</p> <pre><code>from evoagentx.models import LiteLLMConfig, LiteLLM\n\n# Configure the model\nconfig = LiteLLMConfig(\n    model=\"anthropic/claude-3-opus-20240229\", \n    anthropic_key=\"your-anthropic-api-key\",\n    temperature=0.7,\n    max_tokens=1000\n)\n\n# Initialize the model\nllm = LiteLLM(config=config)\n\n# Generate text\nresponse = llm.generate(\n    prompt=\"Design a system for autonomous vehicles.\",\n    system_message=\"You are an expert in autonomous systems design.\"\n)\n</code></pre>"},{"location":"zh/modules/llm/#siliconflowllm","title":"SiliconFlowLLM","text":"<p>SiliconFlowLLM \u662f SiliconFlow \u5e73\u53f0 \u4e0a\u6258\u7ba1\u6a21\u578b\u7684\u9002\u914d\u5668\uff0c\u8be5\u5e73\u53f0\u901a\u8fc7 OpenAI \u517c\u5bb9\u7684 API \u63d0\u4f9b\u5bf9\u5f00\u6e90\u548c\u4e13\u6709\u6a21\u578b\u7684\u8bbf\u95ee\u3002\u5b83\u4f7f\u60a8\u80fd\u591f\u901a\u8fc7\u4f7f\u7528 SiliconFlow \u5e73\u53f0\u7684\u547d\u540d\u7ea6\u5b9a\u6307\u5b9a\u6a21\u578b\u540d\u79f0\u6765\u96c6\u6210 Qwen\u3001DeepSeek \u6216 Mixtral \u7b49\u6a21\u578b\u3002</p> <p>\u5f97\u76ca\u4e8e SiliconFlow \u7684\u7edf\u4e00\u63a5\u53e3\uff0cEvoAgentX \u4e2d\u7684 <code>SiliconFlowLLM</code> \u6a21\u578b\u7c7b\u5141\u8bb8\u4f7f\u7528\u76f8\u540c\u7684 API \u683c\u5f0f\u5728 SiliconFlow \u4e0a\u6258\u7ba1\u7684\u5404\u79cd\u5f3a\u5927 LLM \u4e4b\u95f4\u65e0\u7f1d\u5207\u6362\u3002</p> <p>\u57fa\u672c\u7528\u6cd5\uff1a</p> <pre><code>from evoagentx.models import SiliconFlowConfig, SiliconFlowLLM\n\n# Configure the model\nconfig = SiliconFlowConfig(\n    model=\"deepseek-ai/DeepSeek-V3\",\n    siliconflow_key=\"your-siliconflow-api-key\",\n    temperature=0.7,\n    max_tokens=1000\n)\n\n# Initialize the model\nllm = SiliconFlowLLM(config=config)\n\n# Generate text\nresponse = llm.generate(\n    prompt=\"Write a poem about artificial intelligence.\",\n    system_message=\"You are a creative poet.\"\n)\n</code></pre>"},{"location":"zh/modules/llm/#\u6838\u5fc3\u529f\u80fd","title":"\u6838\u5fc3\u529f\u80fd","text":"<p>EvoAgentX \u4e2d\u7684\u6240\u6709 LLM \u5b9e\u73b0\u90fd\u63d0\u4f9b\u4e86\u4e00\u7ec4\u4e00\u81f4\u7684\u6838\u5fc3\u529f\u80fd\uff0c\u7528\u4e8e\u751f\u6210\u6587\u672c\u548c\u7ba1\u7406\u751f\u6210\u8fc7\u7a0b\u3002</p>"},{"location":"zh/modules/llm/#generate-\u51fd\u6570","title":"Generate \u51fd\u6570","text":"<p><code>generate</code> \u51fd\u6570\u662f\u4f7f\u7528\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6587\u672c\u7684\u4e3b\u8981\u65b9\u6cd5\uff1a</p> <pre><code>def generate(\n    self,\n    prompt: Optional[Union[str, List[str]]] = None,\n    system_message: Optional[Union[str, List[str]]] = None,\n    messages: Optional[Union[List[dict],List[List[dict]]]] = None,\n    parser: Optional[Type[LLMOutputParser]] = None,\n    parse_mode: Optional[str] = \"json\", \n    parse_func: Optional[Callable] = None,\n    **kwargs\n) -&gt; Union[LLMOutputParser, List[LLMOutputParser]]:\n    \"\"\"\n    Generate text based on the prompt and optional system message.\n\n    Args:\n        prompt: Input prompt(s) to the LLM.\n        system_message: System message(s) for the LLM.\n        messages: Chat message(s) for the LLM, already in the required format (either `prompt` or `messages` must be provided).\n        parser: Parser class to use for processing the output into a structured format.\n        parse_mode: The mode to use for parsing, must be the `parse_mode` supported by the `parser`. \n        parse_func: A function to apply to the parsed output.\n        **kwargs: Additional generation configuration parameters.\n\n    Returns:\n        For single generation: An LLMOutputParser instance.\n        For batch generation: A list of LLMOutputParser instances.\n    \"\"\"\n</code></pre>"},{"location":"zh/modules/llm/#\u8f93\u5165\u65b9\u5f0f","title":"\u8f93\u5165\u65b9\u5f0f","text":"<p>\u5728 EvoAgentX \u4e2d\uff0c\u6709\u51e0\u79cd\u4f7f\u7528 <code>generate</code> \u51fd\u6570\u5411 LLM \u63d0\u4f9b\u8f93\u5165\u7684\u65b9\u5f0f\uff1a</p> <p>\u65b9\u6cd5 1\uff1a\u63d0\u793a\u548c\u7cfb\u7edf\u6d88\u606f</p> <ol> <li> <p>\u63d0\u793a\uff08Prompt\uff09\uff1a\u60a8\u60f3\u8981\u83b7\u5f97\u54cd\u5e94\u7684\u5177\u4f53\u67e5\u8be2\u6216\u6307\u4ee4\u3002</p> </li> <li> <p>\u7cfb\u7edf\u6d88\u606f\uff08System Message\uff09\uff08\u53ef\u9009\uff09\uff1a\u6307\u5bfc\u6a21\u578b\u6574\u4f53\u884c\u4e3a\u548c\u89d2\u8272\u7684\u6307\u4ee4\u3002\u8fd9\u4e3a\u6a21\u578b\u5e94\u8be5\u5982\u4f55\u54cd\u5e94\u8bbe\u7f6e\u4e86\u4e0a\u4e0b\u6587\u3002</p> </li> </ol> <p>\u8fd9\u4e9b\u7ec4\u4ef6\u88ab\u8f6c\u6362\u4e3a\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u7406\u89e3\u7684\u6807\u51c6\u6d88\u606f\u683c\u5f0f\uff1a</p> <pre><code># Simple example using prompt and system message\nresponse = llm.generate(\n    prompt=\"What are three ways to improve productivity?\",\n    system_message=\"You are a productivity expert providing concise, actionable advice.\"\n)\n</code></pre> <p>\u5728\u540e\u53f0\uff0c\u8fd9\u88ab\u8f6c\u6362\u4e3a\u5177\u6709\u9002\u5f53\u89d2\u8272\u7684\u6d88\u606f\uff1a</p> <pre><code>messages = [\n    {\"role\": \"system\", \"content\": \"You are a productivity expert providing concise, actionable advice.\"},\n    {\"role\": \"user\", \"content\": \"What are three ways to improve productivity?\"}\n]\n</code></pre> <p>\u65b9\u6cd5 2\uff1a\u76f4\u63a5\u4f7f\u7528\u6d88\u606f</p> <p>\u5bf9\u4e8e\u66f4\u590d\u6742\u7684\u5bf9\u8bdd\u6216\u5f53\u60a8\u9700\u8981\u7cbe\u786e\u63a7\u5236\u6d88\u606f\u683c\u5f0f\u65f6\uff0c\u60a8\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 <code>messages</code> \u53c2\u6570\uff1a</p> <pre><code># Direct use of messages for multi-turn conversation\nresponse = llm.generate(\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello, who are you?\"},\n        {\"role\": \"assistant\", \"content\": \"I'm an AI assistant designed to help with various tasks.\"},\n        {\"role\": \"user\", \"content\": \"Can you help me with programming?\"}\n    ]\n)\n</code></pre>"},{"location":"zh/modules/llm/#\u6279\u91cf\u751f\u6210","title":"\u6279\u91cf\u751f\u6210","text":"<p>\u5bf9\u4e8e\u6279\u91cf\u5904\u7406\uff0c\u60a8\u53ef\u4ee5\u63d0\u4f9b\u63d0\u793a/\u7cfb\u7edf\u6d88\u606f\u5217\u8868\u6216\u6d88\u606f\u5217\u8868\u3002\u4f8b\u5982\uff1a</p> <pre><code># Batch processing example\nresponses = llm.generate(\n    prompt=[\"What is machine learning?\", \"Explain neural networks.\"],\n    system_message=[\"You are a data scientist.\", \"You are an AI researcher.\"]\n)\n</code></pre>"},{"location":"zh/modules/llm/#\u8f93\u51fa\u89e3\u6790","title":"\u8f93\u51fa\u89e3\u6790","text":"<p><code>generate</code> \u51fd\u6570\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u9009\u9879\u6765\u89e3\u6790\u548c\u7ed3\u6784\u5316\u6765\u81ea\u8bed\u8a00\u6a21\u578b\u7684\u539f\u59cb\u6587\u672c\u8f93\u51fa\uff1a</p> <ul> <li>parser\uff1a\u63a5\u53d7\u4e00\u4e2a\u7c7b\uff08\u901a\u5e38\u7ee7\u627f\u81ea <code>LLMOutputParser/ActionOutput</code>\uff09\uff0c\u8be5\u7c7b\u5b9a\u4e49\u4e86\u89e3\u6790\u8f93\u51fa\u7684\u7ed3\u6784\u3002\u5982\u679c\u672a\u63d0\u4f9b\uff0cLLM \u8f93\u51fa\u5c06\u4e0d\u4f1a\u88ab\u89e3\u6790\u3002\u5728\u8fd9\u4e24\u79cd\u60c5\u51b5\u4e0b\uff0c\u90fd\u53ef\u4ee5\u901a\u8fc7\u8fd4\u56de\u5bf9\u8c61\u7684 <code>.content</code> \u5c5e\u6027\u8bbf\u95ee\u539f\u59cb LLM \u8f93\u51fa\u3002</li> <li>parse_mode\uff1a\u786e\u5b9a\u5982\u4f55\u5c06\u539f\u59cb LLM \u8f93\u51fa\u89e3\u6790\u4e3a\u89e3\u6790\u5668\u5b9a\u4e49\u7684\u7ed3\u6784\uff0c\u6709\u6548\u9009\u9879\u4e3a\uff1a<code>'str'</code>\u3001<code>'json'</code>\uff08\u9ed8\u8ba4\uff09\u3001<code>'xml'</code>\u3001<code>'title'</code>\u3001<code>'custom'</code>\u3002</li> <li>parse_func\uff1a\u7528\u4e8e\u5728\u66f4\u590d\u6742\u7684\u573a\u666f\u4e2d\u5904\u7406\u89e3\u6790\u7684\u81ea\u5b9a\u4e49\u51fd\u6570\uff0c\u4ec5\u5728 <code>parse_mode</code> \u4e3a <code>'custom'</code> \u65f6\u4f7f\u7528\u3002</li> </ul> <p>\u7ed3\u6784\u5316\u8f93\u51fa\u793a\u4f8b\uff1a <pre><code>from evoagentx.models import LLMOutputParser \nfrom pydantic import Field\n\nclass CodeWriterOutput(LLMOutputParser):\n    thought: str = Field(description=\"Thought process for writing the code\") \n    code: str = Field(description=\"The generated code\")\n\nprompt = \"\"\"\nWrite a Python function to calculate Fibonacci numbers. \n\nYour output should always be in the following format:\n\n## thought \n[Your thought process for writing the code]\n\n## code\n[The generated code]\n\"\"\"\nresponse = llm.generate(\n    prompt=prompt,\n    parser=CodeWriterOutput,\n    parse_mode=\"title\"\n)\n\nprint(\"Thought:\\n\", response.thought)\nprint(\"Code:\\n\", response.code)\n</code></pre></p>"},{"location":"zh/modules/llm/#\u89e3\u6790\u6a21\u5f0f","title":"\u89e3\u6790\u6a21\u5f0f","text":"<p>EvoAgentX \u652f\u6301\u51e0\u79cd\u89e3\u6790\u7b56\u7565\uff1a</p> <ol> <li>\"str\"\uff1a\u76f4\u63a5\u4f7f\u7528\u539f\u59cb\u8f93\u51fa\u4f5c\u4e3a\u89e3\u6790\u5668\u4e2d\u5b9a\u4e49\u7684\u6bcf\u4e2a\u5b57\u6bb5\u3002</li> <li>\"json\"\uff08\u9ed8\u8ba4\uff09\uff1a\u4ece\u8f93\u51fa\u4e2d\u7684 JSON \u5b57\u7b26\u4e32\u63d0\u53d6\u5b57\u6bb5\u3002</li> <li>\"xml\"\uff1a\u4ece\u4e0e\u5b57\u6bb5\u540d\u79f0\u5339\u914d\u7684 XML \u6807\u7b7e\u4e2d\u63d0\u53d6\u5185\u5bb9\u3002</li> <li>\"title\"\uff1a\u4ece markdown \u7ae0\u8282\u4e2d\u63d0\u53d6\u5185\u5bb9\uff08\u9ed8\u8ba4\u683c\u5f0f\uff1a\"## {title}\"\uff09\u3002</li> <li>\"custom\"\uff1a\u4f7f\u7528\u7531 <code>parse_func</code> \u6307\u5b9a\u7684\u81ea\u5b9a\u4e49\u89e3\u6790\u51fd\u6570\u3002</li> </ol> <p>Note</p> <p>\u5bf9\u4e8e <code>'json'</code>\u3001<code>'xml'</code> \u548c <code>'title'</code>\uff0c\u60a8\u5e94\u8be5\u901a\u8fc7 <code>prompt</code> \u6307\u793a LLM \u4ee5\u53ef\u4ee5\u88ab\u89e3\u6790\u5668\u89e3\u6790\u7684\u6307\u5b9a\u683c\u5f0f\u8f93\u51fa\u5185\u5bb9\u3002\u5426\u5219\uff0c\u89e3\u6790\u5c06\u5931\u8d25\u3002</p> <ol> <li> <p>\u5bf9\u4e8e <code>'json'</code>\uff0c\u60a8\u5e94\u8be5\u6307\u793a LLM \u8f93\u51fa\u4e00\u4e2a\u5305\u542b\u4e0e\u89e3\u6790\u5668\u7c7b\u4e2d\u7684\u5b57\u6bb5\u540d\u79f0\u5339\u914d\u7684\u952e\u7684\u6709\u6548 JSON \u5b57\u7b26\u4e32\u3002\u5982\u679c\u539f\u59cb LLM \u8f93\u51fa\u4e2d\u6709\u591a\u4e2a JSON \u5b57\u7b26\u4e32\uff0c\u53ea\u4f1a\u89e3\u6790\u7b2c\u4e00\u4e2a\u3002</p> </li> <li> <p>\u5bf9\u4e8e <code>xml</code>\uff0c\u60a8\u5e94\u8be5\u6307\u793a LLM \u8f93\u51fa\u5305\u542b\u4e0e\u89e3\u6790\u5668\u7c7b\u4e2d\u7684\u5b57\u6bb5\u540d\u79f0\u5339\u914d\u7684 XML \u6807\u7b7e\u7684\u5185\u5bb9\uff0c\u4f8b\u5982 <code>&lt;{field_name}&gt;...&lt;/{field_name}&gt;</code>\u3002\u5982\u679c\u6709\u591a\u4e2a\u5177\u6709\u76f8\u540c\u5b57\u6bb5\u540d\u79f0\u7684 XML \u6807\u7b7e\uff0c\u53ea\u4f1a\u4f7f\u7528\u7b2c\u4e00\u4e2a\u3002</p> </li> <li> <p>\u5bf9\u4e8e <code>title</code>\uff0c\u60a8\u5e94\u8be5\u6307\u793a LLM \u8f93\u51fa\u5305\u542b\u6807\u9898\u4e0e\u89e3\u6790\u5668\u7c7b\u4e2d\u7684\u5b57\u6bb5\u540d\u79f0\u5b8c\u5168\u5339\u914d\u7684 markdown \u7ae0\u8282\u7684\u5185\u5bb9\u3002\u9ed8\u8ba4\u6807\u9898\u683c\u5f0f\u662f \"## {title}\"\u3002\u60a8\u53ef\u4ee5\u901a\u8fc7\u5728 <code>generate</code> \u51fd\u6570\u4e2d\u8bbe\u7f6e <code>title_format</code> \u53c2\u6570\u6765\u66f4\u6539\u5b83\uff0c\u4f8b\u5982 <code>generate(..., title_format=\"### {title}\")</code>\u3002<code>title_format</code> \u5fc5\u987b\u5305\u542b <code>{title}</code> \u4f5c\u4e3a\u5b57\u6bb5\u540d\u79f0\u7684\u5360\u4f4d\u7b26\u3002</p> </li> </ol>"},{"location":"zh/modules/llm/#\u81ea\u5b9a\u4e49\u89e3\u6790\u51fd\u6570","title":"\u81ea\u5b9a\u4e49\u89e3\u6790\u51fd\u6570","text":"<p>\u4e3a\u4e86\u83b7\u5f97\u6700\u5927\u7684\u7075\u6d3b\u6027\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528 <code>parse_func</code> \u5b9a\u4e49\u81ea\u5b9a\u4e49\u89e3\u6790\u51fd\u6570\uff1a</p> <pre><code>from evoagentx.models import LLMOutputParser\nfrom evoagentx.core.module_utils import extract_code_block\n\nclass CodeOutput(LLMOutputParser):\n    code: str = Field(description=\"The generated code\")\n\n# Use custom parsing\nresponse = llm.generate(\n    prompt=\"Write a Python function to calculate Fibonacci numbers.\",\n    parser=CodeOutput,\n    parse_mode=\"custom\",\n    parse_func=lambda content: {\"code\": extract_code_block(content)[0]}\n)\n</code></pre> <p>Note</p> <p>\u89e3\u6790\u51fd\u6570\u5e94\u8be5\u6709\u4e00\u4e2a\u63a5\u6536\u539f\u59cb LLM \u8f93\u51fa\u7684\u8f93\u5165\u53c2\u6570 <code>content</code>\uff0c\u5e76\u8fd4\u56de\u4e00\u4e2a\u5b57\u5178\uff0c\u5176\u952e\u4e0e\u89e3\u6790\u5668\u7c7b\u4e2d\u7684\u5b57\u6bb5\u540d\u79f0\u5339\u914d\u3002</p>"},{"location":"zh/modules/llm/#\u5f02\u6b65\u751f\u6210\u51fd\u6570","title":"\u5f02\u6b65\u751f\u6210\u51fd\u6570","text":"<p>\u5bf9\u4e8e\u9700\u8981\u5f02\u6b65\u64cd\u4f5c\u7684\u5e94\u7528\u7a0b\u5e8f\uff0c<code>async_generate</code> \u51fd\u6570\u63d0\u4f9b\u4e86\u4e0e <code>generate</code> \u51fd\u6570\u76f8\u540c\u7684\u529f\u80fd\uff0c\u4f46\u4ee5\u975e\u963b\u585e\u65b9\u5f0f\u8fd0\u884c\uff1a</p> <pre><code>async def async_generate(\n        self,\n        prompt: Optional[Union[str, List[str]]] = None,\n        system_message: Optional[Union[str, List[str]]] = None,\n        messages: Optional[Union[List[dict],List[List[dict]]]] = None,\n        parser: Optional[Type[LLMOutputParser]] = None,\n        parse_mode: Optional[str] = \"json\", \n        parse_func: Optional[Callable] = None,\n        **kwargs\n    ) -&gt; Union[LLMOutputParser, List[LLMOutputParser]]:\n    \"\"\"\n    \u57fa\u4e8e\u63d0\u793a\u548c\u53ef\u9009\u7684\u7cfb\u7edf\u6d88\u606f\u5f02\u6b65\u751f\u6210\u6587\u672c\u3002\n\n    \u53c2\u6570\uff1a\n        prompt: \u8f93\u5165\u5230 LLM \u7684\u63d0\u793a\u3002\n        system_message: LLM \u7684\u7cfb\u7edf\u6d88\u606f\u3002\n        messages: LLM \u7684\u804a\u5929\u6d88\u606f\uff0c\u5df2\u7ecf\u662f\u6240\u9700\u683c\u5f0f\uff08\u5fc5\u987b\u63d0\u4f9b `prompt` \u6216 `messages` \u4e4b\u4e00\uff09\u3002\n        parser: \u7528\u4e8e\u5c06\u8f93\u51fa\u5904\u7406\u4e3a\u7ed3\u6784\u5316\u683c\u5f0f\u7684\u89e3\u6790\u5668\u7c7b\u3002\n        parse_mode: \u7528\u4e8e\u89e3\u6790\u7684\u6a21\u5f0f\uff0c\u5fc5\u987b\u662f `parser` \u652f\u6301\u7684 `parse_mode`\u3002\n        parse_func: \u5e94\u7528\u4e8e\u89e3\u6790\u8f93\u51fa\u7684\u51fd\u6570\u3002\n        **kwargs: \u989d\u5916\u7684\u751f\u6210\u914d\u7f6e\u53c2\u6570\u3002\n\n    \u8fd4\u56de\uff1a\n        \u5355\u6b21\u751f\u6210\uff1a\u4e00\u4e2a LLMOutputParser \u5b9e\u4f8b\u3002\n        \u6279\u91cf\u751f\u6210\uff1aLLMOutputParser \u5b9e\u4f8b\u5217\u8868\u3002\n    \"\"\"\n</code></pre>"},{"location":"zh/modules/llm/#\u6d41\u5f0f\u54cd\u5e94","title":"\u6d41\u5f0f\u54cd\u5e94","text":"<p>EvoAgentX \u652f\u6301\u6765\u81ea LLM \u7684\u6d41\u5f0f\u54cd\u5e94\uff0c\u8fd9\u4f7f\u60a8\u53ef\u4ee5\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u9010\u4ee4\u724c\u67e5\u770b\u6a21\u578b\u7684\u8f93\u51fa\uff0c\u800c\u4e0d\u662f\u7b49\u5f85\u5b8c\u6574\u54cd\u5e94\u3002\u8fd9\u5bf9\u4e8e\u751f\u6210\u957f\u7bc7\u5185\u5bb9\u6216\u63d0\u4f9b\u66f4\u4ea4\u4e92\u5f0f\u7684\u4f53\u9a8c\u7279\u522b\u6709\u7528\u3002</p> <p>\u6709\u4e24\u79cd\u542f\u7528\u6d41\u5f0f\u4f20\u8f93\u7684\u65b9\u5f0f\uff1a</p>"},{"location":"zh/modules/llm/#\u5728-llm-\u914d\u7f6e\u4e2d\u914d\u7f6e\u6d41\u5f0f\u4f20\u8f93","title":"\u5728 LLM \u914d\u7f6e\u4e2d\u914d\u7f6e\u6d41\u5f0f\u4f20\u8f93","text":"<p>\u60a8\u53ef\u4ee5\u5728\u521d\u59cb\u5316 LLM \u65f6\u901a\u8fc7\u8bbe\u7f6e\u914d\u7f6e\u4e2d\u7684\u9002\u5f53\u53c2\u6570\u6765\u542f\u7528\u6d41\u5f0f\u4f20\u8f93\uff1a</p> <pre><code># \u5728\u521d\u59cb\u5316\u65f6\u542f\u7528\u6d41\u5f0f\u4f20\u8f93\nconfig = OpenAILLMConfig(\n    model=\"gpt-4o-mini\",\n    openai_key=\"your-api-key\",\n    stream=True,  # \u542f\u7528\u6d41\u5f0f\u4f20\u8f93\n    output_response=True  # \u5b9e\u65f6\u5c06\u4ee4\u724c\u6253\u5370\u5230\u63a7\u5236\u53f0\n)\n\nllm = OpenAILLM(config=config)\n\n# \u73b0\u5728\u6240\u6709\u5bf9 generate() \u7684\u8c03\u7528\u90fd\u5c06\u9ed8\u8ba4\u4f7f\u7528\u6d41\u5f0f\u4f20\u8f93\nresponse = llm.generate(\n    prompt=\"Write a story about space exploration.\"\n)\n</code></pre>"},{"location":"zh/modules/llm/#\u5728\u751f\u6210\u65b9\u6cd5\u4e2d\u542f\u7528\u6d41\u5f0f\u4f20\u8f93","title":"\u5728\u751f\u6210\u65b9\u6cd5\u4e2d\u542f\u7528\u6d41\u5f0f\u4f20\u8f93","text":"<p>\u6216\u8005\uff0c\u60a8\u53ef\u4ee5\u4e3a\u7279\u5b9a\u7684\u751f\u6210\u8c03\u7528\u542f\u7528\u6d41\u5f0f\u4f20\u8f93\uff1a</p> <pre><code># \u4f7f\u7528\u9ed8\u8ba4\u975e\u6d41\u5f0f\u884c\u4e3a\u521d\u59cb\u5316\u7684 LLM\nconfig = OpenAILLMConfig(\n    model=\"gpt-4o-mini\",\n    openai_key=\"your-api-key\"\n)\n\nllm = OpenAILLM(config=config)\n\n# \u4ec5\u4e3a\u6b64\u7279\u5b9a\u8c03\u7528\u8986\u76d6\u8bbe\u7f6e\nresponse = llm.generate(\n    prompt=\"Write a story about space exploration.\",\n    stream=True,  # \u4ec5\u4e3a\u6b64\u8c03\u7528\u542f\u7528\u6d41\u5f0f\u4f20\u8f93\n    output_response=True  # \u5b9e\u65f6\u5c06\u4ee4\u724c\u6253\u5370\u5230\u63a7\u5236\u53f0\n)\n</code></pre>"},{"location":"zh/modules/workflow_graph/","title":"\u5de5\u4f5c\u6d41\u56fe","text":""},{"location":"zh/modules/workflow_graph/#\u7b80\u4ecb","title":"\u7b80\u4ecb","text":"<p><code>WorkFlowGraph</code> \u7c7b\u662f EvoAgentX \u6846\u67b6\u4e2d\u7684\u4e00\u4e2a\u57fa\u7840\u7ec4\u4ef6\uff0c\u7528\u4e8e\u521b\u5efa\u3001\u7ba1\u7406\u548c\u6267\u884c\u590d\u6742\u7684 AI \u4ee3\u7406\u5de5\u4f5c\u6d41\u3002\u5b83\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ed3\u6784\u5316\u7684\u65b9\u5f0f\u6765\u5b9a\u4e49\u4efb\u52a1\u4f9d\u8d56\u5173\u7cfb\u3001\u6267\u884c\u987a\u5e8f\u548c\u4efb\u52a1\u4e4b\u95f4\u7684\u6570\u636e\u6d41\u3002</p> <p>\u5de5\u4f5c\u6d41\u56fe\u8868\u793a\u4e00\u7ec4\u4efb\u52a1\uff08\u8282\u70b9\uff09\u53ca\u5176\u4f9d\u8d56\u5173\u7cfb\uff08\u8fb9\uff09\uff0c\u8fd9\u4e9b\u4efb\u52a1\u9700\u8981\u6309\u7279\u5b9a\u987a\u5e8f\u6267\u884c\u4ee5\u5b9e\u73b0\u76ee\u6807\u3002<code>SequentialWorkFlowGraph</code> \u662f\u4e00\u4e2a\u4e13\u95e8\u7684\u5b9e\u73b0\uff0c\u4e13\u6ce8\u4e8e\u4ece\u5f00\u59cb\u5230\u7ed3\u675f\u7684\u5355\u4e00\u8def\u5f84\u7684\u7ebf\u6027\u5de5\u4f5c\u6d41\u3002</p>"},{"location":"zh/modules/workflow_graph/#\u67b6\u6784","title":"\u67b6\u6784","text":""},{"location":"zh/modules/workflow_graph/#\u5de5\u4f5c\u6d41\u56fe\u67b6\u6784","title":"\u5de5\u4f5c\u6d41\u56fe\u67b6\u6784","text":"<p><code>WorkFlowGraph</code> \u7531\u51e0\u4e2a\u5173\u952e\u7ec4\u4ef6\u7ec4\u6210\uff1a</p> <ol> <li> <p>\u8282\u70b9\uff08WorkFlowNode\uff09\uff1a</p> <p>\u6bcf\u4e2a\u8282\u70b9\u4ee3\u8868\u5de5\u4f5c\u6d41\u4e2d\u7684\u4e00\u4e2a\u4efb\u52a1\u6216\u64cd\u4f5c\uff0c\u5177\u6709\u4ee5\u4e0b\u5c5e\u6027\uff1a</p> <ul> <li><code>name</code>\uff1a\u4efb\u52a1\u7684\u552f\u4e00\u6807\u8bc6\u7b26</li> <li><code>description</code>\uff1a\u4efb\u52a1\u529f\u80fd\u7684\u8be6\u7ec6\u63cf\u8ff0</li> <li><code>inputs</code>\uff1a\u4efb\u52a1\u6240\u9700\u7684\u8f93\u5165\u53c2\u6570\u5217\u8868\uff0c\u6bcf\u4e2a\u8f93\u5165\u53c2\u6570\u662f <code>Parameter</code> \u7c7b\u7684\u5b9e\u4f8b</li> <li><code>outputs</code>\uff1a\u4efb\u52a1\u4ea7\u751f\u7684\u8f93\u51fa\u53c2\u6570\u5217\u8868\uff0c\u6bcf\u4e2a\u8f93\u51fa\u53c2\u6570\u662f <code>Parameter</code> \u7c7b\u7684\u5b9e\u4f8b</li> <li><code>agents</code>\uff08\u53ef\u9009\uff09\uff1a\u53ef\u4ee5\u6267\u884c\u6b64\u4efb\u52a1\u7684\u4ee3\u7406\u5217\u8868\uff0c\u6bcf\u4e2a\u4ee3\u7406\u5e94\u8be5\u662f\u4e00\u4e2a\u4e0e <code>agent_manager</code> \u4e2d\u4ee3\u7406\u540d\u79f0\u5339\u914d\u7684\u5b57\u7b26\u4e32\uff0c\u6216\u8005\u662f\u4e00\u4e2a\u6307\u5b9a\u4ee3\u7406\u540d\u79f0\u548c\u914d\u7f6e\u7684\u5b57\u5178\uff0c\u8be5\u914d\u7f6e\u5c06\u7528\u4e8e\u5728 <code>agent_manager</code> \u4e2d\u521b\u5efa <code>CustomizeAgent</code> \u5b9e\u4f8b\u3002\u6709\u5173\u4ee3\u7406\u914d\u7f6e\u7684\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u81ea\u5b9a\u4e49\u4ee3\u7406\u6587\u6863\u3002</li> <li><code>action_graph</code>\uff08\u53ef\u9009\uff09\uff1a<code>ActionGraph</code> \u7c7b\u7684\u5b9e\u4f8b\uff0c\u5176\u4e2d\u6bcf\u4e2a\u52a8\u4f5c\u90fd\u662f <code>Operator</code> \u7c7b\u7684\u5b9e\u4f8b\u3002\u6709\u5173\u52a8\u4f5c\u56fe\u7684\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u52a8\u4f5c\u56fe\u6587\u6863\u3002</li> <li><code>status</code>\uff1a\u4efb\u52a1\u7684\u5f53\u524d\u6267\u884c\u72b6\u6001\uff08PENDING\u3001RUNNING\u3001COMPLETED\u3001FAILED\uff09</li> </ul> <p>Note</p> <ol> <li> <p>\u60a8\u5e94\u8be5\u63d0\u4f9b <code>agents</code> \u6216 <code>action_graph</code> \u6765\u6267\u884c\u4efb\u52a1\u3002\u5982\u679c\u4e24\u8005\u90fd\u63d0\u4f9b\uff0c\u5c06\u4f7f\u7528 <code>action_graph</code>\u3002</p> </li> <li> <p>\u5982\u679c\u60a8\u63d0\u4f9b\u4e00\u7ec4 <code>agents</code>\uff0c\u8fd9\u4e9b\u4ee3\u7406\u5c06\u534f\u540c\u5de5\u4f5c\u4ee5\u5b8c\u6210\u4efb\u52a1\u3002\u4f7f\u7528 <code>WorkFlow</code> \u6267\u884c\u4efb\u52a1\u65f6\uff0c\u7cfb\u7edf\u5c06\u6839\u636e\u4ee3\u7406\u4fe1\u606f\u548c\u6267\u884c\u5386\u53f2\u81ea\u52a8\u786e\u5b9a\u6267\u884c\u987a\u5e8f\uff08\u52a8\u4f5c\uff09\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6267\u884c\u4efb\u52a1\u65f6\uff0c<code>WorkFlow</code> \u5c06\u5206\u6790\u8fd9\u4e9b\u4ee3\u7406\u4e2d\u7684\u6240\u6709\u53ef\u80fd\u52a8\u4f5c\uff0c\u5e76\u6839\u636e\u4efb\u52a1\u63cf\u8ff0\u548c\u6267\u884c\u5386\u53f2\u91cd\u590d\u9009\u62e9\u6700\u4f73\u52a8\u4f5c\u6267\u884c\u3002</p> </li> <li> <p>\u5982\u679c\u60a8\u63d0\u4f9b <code>action_graph</code>\uff0c\u5b83\u5c06\u76f4\u63a5\u7528\u4e8e\u5b8c\u6210\u4efb\u52a1\u3002\u4f7f\u7528 <code>WorkFlow</code> \u6267\u884c\u4efb\u52a1\u65f6\uff0c\u7cfb\u7edf\u5c06\u6309\u7167 <code>action_graph</code> \u5b9a\u4e49\u7684\u987a\u5e8f\u6267\u884c\u52a8\u4f5c\u5e76\u8fd4\u56de\u7ed3\u679c\u3002</p> </li> </ol> </li> <li> <p>\u8fb9\uff08WorkFlowEdge\uff09\uff1a</p> <p>\u8fb9\u8868\u793a\u4efb\u52a1\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u5b9a\u4e49\u6267\u884c\u987a\u5e8f\u548c\u6570\u636e\u6d41\u3002\u6bcf\u6761\u8fb9\u5177\u6709\uff1a</p> <ul> <li><code>source</code>\uff1a\u6e90\u8282\u70b9\u540d\u79f0\uff08\u8fb9\u7684\u8d77\u70b9\uff09</li> <li><code>target</code>\uff1a\u76ee\u6807\u8282\u70b9\u540d\u79f0\uff08\u8fb9\u7684\u7ec8\u70b9\uff09</li> <li><code>priority</code>\uff08\u53ef\u9009\uff09\uff1a\u5f71\u54cd\u6267\u884c\u987a\u5e8f\u7684\u6570\u503c\u4f18\u5148\u7ea7</li> </ul> </li> <li> <p>\u56fe\u7ed3\u6784\uff1a</p> <p>\u5728\u5185\u90e8\uff0c\u5de5\u4f5c\u6d41\u8868\u793a\u4e3a\u6709\u5411\u56fe\uff0c\u5176\u4e2d\uff1a</p> <ul> <li>\u8282\u70b9\u8868\u793a\u4efb\u52a1</li> <li>\u8fb9\u8868\u793a\u4efb\u52a1\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u548c\u6570\u636e\u6d41</li> <li>\u56fe\u7ed3\u6784\u652f\u6301\u7ebf\u6027\u5e8f\u5217\u548c\u66f4\u590d\u6742\u7684\u6a21\u5f0f\uff1a<ul> <li>\u5206\u53c9-\u5408\u5e76\u6a21\u5f0f\uff08\u7a0d\u540e\u91cd\u65b0\u6c47\u5408\u7684\u5e76\u884c\u6267\u884c\u8def\u5f84\uff09</li> <li>\u6761\u4ef6\u5206\u652f</li> <li>\u5de5\u4f5c\u6d41\u4e2d\u6f5c\u5728\u7684\u5faa\u73af</li> </ul> </li> </ul> </li> <li> <p>\u8282\u70b9\u72b6\u6001\uff1a</p> <p>\u5de5\u4f5c\u6d41\u4e2d\u7684\u6bcf\u4e2a\u8282\u70b9\u53ef\u4ee5\u5904\u4e8e\u4ee5\u4e0b\u72b6\u6001\u4e4b\u4e00\uff1a</p> <ul> <li><code>PENDING</code>\uff1a\u4efb\u52a1\u7b49\u5f85\u6267\u884c</li> <li><code>RUNNING</code>\uff1a\u4efb\u52a1\u6b63\u5728\u6267\u884c</li> <li><code>COMPLETED</code>\uff1a\u4efb\u52a1\u5df2\u6210\u529f\u6267\u884c</li> <li><code>FAILED</code>\uff1a\u4efb\u52a1\u6267\u884c\u5931\u8d25</li> </ul> </li> </ol>"},{"location":"zh/modules/workflow_graph/#\u987a\u5e8f\u5de5\u4f5c\u6d41\u56fe\u67b6\u6784","title":"\u987a\u5e8f\u5de5\u4f5c\u6d41\u56fe\u67b6\u6784","text":"<p><code>SequentialWorkFlowGraph</code> \u662f <code>WorkFlowGraph</code> \u7684\u4e13\u95e8\u5b9e\u73b0\uff0c\u5b83\u81ea\u52a8\u63a8\u65ad\u8282\u70b9\u8fde\u63a5\u4ee5\u521b\u5efa\u7ebf\u6027\u5de5\u4f5c\u6d41\u3002\u5b83\u4e13\u4e3a\u66f4\u7b80\u5355\u7684\u7528\u4f8b\u8bbe\u8ba1\uff0c\u5176\u4e2d\u4efb\u52a1\u9700\u8981\u6309\u987a\u5e8f\u6267\u884c\uff0c\u4e00\u4e2a\u4efb\u52a1\u7684\u8f93\u51fa\u4f5c\u4e3a\u4e0b\u4e00\u4e2a\u4efb\u52a1\u7684\u8f93\u5165\u3002</p>"},{"location":"zh/modules/workflow_graph/#\u8f93\u5165\u683c\u5f0f","title":"\u8f93\u5165\u683c\u5f0f","text":"<p><code>SequentialWorkFlowGraph</code> \u63a5\u53d7\u7b80\u5316\u7684\u8f93\u5165\u683c\u5f0f\uff0c\u4f7f\u5b9a\u4e49\u7ebf\u6027\u5de5\u4f5c\u6d41\u53d8\u5f97\u5bb9\u6613\u3002\u60a8\u4e0d\u9700\u8981\u663e\u5f0f\u5b9a\u4e49\u8282\u70b9\u548c\u8fb9\uff0c\u800c\u662f\u6309\u7167\u6267\u884c\u987a\u5e8f\u63d0\u4f9b\u4efb\u52a1\u5217\u8868\u3002\u6bcf\u4e2a\u4efb\u52a1\u90fd\u5b9a\u4e49\u4e3a\u4e00\u4e2a\u5b57\u5178\uff0c\u5305\u542b\u4ee5\u4e0b\u5b57\u6bb5\uff1a</p> <ul> <li><code>name</code>\uff08\u5fc5\u9700\uff09\uff1a\u4efb\u52a1\u7684\u552f\u4e00\u6807\u8bc6\u7b26</li> <li><code>description</code>\uff08\u5fc5\u9700\uff09\uff1a\u4efb\u52a1\u529f\u80fd\u7684\u8be6\u7ec6\u63cf\u8ff0</li> <li><code>inputs</code>\uff08\u5fc5\u9700\uff09\uff1a\u4efb\u52a1\u7684\u8f93\u5165\u53c2\u6570\u5217\u8868</li> <li><code>outputs</code>\uff08\u5fc5\u9700\uff09\uff1a\u4efb\u52a1\u4ea7\u751f\u7684\u8f93\u51fa\u53c2\u6570\u5217\u8868</li> <li><code>prompt</code>\uff08\u5fc5\u9700\uff09\uff1a\u6307\u5bfc\u4ee3\u7406\u884c\u4e3a\u7684\u63d0\u793a\u6a21\u677f</li> <li><code>system_prompt</code>\uff08\u53ef\u9009\uff09\uff1a\u4e3a\u4ee3\u7406\u63d0\u4f9b\u4e0a\u4e0b\u6587\u7684\u7cfb\u7edf\u6d88\u606f</li> <li><code>output_parser</code>\uff08\u53ef\u9009\uff09\uff1a\u7528\u4e8e\u89e3\u6790\u4efb\u52a1\u8f93\u51fa\u7684\u89e3\u6790\u5668</li> <li><code>parse_mode</code>\uff08\u53ef\u9009\uff09\uff1a\u89e3\u6790\u8f93\u51fa\u7684\u6a21\u5f0f\uff0c\u9ed8\u8ba4\u4e3a \"str\"</li> <li><code>parse_func</code>\uff08\u53ef\u9009\uff09\uff1a\u7528\u4e8e\u89e3\u6790\u8f93\u51fa\u7684\u81ea\u5b9a\u4e49\u51fd\u6570</li> <li><code>parse_title</code>\uff08\u53ef\u9009\uff09\uff1a\u89e3\u6790\u8f93\u51fa\u7684\u6807\u9898</li> </ul> <p>\u4e0e\u63d0\u793a\u548c\u89e3\u6790\u76f8\u5173\u7684\u53c2\u6570\u5c06\u7528\u4e8e\u5728 <code>agent_manager</code> \u4e2d\u521b\u5efa <code>CustomizeAgent</code> \u5b9e\u4f8b\u3002\u6709\u5173\u4ee3\u7406\u914d\u7f6e\u7684\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u81ea\u5b9a\u4e49\u4ee3\u7406\u6587\u6863\u3002</p>"},{"location":"zh/modules/workflow_graph/#\u5185\u90e8\u8f6c\u6362\u4e3a\u5de5\u4f5c\u6d41\u56fe","title":"\u5185\u90e8\u8f6c\u6362\u4e3a\u5de5\u4f5c\u6d41\u56fe","text":"<p>\u5728\u5185\u90e8\uff0c<code>SequentialWorkFlowGraph</code> \u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u81ea\u52a8\u5c06\u6b64\u7b80\u5316\u7684\u4efb\u52a1\u5217\u8868\u8f6c\u6362\u4e3a\u5b8c\u6574\u7684 <code>WorkFlowGraph</code>\uff1a</p> <ol> <li> <p>\u521b\u5efa WorkFlowNode \u5b9e\u4f8b\uff1a\u5bf9\u4e8e\u8f93\u5165\u5217\u8868\u4e2d\u7684\u6bcf\u4e2a\u4efb\u52a1\uff0c\u5b83\u521b\u5efa\u4e00\u4e2a\u5177\u6709\u9002\u5f53\u5c5e\u6027\u7684 <code>WorkFlowNode</code>\u3002\u5728\u6b64\u8fc7\u7a0b\u4e2d\uff1a</p> <ul> <li>\u5b83\u5c06\u4efb\u52a1\u5b9a\u4e49\u8f6c\u6362\u4e3a\u5177\u6709\u8f93\u5165\u3001\u8f93\u51fa\u548c\u5173\u8054\u4ee3\u7406\u7684\u8282\u70b9</li> <li>\u5b83\u6839\u636e\u4efb\u52a1\u540d\u79f0\u81ea\u52a8\u751f\u6210\u552f\u4e00\u7684\u4ee3\u7406\u540d\u79f0</li> <li>\u5b83\u4f7f\u7528\u63d0\u4f9b\u7684\u63d0\u793a\u3001\u7cfb\u7edf\u63d0\u793a\u548c\u89e3\u6790\u9009\u9879\u914d\u7f6e\u4ee3\u7406</li> </ul> </li> <li> <p>\u63a8\u65ad\u8fb9\u8fde\u63a5\uff1a\u5b83\u68c0\u67e5\u6bcf\u4e2a\u4efb\u52a1\u7684\u8f93\u5165\u548c\u8f93\u51fa\u53c2\u6570\uff0c\u5e76\u81ea\u52a8\u521b\u5efa <code>WorkFlowEdge</code> \u5b9e\u4f8b\u6765\u8fde\u63a5\u4efb\u52a1\uff0c\u5176\u4e2d\u4e00\u4e2a\u4efb\u52a1\u7684\u8f93\u51fa\u4e0e\u53e6\u4e00\u4e2a\u4efb\u52a1\u7684\u8f93\u5165\u5339\u914d</p> </li> <li> <p>\u6784\u5efa\u56fe\u7ed3\u6784\uff1a\u6700\u540e\uff0c\u5b83\u6784\u5efa\u8868\u793a\u5de5\u4f5c\u6d41\u7684\u5b8c\u6574\u6709\u5411\u56fe\uff0c\u6240\u6709\u8282\u70b9\u548c\u8fb9\u90fd\u6b63\u786e\u8fde\u63a5</p> </li> </ol> <p>\u8fd9\u79cd\u81ea\u52a8\u8f6c\u6362\u8fc7\u7a0b\u4f7f\u5f97\u5b9a\u4e49\u987a\u5e8f\u5de5\u4f5c\u6d41\u53d8\u5f97\u66f4\u5bb9\u6613\uff0c\u65e0\u9700\u624b\u52a8\u6307\u5b9a\u6240\u6709\u56fe\u7ec4\u4ef6\u3002</p>"},{"location":"zh/modules/workflow_graph/#\u4f7f\u7528\u65b9\u6cd5","title":"\u4f7f\u7528\u65b9\u6cd5","text":""},{"location":"zh/modules/workflow_graph/#\u57fa\u672c\u5de5\u4f5c\u6d41\u56fe\u521b\u5efa\u4e0e\u6267\u884c","title":"\u57fa\u672c\u5de5\u4f5c\u6d41\u56fe\u521b\u5efa\u4e0e\u6267\u884c","text":"<pre><code>from evoagentx.workflow.workflow_graph import WorkFlowNode, WorkFlowGraph, WorkFlowEdge\nfrom evoagentx.workflow.workflow import WorkFlow \nfrom evoagentx.agents import AgentManager, CustomizeAgent \nfrom evoagentx.models import OpenAILLMConfig, OpenAILLM \n\nllm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=\"xxx\", stream=True, output_response=True)\nllm = OpenAILLM(llm_config)\n\nagent_manager = AgentManager()\n\ndata_extraction_agent = CustomizeAgent(\n    name=\"DataExtractionAgent\",\n    description=\"Extract data from source\",\n    inputs=[{\"name\": \"data_source\", \"type\": \"string\", \"description\": \"Source data location\"}],\n    outputs=[{\"name\": \"extracted_data\", \"type\": \"string\", \"description\": \"Extracted data\"}],\n    prompt=\"Extract data from source: {data_source}\",\n    llm_config=llm_config\n)  \n\ndata_transformation_agent = CustomizeAgent(\n    name=\"DataTransformationAgent\",\n    description=\"Transform data\",\n    inputs=[{\"name\": \"extracted_data\", \"type\": \"string\", \"description\": \"Extracted data\"}],\n    outputs=[{\"name\": \"transformed_data\", \"type\": \"string\", \"description\": \"Transformed data\"}],\n    prompt=\"Transform data: {extracted_data}\",\n    llm_config=llm_config\n)\n\n# \u5c06\u4ee3\u7406\u6dfb\u52a0\u5230\u4ee3\u7406\u7ba1\u7406\u5668\u4ee5\u6267\u884c\u5de5\u4f5c\u6d41\ndata_extraction_agent = agent_manager.add_agents(agents = [data_extraction_agent, data_transformation_agent])\n\n# \u521b\u5efa\u5de5\u4f5c\u6d41\u8282\u70b9\ntask1 = WorkFlowNode(\n    name=\"Task1\",\n    description=\"Extract data from source\",\n    inputs=[{\"name\": \"data_source\", \"type\": \"string\", \"description\": \"Source data location\"}],\n    outputs=[{\"name\": \"extracted_data\", \"type\": \"string\", \"description\": \"Extracted data\"}],\n    agents=[\"DataExtractionAgent\"] # \u5e94\u8be5\u4e0e\u4ee3\u7406\u7ba1\u7406\u5668\u4e2d\u7684\u4ee3\u7406\u540d\u79f0\u5339\u914d\n)\n\ntask2 = WorkFlowNode(\n    name=\"Task2\",\n    description=\"Transform data\",\n    inputs=[{\"name\": \"extracted_data\", \"type\": \"string\", \"description\": \"Data to transform\"}],\n    outputs=[{\"name\": \"transformed_data\", \"type\": \"string\", \"description\": \"Transformed data\"}],\n    agents=[\"DataTransformationAgent\"] # \u5e94\u8be5\u4e0e\u4ee3\u7406\u7ba1\u7406\u5668\u4e2d\u7684\u4ee3\u7406\u540d\u79f0\u5339\u914d\n)\n\ntask3 = WorkFlowNode(\n    name=\"Task3\",\n    description=\"Analyze data and generate insights\",\n    inputs=[{\"name\": \"transformed_data\", \"type\": \"string\", \"description\": \"Data to analyze\"}],\n    outputs=[{\"name\": \"insights\", \"type\": \"string\", \"description\": \"Generated insights\"}],\n    agents=[\n        {\n            \"name\": \"DataAnalysisAgent\",\n            \"description\": \"Analyze data and generate insights\",\n            \"inputs\": [{\"name\": \"transformed_data\", \"type\": \"string\", \"description\": \"Data to analyze\"}],\n            \"outputs\": [{\"name\": \"insights\", \"type\": \"string\", \"description\": \"Generated insights\"}],\n            \"prompt\": \"Analyze data and generate insights: {transformed_data}\",\n            \"parse_mode\": \"str\",\n        } # \u5c06\u7528\u4e8e\u5728 agent_manager \u4e2d\u521b\u5efa CustomizeAgent \u5b9e\u4f8b\n    ]\n)\n\n# \u521b\u5efa\u5de5\u4f5c\u6d41\u8fb9\nedge1 = WorkFlowEdge(source=\"Task1\", target=\"Task2\")\nedge2 = WorkFlowEdge(source=\"Task2\", target=\"Task3\")\n\n# \u521b\u5efa\u5de5\u4f5c\u6d41\u56fe\nworkflow_graph = WorkFlowGraph(\n    goal=\"Extract, transform, and analyze data to generate insights\",\n    nodes=[task1, task2, task3],\n    edges=[edge1, edge2]\n)\n\n# \u5c06\u4ee3\u7406\u6dfb\u52a0\u5230\u4ee3\u7406\u7ba1\u7406\u5668\u4ee5\u6267\u884c\u5de5\u4f5c\u6d41\nagent_manager.add_agents_from_workflow(workflow_graph, llm_config=llm_config)\n\n# \u521b\u5efa\u5de5\u4f5c\u6d41\u5b9e\u4f8b\u4ee5\u6267\u884c\nworkflow = WorkFlow(graph=workflow_graph, agent_manager=agent_manager, llm=llm)\nworkflow.execute(inputs={\"data_source\": \"xxx\"})\n</code></pre>"},{"location":"zh/modules/workflow_graph/#\u521b\u5efa\u987a\u5e8f\u5de5\u4f5c\u6d41\u56fe","title":"\u521b\u5efa\u987a\u5e8f\u5de5\u4f5c\u6d41\u56fe","text":"<pre><code>from evoagentx.workflow.workflow_graph import SequentialWorkFlowGraph\n\n# \u5b9a\u4e49\u4efb\u52a1\u53ca\u5176\u8f93\u5165\u3001\u8f93\u51fa\u548c\u63d0\u793a\ntasks = [\n    {\n        \"name\": \"DataExtraction\",\n        \"description\": \"Extract data from the specified source\",\n        \"inputs\": [\n            {\"name\": \"data_source\", \"type\": \"string\", \"required\": True, \"description\": \"Source data location\"}\n        ],\n        \"outputs\": [\n            {\"name\": \"extracted_data\", \"type\": \"string\", \"required\": True, \"description\": \"Extracted data\"}\n        ],\n        \"prompt\": \"Extract data from the following source: {data_source}\", \n        \"parse_mode\": \"str\"\n    },\n    {\n        \"name\": \"DataTransformation\",\n        \"description\": \"Transform the extracted data\",\n        \"inputs\": [\n            {\"name\": \"extracted_data\", \"type\": \"string\", \"required\": True, \"description\": \"Data to transform\"}\n        ],\n        \"outputs\": [\n            {\"name\": \"transformed_data\", \"type\": \"string\", \"required\": True, \"description\": \"Transformed data\"}\n        ],\n        \"prompt\": \"Transform the following data: {extracted_data}\", \n        \"parse_mode\": \"str\"\n    },\n    {\n        \"name\": \"DataAnalysis\",\n        \"description\": \"Analyze data and generate insights\",\n        \"inputs\": [\n            {\"name\": \"transformed_data\", \"type\": \"string\", \"required\": True, \"description\": \"Data to analyze\"}\n        ],\n        \"outputs\": [\n            {\"name\": \"insights\", \"type\": \"string\", \"required\": True, \"description\": \"Generated insights\"}\n        ],\n        \"prompt\": \"Analyze the following data and generate insights: {transformed_data}\", \n        \"parse_mode\": \"str\"\n    }\n]\n\n# \u521b\u5efa\u987a\u5e8f\u5de5\u4f5c\u6d41\u56fe\nsequential_workflow_graph = SequentialWorkFlowGraph(\n    goal=\"Extract, transform, and analyze data to generate insights\",\n    tasks=tasks\n)\n</code></pre>"},{"location":"zh/modules/workflow_graph/#\u4fdd\u5b58\u548c\u52a0\u8f7d\u5de5\u4f5c\u6d41","title":"\u4fdd\u5b58\u548c\u52a0\u8f7d\u5de5\u4f5c\u6d41","text":"<pre><code># \u4fdd\u5b58\u5de5\u4f5c\u6d41\nworkflow_graph.save_module(\"examples/output/my_workflow.json\")\n\n# \u5bf9\u4e8e SequentialWorkFlowGraph\uff0c\u4f7f\u7528 save_module \u548c get_graph_info\nsequential_workflow_graph.save_module(\"examples/output/my_sequential_workflow.json\")\n</code></pre>"},{"location":"zh/modules/workflow_graph/#\u53ef\u89c6\u5316\u5de5\u4f5c\u6d41","title":"\u53ef\u89c6\u5316\u5de5\u4f5c\u6d41","text":"<pre><code># \u4ee5\u53ef\u89c6\u65b9\u5f0f\u663e\u793a\u5de5\u4f5c\u6d41\u56fe\nworkflow_graph.display()\n</code></pre> <p><code>WorkFlowGraph</code> \u548c <code>SequentialWorkFlowGraph</code> \u7c7b\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u800c\u5f3a\u5927\u7684\u65b9\u5f0f\u6765\u8bbe\u8ba1\u590d\u6742\u7684\u4ee3\u7406\u5de5\u4f5c\u6d41\u3001\u8ddf\u8e2a\u5176\u6267\u884c\u5e76\u7ba1\u7406\u4efb\u52a1\u4e4b\u95f4\u7684\u6570\u636e\u6d41\u3002 </p>"},{"location":"zh/tutorial/aflow_optimizer/","title":"AFlow \u4f18\u5316\u5668\u6559\u7a0b","text":"<p>\u672c\u6559\u7a0b\u5c06\u6307\u5bfc\u4f60\u5982\u4f55\u4f7f\u7528 EvoAgentX \u7684 AFlow \u4f18\u5316\u5668\u6765\u4f18\u5316\u4f60\u7684\u5de5\u4f5c\u6d41\u3002AFlow \u4f18\u5316\u5668\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u53ef\u4ee5\u5e2e\u52a9\u4f60\u81ea\u52a8\u4f18\u5316\u5de5\u4f5c\u6d41\u7684\u6027\u80fd\u3002</p>"},{"location":"zh/tutorial/aflow_optimizer/#1-\u6982\u8ff0","title":"1. \u6982\u8ff0","text":"<p>AFlow \u4f18\u5316\u5668\u662f EvoAgentX \u6846\u67b6\u4e2d\u7684\u4e00\u4e2a\u91cd\u8981\u7ec4\u4ef6\uff0c\u5b83\u63d0\u4f9b\u4e86\u4ee5\u4e0b\u529f\u80fd\uff1a</p> <ul> <li>\u81ea\u52a8\u4f18\u5316\u5de5\u4f5c\u6d41\u7684\u6027\u80fd</li> <li>\u652f\u6301\u591a\u79cd\u4f18\u5316\u7b56\u7565</li> <li>\u63d0\u4f9b\u8be6\u7ec6\u7684\u4f18\u5316\u62a5\u544a</li> <li>\u652f\u6301\u81ea\u5b9a\u4e49\u4f18\u5316\u76ee\u6807</li> </ul>"},{"location":"zh/tutorial/aflow_optimizer/#2-\u8bbe\u7f6e-aflow-\u4f18\u5316\u5668","title":"2. \u8bbe\u7f6e AFlow \u4f18\u5316\u5668","text":"<p>\u9996\u5148\uff0c\u4f60\u9700\u8981\u5bfc\u5165\u76f8\u5173\u6a21\u5757\u5e76\u8bbe\u7f6e AFlow \u4f18\u5316\u5668\u3002</p> <pre><code>from evoagentx.optimizers import AFlowOptimizer\nfrom evoagentx.config import Config\nfrom evoagentx.models import OpenAIConfig, OpenAI\n</code></pre>"},{"location":"zh/tutorial/aflow_optimizer/#\u914d\u7f6e-llm-\u6a21\u578b","title":"\u914d\u7f6e LLM \u6a21\u578b","text":"<p>\u4f60\u9700\u8981\u4e00\u4e2a\u6709\u6548\u7684 OpenAI API \u5bc6\u94a5\u6765\u521d\u59cb\u5316 LLM\u3002\u5efa\u8bae\u5c06 API \u5bc6\u94a5\u4fdd\u5b58\u5728 <code>.env</code> \u6587\u4ef6\u4e2d\uff0c\u5e76\u4f7f\u7528 <code>load_dotenv</code> \u51fd\u6570\u52a0\u8f7d\u5b83\uff1a <pre><code>import os\nfrom dotenv import load_dotenv\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nllm_config = OpenAIConfig(model=\"gpt-4\", openai_key=OPENAI_API_KEY)\nllm = OpenAI(config=llm_config)\n</code></pre></p>"},{"location":"zh/tutorial/aflow_optimizer/#3-\u8bbe\u7f6e\u7ec4\u4ef6","title":"3. \u8bbe\u7f6e\u7ec4\u4ef6","text":""},{"location":"zh/tutorial/aflow_optimizer/#\u7b2c\u4e00\u6b65\u5b9a\u4e49\u4efb\u52a1\u914d\u7f6e","title":"\u7b2c\u4e00\u6b65\uff1a\u5b9a\u4e49\u4efb\u52a1\u914d\u7f6e","text":"<p>AFlow \u4f18\u5316\u5668\u9700\u8981\u4e00\u4e2a\u914d\u7f6e\u6765\u6307\u5b9a\u4efb\u52a1\u7c7b\u578b\u548c\u53ef\u7528\u7684\u64cd\u4f5c\u7b26\u3002\u4ee5\u4e0b\u662f\u4e0d\u540c\u4efb\u52a1\u7c7b\u578b\u7684\u793a\u4f8b\u914d\u7f6e\uff1a</p> <pre><code>EXPERIMENTAL_CONFIG = {\n    \"humaneval\": {\n        \"question_type\": \"code\", \n        \"operators\": [\"Custom\", \"CustomCodeGenerate\", \"Test\", \"ScEnsemble\"] \n    }, \n    \"mbpp\": {\n        \"question_type\": \"code\", \n        \"operators\": [\"Custom\", \"CustomCodeGenerate\", \"Test\", \"ScEnsemble\"] \n    },\n    \"hotpotqa\": {\n        \"question_type\": \"qa\", \n        \"operators\": [\"Custom\", \"AnswerGenerate\", \"QAScEnsemble\"]\n    },\n    \"gsm8k\": {\n        \"question_type\": \"math\", \n        \"operators\": [\"Custom\", \"ScEnsemble\", \"Programmer\"]\n    },\n    \"math\": {\n        \"question_type\": \"math\", \n        \"operators\": [\"Custom\", \"ScEnsemble\", \"Programmer\"]\n    }\n}\n</code></pre>"},{"location":"zh/tutorial/aflow_optimizer/#\u7b2c\u4e8c\u6b65\u5b9a\u4e49\u521d\u59cb\u5de5\u4f5c\u6d41","title":"\u7b2c\u4e8c\u6b65\uff1a\u5b9a\u4e49\u521d\u59cb\u5de5\u4f5c\u6d41","text":"<p>AFlow \u4f18\u5316\u5668\u9700\u8981\u4e24\u4e2a\u6587\u4ef6\uff1a - <code>graph.py</code>\uff1a\u8be5\u6587\u4ef6\u7528 Python \u4ee3\u7801\u5b9a\u4e49\u521d\u59cb\u5de5\u4f5c\u6d41\u56fe\u3002 - <code>prompt.py</code>\uff1a\u8be5\u6587\u4ef6\u5b9a\u4e49\u5de5\u4f5c\u6d41\u4e2d\u4f7f\u7528\u7684\u63d0\u793a\u3002</p> <p>\u4ee5\u4e0b\u662f HumanEval \u57fa\u51c6\u7684 <code>graph.py</code> \u6587\u4ef6\u793a\u4f8b\uff1a</p> <pre><code>import evoagentx.workflow.operators as operator\nimport examples.aflow.code_generation.prompt as prompt_custom # noqa: F401\nfrom evoagentx.models.model_configs import LLMConfig\nfrom evoagentx.benchmark.benchmark import Benchmark\nfrom evoagentx.models.model_utils import create_llm_instance\n\nclass Workflow:\n\n    def __init__(\n        self,\n        name: str,\n        llm_config: LLMConfig,\n        benchmark: Benchmark\n    ):\n        self.name = name\n        self.llm = create_llm_instance(llm_config)\n        self.benchmark = benchmark \n        self.custom = operator.Custom(self.llm)\n        self.custom_code_generate = operator.CustomCodeGenerate(self.llm)\n\n    async def __call__(self, problem: str, entry_point: str):\n        \"\"\"\n        \u5de5\u4f5c\u6d41\u7684\u5b9e\u73b0\n        Custom \u64cd\u4f5c\u7b26\u53ef\u4ee5\u751f\u6210\u4efb\u4f55\u4f60\u60f3\u8981\u7684\u5185\u5bb9\u3002\n        \u4f46\u5f53\u4f60\u60f3\u83b7\u53d6\u6807\u51c6\u4ee3\u7801\u65f6\uff0c\u5e94\u8be5\u4f7f\u7528 custom_code_generate \u64cd\u4f5c\u7b26\u3002\n        \"\"\"\n        # await self.custom(input=, instruction=\"\")\n        solution = await self.custom_code_generate(problem=problem, entry_point=entry_point, instruction=prompt_custom.GENERATE_PYTHON_CODE_PROMPT) # \u4f46\u5f53\u4f60\u60f3\u83b7\u53d6\u6807\u51c6\u4ee3\u7801\u65f6\uff0c\u5e94\u8be5\u4f7f\u7528 customcodegenerator\n        return solution['response']\n</code></pre> <p>\u6ce8\u610f</p> <p>\u5728\u5b9a\u4e49\u5de5\u4f5c\u6d41\u65f6\uff0c\u8bf7\u6ce8\u610f\u4ee5\u4e0b\u5173\u952e\u70b9\uff1a</p> <ol> <li> <p>\u63d0\u793a\u5bfc\u5165\u8def\u5f84\uff1a\u786e\u4fdd\u6b63\u786e\u6307\u5b9a <code>prompt.py</code> \u7684\u5bfc\u5165\u8def\u5f84\uff08\u4f8b\u5982 <code>examples.aflow.code_generation.prompt</code>\uff09\u3002\u6b64\u8def\u5f84\u5e94\u8be5\u4e0e\u4f60\u7684\u9879\u76ee\u7ed3\u6784\u5339\u914d\uff0c\u4ee5\u4fbf\u6b63\u786e\u52a0\u8f7d\u63d0\u793a\u3002</p> </li> <li> <p>\u64cd\u4f5c\u7b26\u521d\u59cb\u5316\uff1a\u5728 <code>__init__</code> \u51fd\u6570\u4e2d\uff0c\u4f60\u5fc5\u987b\u521d\u59cb\u5316\u5de5\u4f5c\u6d41\u4e2d\u5c06\u4f7f\u7528\u7684\u6240\u6709\u64cd\u4f5c\u7b26\u3002\u6bcf\u4e2a\u64cd\u4f5c\u7b26\u90fd\u5e94\u8be5\u4f7f\u7528\u9002\u5f53\u7684 LLM \u5b9e\u4f8b\u8fdb\u884c\u5b9e\u4f8b\u5316\u3002</p> </li> <li> <p>\u5de5\u4f5c\u6d41\u6267\u884c\uff1a<code>__call__</code> \u51fd\u6570\u662f\u5de5\u4f5c\u6d41\u6267\u884c\u7684\u4e3b\u8981\u5165\u53e3\u70b9\u3002\u5b83\u5e94\u8be5\u5b9a\u4e49\u5de5\u4f5c\u6d41\u7684\u5b8c\u6574\u6267\u884c\u903b\u8f91\uff0c\u5e76\u8fd4\u56de\u5c06\u7528\u4e8e\u8bc4\u4f30\u7684\u6700\u7ec8\u8f93\u51fa\u3002</p> </li> </ol> <p>\u4ee5\u4e0b\u662f HumanEval \u57fa\u51c6\u7684 <code>prompt.py</code> \u6587\u4ef6\u793a\u4f8b\uff1a</p> <pre><code>GENERATE_PYTHON_CODE_PROMPT = \"\"\"\nGenerate a functional and correct Python code for the given problem.\n\nProblem: \"\"\"\n</code></pre> <p>\u6ce8\u610f</p> <p>\u5982\u679c\u5de5\u4f5c\u6d41\u4e0d\u9700\u8981\u4efb\u4f55\u63d0\u793a\uff0c\u5219 <code>prompt.py</code> \u6587\u4ef6\u53ef\u4ee5\u4e3a\u7a7a\u3002</p>"},{"location":"zh/tutorial/aflow_optimizer/#\u7b2c\u4e09\u6b65\u51c6\u5907\u57fa\u51c6","title":"\u7b2c\u4e09\u6b65\uff1a\u51c6\u5907\u57fa\u51c6","text":"<p>\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u4f7f\u7528 AFlowHumanEval \u57fa\u51c6\u3002\u5b83\u9075\u5faa\u4e0e \u539f\u59cb AFlow \u5b9e\u73b0 \u4e2d\u76f8\u540c\u7684\u6570\u636e\u5212\u5206\u548c\u683c\u5f0f\u3002</p> <pre><code># Initialize the benchmark\nhumaneval = AFlowHumanEval()\n</code></pre>"},{"location":"zh/tutorial/aflow_optimizer/#4-\u914d\u7f6e\u548c\u8fd0\u884c-aflow-\u4f18\u5316\u5668","title":"4. \u914d\u7f6e\u548c\u8fd0\u884c AFlow \u4f18\u5316\u5668","text":"<p>AFlow \u4f18\u5316\u5668\u53ef\u4ee5\u901a\u8fc7\u5404\u79cd\u53c2\u6570\u8fdb\u884c\u914d\u7f6e\uff0c\u4ee5\u63a7\u5236\u4f18\u5316\u8fc7\u7a0b\uff1a</p> <pre><code>optimizer = AFlowOptimizer(\n    graph_path=\"examples/aflow/code_generation\",  # \u521d\u59cb\u5de5\u4f5c\u6d41\u56fe\u7684\u8def\u5f84\n    optimized_path=\"examples/aflow/humaneval/optimized\",  # \u4fdd\u5b58\u4f18\u5316\u5de5\u4f5c\u6d41\u7684\u8def\u5f84\n    optimizer_llm=optimizer_llm,  # \u7528\u4e8e\u4f18\u5316\u7684 LLM\n    executor_llm=executor_llm,    # \u7528\u4e8e\u6267\u884c\u7684 LLM\n    validation_rounds=3,          # \u4f18\u5316\u671f\u95f4\u5728\u5f00\u53d1\u96c6\u4e0a\u8fd0\u884c\u9a8c\u8bc1\u7684\u6b21\u6570\n    eval_rounds=3,               # \u6d4b\u8bd5\u671f\u95f4\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8fd0\u884c\u8bc4\u4f30\u7684\u6b21\u6570\n    max_rounds=20,               # \u6700\u5927\u4f18\u5316\u8f6e\u6570\n    **EXPERIMENTAL_CONFIG[\"humaneval\"]  # \u7279\u5b9a\u4efb\u52a1\u7684\u914d\u7f6e\uff0c\u7528\u4e8e\u6307\u5b9a\u4efb\u52a1\u7c7b\u578b\u548c\u53ef\u7528\u64cd\u4f5c\u7b26\n)\n</code></pre>"},{"location":"zh/tutorial/aflow_optimizer/#\u8fd0\u884c\u4f18\u5316","title":"\u8fd0\u884c\u4f18\u5316","text":"<p>\u8981\u5f00\u59cb\u4f18\u5316\u8fc7\u7a0b\uff1a</p> <pre><code># \u4f18\u5316\u5de5\u4f5c\u6d41\noptimizer.optimize(humaneval)\n</code></pre> <p>\u6ce8\u610f</p> <p>\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\uff0c\u5de5\u4f5c\u6d41\u5c06\u5728\u6bcf\u4e00\u6b65\u9aa4\u4e0a\u5bf9\u5f00\u53d1\u96c6\u8fdb\u884c <code>validation_rounds</code> \u6b21\u9a8c\u8bc1\u3002\u786e\u4fdd\u57fa\u51c6 <code>humaneval</code> \u5305\u542b\u5f00\u53d1\u96c6\uff08\u5373 <code>self._dev_data</code> \u4e0d\u4e3a\u7a7a\uff09\u3002</p>"},{"location":"zh/tutorial/aflow_optimizer/#\u6d4b\u8bd5\u4f18\u5316\u540e\u7684\u5de5\u4f5c\u6d41","title":"\u6d4b\u8bd5\u4f18\u5316\u540e\u7684\u5de5\u4f5c\u6d41","text":"<p>\u8981\u6d4b\u8bd5\u4f18\u5316\u540e\u7684\u5de5\u4f5c\u6d41\uff1a</p> <p><pre><code># \u6d4b\u8bd5\u4f18\u5316\u540e\u7684\u5de5\u4f5c\u6d41\noptimizer.test(humaneval)\n</code></pre> \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u4f18\u5316\u5668\u5c06\u9009\u62e9\u9a8c\u8bc1\u6027\u80fd\u6700\u9ad8\u7684\u5de5\u4f5c\u6d41\u8fdb\u884c\u6d4b\u8bd5\u3002\u4f60\u8fd8\u53ef\u4ee5\u4f7f\u7528 <code>test_rounds: List[int]</code> \u53c2\u6570\u6307\u5b9a\u6d4b\u8bd5\u8f6e\u6b21\u3002\u4f8b\u5982\uff0c\u8981\u8bc4\u4f30\u7b2c\u4e8c\u8f6e\u548c\u7b2c\u4e09\u8f6e\uff0c\u53ef\u4ee5\u4f7f\u7528 <code>optimizer.test(humaneval, test_rounds=[2, 3])</code>\u3002</p> <p>\u6ce8\u610f</p> <p>\u5728\u6d4b\u8bd5\u671f\u95f4\uff0c\u5de5\u4f5c\u6d41\u5c06\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8fdb\u884c <code>eval_rounds</code> \u6b21\u8bc4\u4f30\u3002\u786e\u4fdd\u57fa\u51c6 <code>humaneval</code> \u5305\u542b\u6d4b\u8bd5\u96c6\uff08\u5373 <code>self._test_data</code> \u4e0d\u4e3a\u7a7a\uff09\u3002</p> <p>\u6709\u5173\u5b8c\u6574\u7684\u5de5\u4f5c\u793a\u4f8b\uff0c\u8bf7\u53c2\u9605 aflow_humaneval.py\u3002</p>"},{"location":"zh/tutorial/benchmark_and_evaluation/","title":"\u57fa\u51c6\u6d4b\u8bd5\u548c\u8bc4\u4f30\u6559\u7a0b","text":"<p>\u672c\u6559\u7a0b\u5c06\u6307\u5bfc\u4f60\u4f7f\u7528 EvoAgentX \u8bbe\u7f6e\u548c\u8fd0\u884c\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\u3002\u6211\u4eec\u5c06\u4f7f\u7528 HotpotQA \u6570\u636e\u96c6\u4f5c\u4e3a\u793a\u4f8b\uff0c\u6f14\u793a\u5982\u4f55\u8bbe\u7f6e\u548c\u8fd0\u884c\u8bc4\u4f30\u8fc7\u7a0b\u3002</p>"},{"location":"zh/tutorial/benchmark_and_evaluation/#1-\u6982\u8ff0","title":"1. \u6982\u8ff0","text":"<p>EvoAgentX \u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u4e14\u6a21\u5757\u5316\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4f7f\u4f60\u80fd\u591f\uff1a</p> <ul> <li>\u52a0\u8f7d\u548c\u4f7f\u7528\u9884\u5b9a\u4e49\u7684\u57fa\u51c6\u6570\u636e\u96c6</li> <li>\u81ea\u5b9a\u4e49\u6570\u636e\u52a0\u8f7d\u3001\u5904\u7406\u548c\u540e\u671f\u5904\u7406\u903b\u8f91</li> <li>\u8bc4\u4f30\u591a\u4ee3\u7406\u5de5\u4f5c\u6d41\u7684\u6027\u80fd</li> <li>\u5e76\u884c\u5904\u7406\u591a\u4e2a\u8bc4\u4f30\u4efb\u52a1</li> </ul>"},{"location":"zh/tutorial/benchmark_and_evaluation/#2-\u8bbe\u7f6e\u57fa\u51c6\u6d4b\u8bd5","title":"2. \u8bbe\u7f6e\u57fa\u51c6\u6d4b\u8bd5","text":"<p>\u9996\u5148\uff0c\u4f60\u9700\u8981\u5bfc\u5165\u76f8\u5173\u6a21\u5757\u5e76\u8bbe\u7f6e\u8bc4\u4f30\u8fc7\u7a0b\u4e2d\u4ee3\u7406\u5c06\u4f7f\u7528\u7684\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u3002</p> <pre><code>from evoagentx.config import Config\nfrom evoagentx.models import OpenAIConfig, OpenAI \nfrom evoagentx.benchmark import HotpotQA\nfrom evoagentx.workflow import QAActionGraph \nfrom evoagentx.evaluators import Evaluator \nfrom evoagentx.core.callbacks import suppress_logger_info\n</code></pre>"},{"location":"zh/tutorial/benchmark_and_evaluation/#\u914d\u7f6e-llm-\u6a21\u578b","title":"\u914d\u7f6e LLM \u6a21\u578b","text":"<p>\u4f60\u9700\u8981\u4e00\u4e2a\u6709\u6548\u7684 OpenAI API \u5bc6\u94a5\u6765\u521d\u59cb\u5316 LLM\u3002\u5efa\u8bae\u5c06 API \u5bc6\u94a5\u4fdd\u5b58\u5728 <code>.env</code> \u6587\u4ef6\u4e2d\uff0c\u5e76\u4f7f\u7528 <code>load_dotenv</code> \u51fd\u6570\u52a0\u8f7d\u5b83\uff1a <pre><code>import os \nfrom dotenv import load_dotenv\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nllm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=OPENAI_API_KEY)\nllm = OpenAILLM(config=llm_config)\n</code></pre></p>"},{"location":"zh/tutorial/benchmark_and_evaluation/#3-\u521d\u59cb\u5316\u57fa\u51c6\u6d4b\u8bd5","title":"3. \u521d\u59cb\u5316\u57fa\u51c6\u6d4b\u8bd5","text":"<p>EvoAgentX \u5305\u542b\u591a\u4e2a\u9884\u5b9a\u4e49\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u95ee\u7b54\u3001\u6570\u5b66\u548c\u7f16\u7801\u7b49\u4efb\u52a1\u3002\u6709\u5173\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7684\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 Benchmark README\u3002\u4f60\u8fd8\u53ef\u4ee5\u901a\u8fc7\u6269\u5c55\u57fa\u7840 <code>Benchmark</code> \u63a5\u53e3\u6765\u5b9a\u4e49\u81ea\u5df1\u7684\u57fa\u51c6\u6d4b\u8bd5\u7c7b\uff0c\u6211\u4eec\u5728 \u81ea\u5b9a\u4e49\u57fa\u51c6\u6d4b\u8bd5 \u90e8\u5206\u63d0\u4f9b\u4e86\u4e00\u4e2a\u793a\u4f8b\u3002</p> <p>\u5728\u8fd9\u4e2a\u793a\u4f8b\u4e2d\uff0c\u6211\u4eec\u5c06\u4f7f\u7528 <code>HotpotQA</code> \u57fa\u51c6\u6d4b\u8bd5\u3002 <pre><code>benchmark = HotPotQA(mode=\"dev\")\n</code></pre> \u5176\u4e2d <code>mode</code> \u53c2\u6570\u51b3\u5b9a\u52a0\u8f7d\u6570\u636e\u96c6\u7684\u54ea\u4e2a\u90e8\u5206\u3002\u9009\u9879\u5305\u62ec\uff1a</p> <ul> <li><code>\"train\"</code>\uff1a\u8bad\u7ec3\u6570\u636e</li> <li><code>\"dev\"</code>\uff1a\u5f00\u53d1/\u9a8c\u8bc1\u6570\u636e</li> <li><code>\"test\"</code>\uff1a\u6d4b\u8bd5\u6570\u636e</li> <li><code>\"all\"</code>\uff08\u9ed8\u8ba4\uff09\uff1a\u52a0\u8f7d\u6574\u4e2a\u6570\u636e\u96c6</li> </ul> <p>\u6570\u636e\u5c06\u81ea\u52a8\u4e0b\u8f7d\u5230\u9ed8\u8ba4\u7f13\u5b58\u6587\u4ef6\u5939\uff0c\u4f46\u4f60\u53ef\u4ee5\u901a\u8fc7\u6307\u5b9a <code>path</code> \u53c2\u6570\u6765\u66f4\u6539\u6b64\u4f4d\u7f6e\u3002</p>"},{"location":"zh/tutorial/benchmark_and_evaluation/#4-\u8fd0\u884c\u8bc4\u4f30","title":"4. \u8fd0\u884c\u8bc4\u4f30","text":"<p>\u4e00\u65e6\u4f60\u51c6\u5907\u597d\u4e86\u57fa\u51c6\u6d4b\u8bd5\u548c LLM\uff0c\u4e0b\u4e00\u6b65\u5c31\u662f\u5b9a\u4e49\u4f60\u7684\u4ee3\u7406\u5de5\u4f5c\u6d41\u548c\u8bc4\u4f30\u903b\u8f91\u3002EvoAgentX \u652f\u6301\u5b8c\u5168\u81ea\u5b9a\u4e49\u57fa\u51c6\u6d4b\u8bd5\u793a\u4f8b\u7684\u5904\u7406\u65b9\u5f0f\u548c\u8f93\u51fa\u7684\u89e3\u91ca\u65b9\u5f0f\u3002</p> <p>\u4ee5\u4e0b\u662f\u5982\u4f55\u4f7f\u7528 <code>HotpotQA</code> \u57fa\u51c6\u6d4b\u8bd5\u548c QA \u5de5\u4f5c\u6d41\u8fd0\u884c\u8bc4\u4f30\u3002</p>"},{"location":"zh/tutorial/benchmark_and_evaluation/#\u6b65\u9aa4-1\u5b9a\u4e49\u4ee3\u7406\u5de5\u4f5c\u6d41","title":"\u6b65\u9aa4 1\uff1a\u5b9a\u4e49\u4ee3\u7406\u5de5\u4f5c\u6d41","text":"<p>\u4f60\u53ef\u4ee5\u4f7f\u7528\u9884\u5b9a\u4e49\u7684\u5de5\u4f5c\u6d41\u4e4b\u4e00\u6216\u5b9e\u73b0\u81ea\u5df1\u7684\u5de5\u4f5c\u6d41\u3002\u5728\u8fd9\u4e2a\u793a\u4f8b\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528\u4e3a\u95ee\u7b54\u8bbe\u8ba1\u7684 <code>QAActionGraph</code>\uff0c\u5b83\u7b80\u5355\u5730\u4f7f\u7528\u81ea\u4e00\u81f4\u6027\u6765\u751f\u6210\u6700\u7ec8\u7b54\u6848\uff1a</p> <pre><code>workflow = QAActionGraph(\n    llm_config=llm_config,\n    description=\"This workflow aims to address multi-hop QA tasks.\"\n)\n</code></pre>"},{"location":"zh/tutorial/benchmark_and_evaluation/#\u6b65\u9aa4-2\u81ea\u5b9a\u4e49\u6570\u636e\u9884\u5904\u7406\u548c\u540e\u5904\u7406","title":"\u6b65\u9aa4 2\uff1a\u81ea\u5b9a\u4e49\u6570\u636e\u9884\u5904\u7406\u548c\u540e\u5904\u7406","text":"<p>\u8bc4\u4f30\u7684\u4e0b\u4e00\u4e2a\u5173\u952e\u65b9\u9762\u662f\u6b63\u786e\u5730\u5728\u57fa\u51c6\u6d4b\u8bd5\u3001\u5de5\u4f5c\u6d41\u548c\u8bc4\u4f30\u6307\u6807\u4e4b\u95f4\u8f6c\u6362\u6570\u636e\u3002</p>"},{"location":"zh/tutorial/benchmark_and_evaluation/#\u4e3a\u4ec0\u4e48\u9700\u8981\u9884\u5904\u7406\u548c\u540e\u5904\u7406","title":"\u4e3a\u4ec0\u4e48\u9700\u8981\u9884\u5904\u7406\u548c\u540e\u5904\u7406","text":"<p>\u5728 EvoAgentX \u4e2d\uff0c\u9884\u5904\u7406\u548c\u540e\u5904\u7406\u662f\u786e\u4fdd\u57fa\u51c6\u6d4b\u8bd5\u6570\u636e\u3001\u5de5\u4f5c\u6d41\u548c\u8bc4\u4f30\u903b\u8f91\u4e4b\u95f4\u987a\u7545\u4ea4\u4e92\u7684\u91cd\u8981\u6b65\u9aa4\uff1a</p> <ul> <li> <p>\u9884\u5904\u7406\uff08<code>collate_func</code>\uff09\uff1a  </p> <p>\u6765\u81ea HotpotQA \u7b49\u57fa\u51c6\u6d4b\u8bd5\u7684\u539f\u59cb\u793a\u4f8b\u901a\u5e38\u5305\u542b\u7ed3\u6784\u5316\u5b57\u6bb5\uff0c\u5982\u95ee\u9898\u3001\u7b54\u6848\u548c\u4e0a\u4e0b\u6587\u3002\u4f46\u662f\uff0c\u4f60\u7684\u4ee3\u7406\u5de5\u4f5c\u6d41\u901a\u5e38\u671f\u671b\u4e00\u4e2a\u5355\u4e00\u7684\u63d0\u793a\u5b57\u7b26\u4e32\u6216\u5176\u4ed6\u7ed3\u6784\u5316\u8f93\u5165\u3002<code>collate_func</code> \u7528\u4e8e\u5c06\u6bcf\u4e2a\u539f\u59cb\u793a\u4f8b\u8f6c\u6362\u4e3a\u4f60\u7684\uff08\u81ea\u5b9a\u4e49\uff09\u5de5\u4f5c\u6d41\u53ef\u4ee5\u4f7f\u7528\u7684\u683c\u5f0f\u3002</p> </li> <li> <p>\u540e\u5904\u7406\uff08<code>output_postprocess_func</code>\uff09\uff1a</p> <p>\u5de5\u4f5c\u6d41\u8f93\u51fa\u53ef\u80fd\u5305\u62ec\u63a8\u7406\u6b65\u9aa4\u6216\u8d85\u51fa\u6700\u7ec8\u7b54\u6848\u7684\u989d\u5916\u683c\u5f0f\u3002\u7531\u4e8e <code>Evaluator</code> \u5185\u90e8\u8c03\u7528\u57fa\u51c6\u6d4b\u8bd5\u7684 <code>evaluate</code> \u65b9\u6cd5\u6765\u8ba1\u7b97\u6307\u6807\uff08\u4f8b\u5982\uff0c\u7cbe\u786e\u5339\u914d\u6216 F1\uff09\uff0c\u901a\u5e38\u9700\u8981\u4ee5\u5e72\u51c0\u7684\u683c\u5f0f\u63d0\u53d6\u6700\u7ec8\u7b54\u6848\u3002<code>output_postprocess_func</code> \u5904\u7406\u8fd9\u4e00\u70b9\uff0c\u786e\u4fdd\u8f93\u51fa\u9002\u5408\u8bc4\u4f30\u3002</p> </li> </ul> <p>\u7b80\u800c\u8a00\u4e4b\uff0c\u9884\u5904\u7406\u4e3a\u5de5\u4f5c\u6d41\u51c6\u5907\u57fa\u51c6\u6d4b\u8bd5\u793a\u4f8b\uff0c\u800c\u540e\u5904\u7406\u4e3a\u8bc4\u4f30\u51c6\u5907\u5de5\u4f5c\u6d41\u8f93\u51fa\u3002</p> <p>\u5728\u4ee5\u4e0b\u793a\u4f8b\u4e2d\uff0c\u6211\u4eec\u5b9a\u4e49\u4e86\u4e00\u4e2a <code>collate_func</code> \u6765\u5c06\u539f\u59cb\u793a\u4f8b\u683c\u5f0f\u5316\u4e3a\u5de5\u4f5c\u6d41\u7684\u63d0\u793a\uff0c\u4ee5\u53ca\u4e00\u4e2a <code>output_postprocess_func</code> \u6765\u4ece\u5de5\u4f5c\u6d41\u8f93\u51fa\u4e2d\u63d0\u53d6\u6700\u7ec8\u7b54\u6848\u3002</p> <p>\u53ef\u4ee5\u4f7f\u7528 <code>collate_func</code> \u683c\u5f0f\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6bcf\u4e2a\u793a\u4f8b\uff0c\u5b83\u5c06\u539f\u59cb\u793a\u4f8b\u8f6c\u6362\u4e3a\u4ee3\u7406\u7684\u63d0\u793a\u6216\u7ed3\u6784\u5316\u8f93\u5165\u3002</p> <pre><code>def collate_func(example: dict) -&gt; dict:\n    \"\"\"\n    Args:\n        example (dict): A dictionary containing the raw example data.\n\n    Returns: \n        The expected input for the (custom) workflow.\n    \"\"\"\n    problem = \"Question: {}\\n\\n\".format(example[\"question\"])\n    context_list = []\n    for item in example[\"context\"]:\n        context = \"Title: {}\\nText: {}\".format(item[0], \" \".join([t.strip() for t in item[1]]))\n        context_list.append(context)\n    context = \"\\n\\n\".join(context_list)\n    problem += \"Context: {}\\n\\n\".format(context)\n    problem += \"Answer:\" \n    return {\"problem\": problem}\n</code></pre> <p>\u5728\u4ee3\u7406\u751f\u6210\u8f93\u51fa\u540e\uff0c\u4f60\u53ef\u4ee5\u5b9a\u4e49\u5982\u4f55\u4f7f\u7528 <code>output_postprocess_func</code> \u63d0\u53d6\u6700\u7ec8\u7b54\u6848\u3002 <pre><code>def output_postprocess_func(output: dict) -&gt; dict:\n    \"\"\"\n    Args:\n        output (dict): The output from the workflow.\n\n    Returns: \n        The processed output that can be used to compute the metrics. The output will be directly passed to the benchmark's `evaluate` method. \n    \"\"\"\n    return output[\"answer\"]\n</code></pre></p>"},{"location":"zh/tutorial/benchmark_and_evaluation/#\u6b65\u9aa4-3\u521d\u59cb\u5316\u8bc4\u4f30\u5668","title":"\u6b65\u9aa4 3\uff1a\u521d\u59cb\u5316\u8bc4\u4f30\u5668","text":"<p>\u8bc4\u4f30\u5668\u5c06\u6240\u6709\u5185\u5bb9\u8054\u7cfb\u5728\u4e00\u8d77\u2014\u2014\u5b83\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fd0\u884c\u5de5\u4f5c\u6d41\u5e76\u8ba1\u7b97\u6027\u80fd\u6307\u6807\u3002</p> <p><pre><code>evaluator = Evaluator(\n    llm=llm, \n    collate_func=collate_func,\n    output_postprocess_func=output_postprocess_func,\n    verbose=True, \n    num_workers=3 \n)\n</code></pre> \u5982\u679c <code>num_workers</code> \u5927\u4e8e 1\uff0c\u8bc4\u4f30\u5c06\u5728\u591a\u4e2a\u7ebf\u7a0b\u4e0a\u5e76\u884c\u8fdb\u884c\u3002</p>"},{"location":"zh/tutorial/benchmark_and_evaluation/#\u6b65\u9aa4-4\u8fd0\u884c\u8bc4\u4f30","title":"\u6b65\u9aa4 4\uff1a\u8fd0\u884c\u8bc4\u4f30","text":"<p>\u73b0\u5728\uff0c\u4f60\u53ef\u4ee5\u901a\u8fc7\u5411\u8bc4\u4f30\u5668\u63d0\u4f9b\u5de5\u4f5c\u6d41\u548c\u57fa\u51c6\u6d4b\u8bd5\u6765\u8fd0\u884c\u8bc4\u4f30\uff1a</p> <p><pre><code>with suppress_logger_info():\n    results = evaluator.evaluate(\n        graph=workflow, \n        benchmark=benchmark, \n        eval_mode=\"dev\", # Evaluation split: train / dev / test \n        sample_k=10 # If set, randomly sample k examples from the benchmark for evaluation  \n    )\n\nprint(\"Evaluation metrics: \", results)\n</code></pre> \u5176\u4e2d <code>suppress_logger_info</code> \u7528\u4e8e\u6291\u5236\u65e5\u5fd7\u4fe1\u606f\u3002</p> <p>\u6709\u5173\u5b8c\u6574\u793a\u4f8b\uff0c\u8bf7\u53c2\u8003 \u57fa\u51c6\u6d4b\u8bd5\u548c\u8bc4\u4f30\u793a\u4f8b\u3002</p>"},{"location":"zh/tutorial/benchmark_and_evaluation/#\u81ea\u5b9a\u4e49\u57fa\u51c6\u6d4b\u8bd5","title":"\u81ea\u5b9a\u4e49\u57fa\u51c6\u6d4b\u8bd5","text":"<p>\u8981\u5b9a\u4e49\u81ea\u5b9a\u4e49\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4f60\u9700\u8981\u6269\u5c55 <code>Benchmark</code> \u7c7b\u5e76\u5b9e\u73b0\u4ee5\u4e0b\u65b9\u6cd5\uff1a</p> <ul> <li> <p><code>_load_data(self)</code>\uff1a </p> <p>\u52a0\u8f7d\u57fa\u51c6\u6d4b\u8bd5\u6570\u636e\uff0c\u5e76\u8bbe\u7f6e <code>self._train_data</code>\u3001<code>self._dev_data</code> \u548c <code>self._test_data</code> \u5c5e\u6027\u3002</p> </li> <li> <p><code>_get_id(self, example: Any) -&gt; Any</code>\uff1a </p> <p>\u8fd4\u56de\u793a\u4f8b\u7684\u552f\u4e00\u6807\u8bc6\u7b26\u3002</p> </li> <li> <p><code>_get_label(self, example: Any) -&gt; Any</code>\uff1a</p> <p>\u8fd4\u56de\u4e0e\u7ed9\u5b9a\u793a\u4f8b\u5173\u8054\u7684\u6807\u7b7e\u6216\u771f\u5b9e\u503c\u3002</p> <p>\u8fd9\u7528\u4e8e\u5728\u8bc4\u4f30\u671f\u95f4\u5c06\u9884\u6d4b\u4e0e\u6b63\u786e\u7b54\u6848\u8fdb\u884c\u6bd4\u8f83\u3002\u8f93\u51fa\u5c06\u76f4\u63a5\u4f20\u9012\u7ed9 <code>evaluate</code> \u65b9\u6cd5\u3002</p> </li> <li> <p><code>evaluate(self, prediction: Any, label: Any) -&gt; dict</code>\uff1a </p> <p>\u57fa\u4e8e\u9884\u6d4b\u548c\u771f\u5b9e\u6807\u7b7e\uff08\u4ece <code>_get_label</code> \u83b7\u53d6\uff09\u8ba1\u7b97\u5355\u4e2a\u793a\u4f8b\u7684\u8bc4\u4f30\u6307\u6807\u3002 \u6b64\u65b9\u6cd5\u5e94\u8fd4\u56de\u6307\u6807\u540d\u79f0\u548c\u503c\u7684\u5b57\u5178\u3002</p> </li> <li> <p><code>evaluate(self, prediction: Any, label: Any) -&gt; dict</code>: </p> <p>Compute the evaluation metrics for a single example, based on its prediction and ground-truth label (obtained from <code>_get_label</code>). This method should return a dictionary of metric name(s) and value(s).</p> </li> </ul> <p>For a complete example of a benchmark implementation, please refer to the HotPotQA class.</p>"},{"location":"zh/tutorial/first_agent/","title":"\u6784\u5efa\u4f60\u7684\u7b2c\u4e00\u4e2a\u4ee3\u7406","text":"<p>\u5728 EvoAgentX \u4e2d\uff0c\u4ee3\u7406\u662f\u8bbe\u8ba1\u7528\u6765\u81ea\u4e3b\u5b8c\u6210\u7279\u5b9a\u4efb\u52a1\u7684\u667a\u80fd\u7ec4\u4ef6\u3002\u672c\u6559\u7a0b\u5c06\u5f15\u5bfc\u4f60\u4e86\u89e3\u5728 EvoAgentX \u4e2d\u521b\u5efa\u548c\u4f7f\u7528\u4ee3\u7406\u7684\u57fa\u672c\u6982\u5ff5\uff1a</p> <ol> <li>\u4f7f\u7528 CustomizeAgent \u521b\u5efa\u7b80\u5355\u4ee3\u7406\uff1a\u5b66\u4e60\u5982\u4f55\u4f7f\u7528\u81ea\u5b9a\u4e49\u63d0\u793a\u521b\u5efa\u57fa\u672c\u4ee3\u7406</li> <li>\u4f7f\u7528\u591a\u4e2a\u52a8\u4f5c\uff1a\u521b\u5efa\u53ef\u4ee5\u6267\u884c\u591a\u4e2a\u4efb\u52a1\u7684\u66f4\u590d\u6742\u7684\u4ee3\u7406</li> <li>\u4fdd\u5b58\u548c\u52a0\u8f7d\u4ee3\u7406\uff1a\u5b66\u4e60\u5982\u4f55\u4fdd\u5b58\u548c\u52a0\u8f7d\u4f60\u7684\u4ee3\u7406</li> </ol> <p>\u901a\u8fc7\u672c\u6559\u7a0b\uff0c\u4f60\u5c06\u80fd\u591f\u521b\u5efa\u7b80\u5355\u548c\u590d\u6742\u7684\u4ee3\u7406\uff0c\u4e86\u89e3\u5b83\u4eec\u5982\u4f55\u5904\u7406\u8f93\u5165\u548c\u8f93\u51fa\uff0c\u4ee5\u53ca\u5982\u4f55\u5728\u9879\u76ee\u4e2d\u4fdd\u5b58\u548c\u91cd\u7528\u5b83\u4eec\u3002</p>"},{"location":"zh/tutorial/first_agent/#1-\u4f7f\u7528-customizeagent-\u521b\u5efa\u7b80\u5355\u4ee3\u7406","title":"1. \u4f7f\u7528 CustomizeAgent \u521b\u5efa\u7b80\u5355\u4ee3\u7406","text":"<p>\u521b\u5efa\u4ee3\u7406\u6700\u7b80\u5355\u7684\u65b9\u6cd5\u662f\u4f7f\u7528 <code>CustomizeAgent</code>\uff0c\u5b83\u5141\u8bb8\u4f60\u5feb\u901f\u5b9a\u4e49\u4e00\u4e2a\u5177\u6709\u7279\u5b9a\u63d0\u793a\u7684\u4ee3\u7406\u3002</p> <p>\u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5bfc\u5165\u5fc5\u8981\u7684\u7ec4\u4ef6\u5e76\u8bbe\u7f6e LLM\uff1a</p> <pre><code>import os \nfrom dotenv import load_dotenv\nfrom evoagentx.models import OpenAILLMConfig\nfrom evoagentx.agents import CustomizeAgent\n\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\n# Configure LLM\nopenai_config = OpenAILLMConfig(\n    model=\"gpt-4o-mini\", \n    openai_key=OPENAI_API_KEY, \n    stream=True\n)\n</code></pre> <p>\u73b0\u5728\uff0c\u8ba9\u6211\u4eec\u521b\u5efa\u4e00\u4e2a\u6253\u5370 hello world \u7684\u7b80\u5355\u4ee3\u7406\u3002\u6709\u4e24\u79cd\u65b9\u6cd5\u53ef\u4ee5\u521b\u5efa CustomizeAgent\uff1a</p>"},{"location":"zh/tutorial/first_agent/#\u65b9\u6cd5-1\u76f4\u63a5\u521d\u59cb\u5316","title":"\u65b9\u6cd5 1\uff1a\u76f4\u63a5\u521d\u59cb\u5316","text":"<p>\u4f60\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 <code>CustomizeAgent</code> \u7c7b\u521d\u59cb\u5316\u4ee3\u7406\uff1a <pre><code>first_agent = CustomizeAgent(\n    name=\"FirstAgent\",\n    description=\"A simple agent that prints hello world\",\n    prompt=\"Print 'hello world'\", \n    llm_config=openai_config # specify the LLM configuration \n)\n</code></pre></p>"},{"location":"zh/tutorial/first_agent/#\u65b9\u6cd5-2\u4ece\u5b57\u5178\u521b\u5efa","title":"\u65b9\u6cd5 2\uff1a\u4ece\u5b57\u5178\u521b\u5efa","text":"<p>\u4f60\u4e5f\u53ef\u4ee5\u901a\u8fc7\u5b9a\u4e49\u5b57\u5178\u4e2d\u7684\u914d\u7f6e\u6765\u521b\u5efa\u4ee3\u7406\uff1a</p> <pre><code>agent_data = {\n    \"name\": \"FirstAgent\",\n    \"description\": \"A simple agent that prints hello world\",\n    \"prompt\": \"Print 'hello world'\",\n    \"llm_config\": openai_config\n}\nfirst_agent = CustomizeAgent.from_dict(agent_data) # use .from_dict() to create an agent. \n</code></pre>"},{"location":"zh/tutorial/first_agent/#\u4f7f\u7528\u4ee3\u7406","title":"\u4f7f\u7528\u4ee3\u7406","text":"<p>\u521b\u5efa\u5b8c\u6210\u540e\uff0c\u4f60\u53ef\u4ee5\u4f7f\u7528\u4ee3\u7406\u6765\u6253\u5370 hello world\u3002</p> <pre><code># Execute the agent without input. The agent will return a Message object containing the results. \nmessage = first_agent()\n\nprint(f\"Response from {first_agent.name}:\")\nprint(message.content.content) # the content of a Message object is a LLMOutputParser object, where the `content` attribute is the raw LLM output. \n</code></pre> <p>\u6709\u5173\u5b8c\u6574\u793a\u4f8b\uff0c\u8bf7\u53c2\u8003 CustomizeAgent \u793a\u4f8b\u3002</p> <p>CustomizeAgent \u8fd8\u63d0\u4f9b\u5176\u4ed6\u529f\u80fd\uff0c\u5305\u62ec\u7ed3\u6784\u5316\u8f93\u5165/\u8f93\u51fa\u548c\u591a\u79cd\u89e3\u6790\u7b56\u7565\u3002\u6709\u5173\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 CustomizeAgent \u6587\u6863\u3002</p>"},{"location":"zh/tutorial/first_agent/#2-\u521b\u5efa\u5177\u6709\u591a\u4e2a\u52a8\u4f5c\u7684\u4ee3\u7406","title":"2. \u521b\u5efa\u5177\u6709\u591a\u4e2a\u52a8\u4f5c\u7684\u4ee3\u7406","text":"<p>\u5728 EvoAgentX \u4e2d\uff0c\u4f60\u53ef\u4ee5\u521b\u5efa\u4e00\u4e2a\u5177\u6709\u591a\u4e2a\u9884\u5b9a\u4e49\u52a8\u4f5c\u7684\u4ee3\u7406\u3002\u8fd9\u5141\u8bb8\u4f60\u6784\u5efa\u53ef\u4ee5\u6267\u884c\u591a\u4e2a\u4efb\u52a1\u7684\u66f4\u590d\u6742\u7684\u4ee3\u7406\u3002\u4ee5\u4e0b\u662f\u4e00\u4e2a\u793a\u4f8b\uff0c\u5c55\u793a\u5982\u4f55\u521b\u5efa\u4e00\u4e2a\u5177\u6709 <code>TestCodeGeneration</code> \u548c <code>TestCodeReview</code> \u52a8\u4f5c\u7684\u4ee3\u7406\uff1a</p>"},{"location":"zh/tutorial/first_agent/#\u5b9a\u4e49\u52a8\u4f5c","title":"\u5b9a\u4e49\u52a8\u4f5c","text":"<p>\u9996\u5148\uff0c\u6211\u4eec\u9700\u8981\u5b9a\u4e49\u52a8\u4f5c\uff0c\u5b83\u4eec\u662f <code>Action</code> \u7684\u5b50\u7c7b\uff1a <pre><code>from evoagentx.agents import Agent\nfrom evoagentx.actions import Action, ActionInput, ActionOutput\n\n# Define the CodeGeneration action inputs\nclass TestCodeGenerationInput(ActionInput):\n    requirement: str = Field(description=\"The requirement for the code generation\")\n\n# Define the CodeGeneration action outputs\nclass TestCodeGenerationOutput(ActionOutput):\n    code: str = Field(description=\"The generated code\")\n\n# Define the CodeGeneration action\nclass TestCodeGeneration(Action): \n\n    def __init__(\n        self, \n        name: str=\"TestCodeGeneration\", \n        description: str=\"Generate code based on requirements\", \n        prompt: str=\"Generate code based on requirements: {requirement}\",\n        inputs_format: ActionInput=None, \n        outputs_format: ActionOutput=None, \n        **kwargs\n    ):\n        inputs_format = inputs_format or TestCodeGenerationInput\n        outputs_format = outputs_format or TestCodeGenerationOutput\n        super().__init__(\n            name=name, \n            description=description, \n            prompt=prompt, \n            inputs_format=inputs_format, \n            outputs_format=outputs_format, \n            **kwargs\n        )\n\n    def execute(self, llm: Optional[BaseLLM] = None, inputs: Optional[dict] = None, sys_msg: Optional[str]=None, return_prompt: bool = False, **kwargs) -&gt; TestCodeGenerationOutput:\n        action_input_attrs = self.inputs_format.get_attrs() # obtain the attributes of the action input \n        action_input_data = {attr: inputs.get(attr, \"undefined\") for attr in action_input_attrs}\n        prompt = self.prompt.format(**action_input_data) # format the prompt with the action input data \n        output = llm.generate(\n            prompt=prompt, \n            system_message=sys_msg, \n            parser=self.outputs_format, \n            parse_mode=\"str\" # specify how to parse the output \n        )\n        if return_prompt:\n            return output, prompt\n        return output\n\n\n# Define the CodeReview action inputs\nclass TestCodeReviewInput(ActionInput):\n    code: str = Field(description=\"The code to be reviewed\")\n    requirements: str = Field(description=\"The requirements for the code review\")\n\n# Define the CodeReview action outputs\nclass TestCodeReviewOutput(ActionOutput):\n    review: str = Field(description=\"The review of the code\")\n\n# Define the CodeReview action\nclass TestCodeReview(Action):\n    def __init__(\n        self, \n        name: str=\"TestCodeReview\", \n        description: str=\"Review the code based on requirements\", \n        prompt: str=\"Review the following code based on the requirements:\\n\\nRequirements: {requirements}\\n\\nCode:\\n{code}. You should output a JSON object with the following fields: 'review'.\", \n        inputs_format: ActionInput=None, \n        outputs_format: ActionOutput=None, \n        **kwargs\n    ):\n        inputs_format = inputs_format or TestCodeReviewInput\n        outputs_format = outputs_format or TestCodeReviewOutput\n        super().__init__(\n            name=name, \n            description=description, \n            prompt=prompt, \n            inputs_format=inputs_format, \n            outputs_format=outputs_format, \n            **kwargs\n        )\n\n    def execute(self, llm: Optional[BaseLLM] = None, inputs: Optional[dict] = None, sys_msg: Optional[str]=None, return_prompt: bool = False, **kwargs) -&gt; TestCodeReviewOutput:\n        action_input_attrs = self.inputs_format.get_attrs()\n        action_input_data = {attr: inputs.get(attr, \"undefined\") for attr in action_input_attrs}\n        prompt = self.prompt.format(**action_input_data)\n        output = llm.generate(\n            prompt=prompt, \n            system_message=sys_msg,\n            parser=self.outputs_format, \n            parse_mode=\"json\" # specify how to parse the output \n        ) \n        if return_prompt:\n            return output, prompt\n        return output\n</code></pre></p> <p>\u4ece\u4e0a\u9762\u7684\u793a\u4f8b\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\uff0c\u4e3a\u4e86\u5b9a\u4e49\u4e00\u4e2a\u52a8\u4f5c\uff0c\u6211\u4eec\u9700\u8981\uff1a</p> <ol> <li>\u4f7f\u7528 <code>ActionInput</code> \u548c <code>ActionOutput</code> \u7c7b\u5b9a\u4e49\u52a8\u4f5c\u7684\u8f93\u5165\u548c\u8f93\u51fa</li> <li>\u521b\u5efa\u4e00\u4e2a\u7ee7\u627f\u81ea <code>Action</code> \u7684\u52a8\u4f5c\u7c7b</li> <li>\u5b9e\u73b0 <code>execute</code> \u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f7f\u7528\u52a8\u4f5c\u8f93\u5165\u6570\u636e\u683c\u5f0f\u5316\u63d0\u793a\uff0c\u5e76\u4f7f\u7528 LLM \u751f\u6210\u8f93\u51fa\uff0c\u5e76\u901a\u8fc7 <code>parse_mode</code> \u6307\u5b9a\u5982\u4f55\u89e3\u6790\u8f93\u51fa\u3002</li> </ol>"},{"location":"zh/tutorial/first_agent/#\u5b9a\u4e49\u4ee3\u7406","title":"\u5b9a\u4e49\u4ee3\u7406","text":"<p>\u4e00\u65e6\u6211\u4eec\u5b9a\u4e49\u4e86\u52a8\u4f5c\uff0c\u6211\u4eec\u5c31\u53ef\u4ee5\u901a\u8fc7\u5c06\u52a8\u4f5c\u6dfb\u52a0\u5230\u4ee3\u7406\u4e2d\u6765\u521b\u5efa\u4ee3\u7406\uff1a</p> <pre><code># Initialize the LLM\nopenai_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=os.getenv(\"OPENAI_API_KEY\"))\n\n# Define the agent \ndeveloper = Agent(\n    name=\"Developer\", \n    description=\"A developer who can write code and review code\",\n    actions=[TestCodeGeneration(), TestCodeReview()], \n    llm_config=openai_config\n)\n</code></pre>"},{"location":"zh/tutorial/first_agent/#\u6267\u884c\u4e0d\u540c\u7684\u52a8\u4f5c","title":"\u6267\u884c\u4e0d\u540c\u7684\u52a8\u4f5c","text":"<p>\u4e00\u65e6\u4f60\u521b\u5efa\u4e86\u4e00\u4e2a\u5177\u6709\u591a\u4e2a\u52a8\u4f5c\u7684\u4ee3\u7406\uff0c\u4f60\u53ef\u4ee5\u6267\u884c\u7279\u5b9a\u7684\u52a8\u4f5c\uff1a</p> <pre><code># List all available actions on the agent\nactions = developer.get_all_actions()\nprint(f\"Available actions of agent {developer.name}:\")\nfor action in actions:\n    print(f\"- {action.name}: {action.description}\")\n\n# Generate some code using the CodeGeneration action\ngeneration_result = developer.execute(\n    action_name=\"TestCodeGeneration\", # specify the action name\n    action_input_data={ \n        \"requirement\": \"Write a function that returns the sum of two numbers\"\n    }\n)\n\n# Access the generated code\ngenerated_code = generation_result.content.code\nprint(\"Generated code:\")\nprint(generated_code)\n\n# Review the generated code using the CodeReview action\nreview_result = developer.execute(\n    action_name=\"TestCodeReview\",\n    action_input_data={\n        \"requirements\": \"Write a function that returns the sum of two numbers\",\n        \"code\": generated_code\n    }\n)\n\n# Access the review results\nreview = review_result.content.review\nprint(\"\\nReview:\")\nprint(review)\n</code></pre> <p>\u8fd9\u4e2a\u793a\u4f8b\u6f14\u793a\u4e86\u5982\u4f55\uff1a 1. \u5217\u51fa\u4ee3\u7406\u4e0a\u53ef\u7528\u7684\u6240\u6709\u52a8\u4f5c 2. \u4f7f\u7528 TestCodeGeneration \u52a8\u4f5c\u751f\u6210\u4ee3\u7801 3. \u4f7f\u7528 TestCodeReview \u52a8\u4f5c\u5ba1\u67e5\u751f\u6210\u7684\u4ee3\u7801 4. \u8bbf\u95ee\u6bcf\u4e2a\u52a8\u4f5c\u6267\u884c\u7684\u7ed3\u679c</p> <p>\u6709\u5173\u5b8c\u6574\u7684\u5de5\u4f5c\u793a\u4f8b\uff0c\u8bf7\u53c2\u8003 Agent \u793a\u4f8b\u3002 </p>"},{"location":"zh/tutorial/first_agent/#3-\u4fdd\u5b58\u548c\u52a0\u8f7d\u4ee3\u7406","title":"3. \u4fdd\u5b58\u548c\u52a0\u8f7d\u4ee3\u7406","text":"<p>\u4f60\u53ef\u4ee5\u5c06\u4ee3\u7406\u4fdd\u5b58\u5230\u6587\u4ef6\u5e76\u5728\u7a0d\u540e\u52a0\u8f7d\u5b83\uff1a</p> <pre><code># \u4fdd\u5b58\u4ee3\u7406\ndeveloper.save_module(\"examples/output/developer.json\")\n\n# \u52a0\u8f7d\u4ee3\u7406\nloaded_developer = Agent.load_module(\"examples/output/developer.json\", llm_config=openai_config)\n</code></pre>"},{"location":"zh/tutorial/first_workflow/","title":"\u6784\u5efa\u4f60\u7684\u7b2c\u4e00\u4e2a\u5de5\u4f5c\u6d41","text":"<p>\u5728 EvoAgentX \u4e2d\uff0c\u5de5\u4f5c\u6d41\u5141\u8bb8\u591a\u4e2a\u4ee3\u7406\u6309\u987a\u5e8f\u534f\u4f5c\u5b8c\u6210\u590d\u6742\u4efb\u52a1\u3002\u672c\u6559\u7a0b\u5c06\u6307\u5bfc\u4f60\u521b\u5efa\u548c\u4f7f\u7528\u5de5\u4f5c\u6d41\uff1a</p> <ol> <li>\u7406\u89e3\u987a\u5e8f\u5de5\u4f5c\u6d41\uff1a\u5b66\u4e60\u5de5\u4f5c\u6d41\u5982\u4f55\u5c06\u591a\u4e2a\u4efb\u52a1\u8fde\u63a5\u5728\u4e00\u8d77</li> <li>\u6784\u5efa\u987a\u5e8f\u5de5\u4f5c\u6d41\uff1a\u521b\u5efa\u4e00\u4e2a\u5305\u542b\u89c4\u5212\u548c\u7f16\u7801\u6b65\u9aa4\u7684\u5de5\u4f5c\u6d41</li> <li>\u6267\u884c\u548c\u7ba1\u7406\u5de5\u4f5c\u6d41\uff1a\u4f7f\u7528\u7279\u5b9a\u8f93\u5165\u8fd0\u884c\u5de5\u4f5c\u6d41</li> </ol> <p>\u901a\u8fc7\u672c\u6559\u7a0b\uff0c\u4f60\u5c06\u80fd\u591f\u521b\u5efa\u987a\u5e8f\u5de5\u4f5c\u6d41\uff0c\u534f\u8c03\u591a\u4e2a\u4ee3\u7406\u6765\u89e3\u51b3\u590d\u6742\u95ee\u9898\u3002</p>"},{"location":"zh/tutorial/first_workflow/#1-\u7406\u89e3\u987a\u5e8f\u5de5\u4f5c\u6d41","title":"1. \u7406\u89e3\u987a\u5e8f\u5de5\u4f5c\u6d41","text":"<p>EvoAgentX \u4e2d\u7684\u5de5\u4f5c\u6d41\u4ee3\u8868\u4e00\u7cfb\u5217\u53ef\u4ee5\u7531\u4e0d\u540c\u4ee3\u7406\u6267\u884c\u7684\u4efb\u52a1\u3002\u6700\u7b80\u5355\u7684\u5de5\u4f5c\u6d41\u662f\u987a\u5e8f\u5de5\u4f5c\u6d41\uff0c\u5176\u4e2d\u4efb\u52a1\u4e00\u4e2a\u63a5\u4e00\u4e2a\u5730\u6267\u884c\uff0c\u524d\u4e00\u4e2a\u4efb\u52a1\u7684\u8f93\u51fa\u4f5c\u4e3a\u540e\u7eed\u4efb\u52a1\u7684\u8f93\u5165\u3002</p> <p>\u8ba9\u6211\u4eec\u4ece\u5bfc\u5165\u5fc5\u8981\u7684\u7ec4\u4ef6\u5f00\u59cb\uff1a</p> <pre><code>import os \nfrom dotenv import load_dotenv\nfrom evoagentx.workflow import SequentialWorkFlowGraph, WorkFlow\nfrom evoagentx.agents import AgentManager\nfrom evoagentx.models import OpenAILLMConfig, OpenAILLM\n\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n</code></pre>"},{"location":"zh/tutorial/first_workflow/#2-\u6784\u5efa\u987a\u5e8f\u5de5\u4f5c\u6d41","title":"2. \u6784\u5efa\u987a\u5e8f\u5de5\u4f5c\u6d41","text":"<p>\u987a\u5e8f\u5de5\u4f5c\u6d41\u7531\u4e00\u7cfb\u5217\u4efb\u52a1\u7ec4\u6210\uff0c\u6bcf\u4e2a\u4efb\u52a1\u90fd\u6709\uff1a</p> <ul> <li>\u540d\u79f0\u548c\u63cf\u8ff0</li> <li>\u8f93\u5165\u548c\u8f93\u51fa\u5b9a\u4e49</li> <li>\u63d0\u793a\u6a21\u677f</li> <li>\u89e3\u6790\u6a21\u5f0f\u548c\u51fd\u6570\uff08\u53ef\u9009\uff09</li> </ul> <p>\u4ee5\u4e0b\u662f\u5982\u4f55\u6784\u5efa\u4e00\u4e2a\u5305\u542b\u89c4\u5212\u548c\u7f16\u7801\u4efb\u52a1\u7684\u987a\u5e8f\u5de5\u4f5c\u6d41\uff1a</p> <pre><code># Configure the LLM \nllm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=OPENAI_API_KEY, stream=True, output_response=True)\nllm = OpenAILLM(llm_config)\n\n# Define a custom parsing function (if needed)\nfrom evoagentx.core.registry import register_parse_function\nfrom evoagentx.core.module_utils import extract_code_blocks\n\n# [optional] Define a custom parsing function (if needed)\n# It is suggested to use the `@register_parse_function` decorator to register a custom parsing function, so the workflow can be saved and loaded correctly.  \n\n@register_parse_function\ndef custom_parse_func(content: str) -&gt; str:\n    return {\"code\": extract_code_blocks(content)[0]}\n\n# Define sequential tasks\ntasks = [\n    {\n        \"name\": \"Planning\",\n        \"description\": \"Create a detailed plan for code generation\",\n        \"inputs\": [\n            {\"name\": \"problem\", \"type\": \"str\", \"required\": True, \"description\": \"Description of the problem to be solved\"},\n        ],\n        \"outputs\": [\n            {\"name\": \"plan\", \"type\": \"str\", \"required\": True, \"description\": \"Detailed plan with steps, components, and architecture\"}\n        ],\n        \"prompt\": \"You are a software architect. Your task is to create a detailed implementation plan for the given problem.\\n\\nProblem: {problem}\\n\\nPlease provide a comprehensive implementation plan including:\\n1. Problem breakdown\\n2. Algorithm or approach selection\\n3. Implementation steps\\n4. Potential edge cases and solutions\",\n        \"parse_mode\": \"str\",\n        # \"llm_config\": specific_llm_config # if you want to use a specific LLM for a task, you can add a key `llm_config` in the task dict.  \n    },\n    {\n        \"name\": \"Coding\",\n        \"description\": \"Implement the code based on the implementation plan\",\n        \"inputs\": [\n            {\"name\": \"problem\", \"type\": \"str\", \"required\": True, \"description\": \"Description of the problem to be solved\"},\n            {\"name\": \"plan\", \"type\": \"str\", \"required\": True, \"description\": \"Detailed implementation plan from the Planning phase\"},\n        ],\n        \"outputs\": [\n            {\"name\": \"code\", \"type\": \"str\", \"required\": True, \"description\": \"Implemented code with explanations\"}\n        ],\n        \"prompt\": \"You are a software developer. Your task is to implement the code based on the provided problem and implementation plan.\\n\\nProblem: {problem}\\nImplementation Plan: {plan}\\n\\nPlease provide the implementation code with appropriate comments.\",\n        \"parse_mode\": \"custom\",\n        \"parse_func\": custom_parse_func\n    }\n]\n\n# Create the sequential workflow graph\ngraph = SequentialWorkFlowGraph(\n    goal=\"Generate code to solve programming problems\",\n    tasks=tasks\n)\n</code></pre> <p>Note</p> <p>\u5f53\u4f60\u4f7f\u7528\u4efb\u52a1\u5217\u8868\u521b\u5efa <code>SequentialWorkFlowGraph</code> \u65f6\uff0c\u6846\u67b6\u4f1a\u4e3a\u6bcf\u4e2a\u4efb\u52a1\u521b\u5efa\u4e00\u4e2a <code>CustomizeAgent</code>\u3002\u5de5\u4f5c\u6d41\u4e2d\u7684\u6bcf\u4e2a\u4efb\u52a1\u90fd\u6210\u4e3a\u4e00\u4e2a\u4e13\u95e8\u7684\u4ee3\u7406\uff0c\u914d\u7f6e\u6709\u4f60\u5b9a\u4e49\u7684\u7279\u5b9a\u63d0\u793a\u3001\u8f93\u5165/\u8f93\u51fa\u683c\u5f0f\u548c\u89e3\u6790\u6a21\u5f0f\u3002\u8fd9\u4e9b\u4ee3\u7406\u6309\u987a\u5e8f\u8fde\u63a5\uff0c\u4e00\u4e2a\u4ee3\u7406\u7684\u8f93\u51fa\u6210\u4e3a\u4e0b\u4e00\u4e2a\u4ee3\u7406\u7684\u8f93\u5165\u3002</p> <p><code>parse_mode</code> \u63a7\u5236\u5982\u4f55\u5c06 LLM \u7684\u8f93\u51fa\u89e3\u6790\u4e3a\u7ed3\u6784\u5316\u683c\u5f0f\u3002\u53ef\u7528\u9009\u9879\u6709\uff1a[<code>'str'</code>\uff08\u9ed8\u8ba4\uff09\u3001<code>'json'</code>\u3001<code>'title'</code>\u3001<code>'xml'</code>\u3001<code>'custom'</code>]\u3002\u6709\u5173\u89e3\u6790\u6a21\u5f0f\u548c\u793a\u4f8b\u7684\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 CustomizeAgent \u6587\u6863\u3002</p>"},{"location":"zh/tutorial/first_workflow/#3-\u6267\u884c\u548c\u7ba1\u7406\u5de5\u4f5c\u6d41","title":"3. \u6267\u884c\u548c\u7ba1\u7406\u5de5\u4f5c\u6d41","text":"<p>\u4e00\u65e6\u4f60\u521b\u5efa\u4e86\u5de5\u4f5c\u6d41\u56fe\uff0c\u4f60\u5c31\u53ef\u4ee5\u521b\u5efa\u5de5\u4f5c\u6d41\u5b9e\u4f8b\u5e76\u6267\u884c\u5b83\uff1a</p> <pre><code># Create agent manager and add agents from the workflow. It will create a `CustomizeAgent` for each task in the workflow. \nagent_manager = AgentManager()\nagent_manager.add_agents_from_workflow(\n    graph, \n    llm_config=llm_config  # This config will be used for all tasks without `llm_config`. \n)\n\n# Create workflow instance\nworkflow = WorkFlow(graph=graph, agent_manager=agent_manager, llm=llm)\n\n# Execute the workflow with inputs\noutput = workflow.execute(\n    inputs = {\n        \"problem\": \"Write a function to find the longest palindromic substring in a given string.\"\n    }\n)\n\nprint(\"Workflow completed!\")\nprint(\"Workflow output:\\n\", output)\n</code></pre> <p>\u4f60\u5e94\u8be5\u5728 <code>execute</code> \u65b9\u6cd5\u7684 <code>inputs</code> \u53c2\u6570\u4e2d\u6307\u5b9a\u5de5\u4f5c\u6d41\u6240\u9700\u7684\u6240\u6709\u8f93\u5165\u3002</p> <p>\u6709\u5173\u5b8c\u6574\u7684\u5de5\u4f5c\u793a\u4f8b\uff0c\u8bf7\u53c2\u8003 \u987a\u5e8f\u5de5\u4f5c\u6d41\u793a\u4f8b\u3002</p>"},{"location":"zh/tutorial/first_workflow/#4-\u4fdd\u5b58\u548c\u52a0\u8f7d\u5de5\u4f5c\u6d41","title":"4. \u4fdd\u5b58\u548c\u52a0\u8f7d\u5de5\u4f5c\u6d41","text":"<p>\u4f60\u53ef\u4ee5\u4fdd\u5b58\u5de5\u4f5c\u6d41\u56fe\u4ee5\u4f9b\u5c06\u6765\u4f7f\u7528\uff1a</p> <pre><code># Save the workflow graph to a file\ngraph.save_module(\"examples/output/saved_sequential_workflow.json\")\n\n# Load the workflow graph from a file\nloaded_graph = SequentialWorkFlowGraph.from_file(\"examples/output/saved_sequential_workflow.json\")\n\n# Create a new workflow with the loaded graph\nnew_workflow = WorkFlow(graph=loaded_graph, agent_manager=agent_manager, llm=llm)\n</code></pre> <p>\u6709\u5173\u66f4\u590d\u6742\u7684\u5de5\u4f5c\u6d41\u6216\u4e0d\u540c\u7c7b\u578b\u7684\u5de5\u4f5c\u6d41\u56fe\uff0c\u8bf7\u53c2\u9605 \u5de5\u4f5c\u6d41\u56fe \u6587\u6863\u548c \u52a8\u4f5c\u56fe \u6587\u6863\u3002</p>"},{"location":"zh/tutorial/sew_optimizer/","title":"SEW\u4f18\u5316\u5668\u6559\u7a0b","text":"<p>\u672c\u6559\u7a0b\u5c06\u6307\u5bfc\u60a8\u8bbe\u7f6e\u548c\u8fd0\u884cEvoAgentX\u4e2d\u7684SEW\uff08Self-Evolving Workflow\uff0c\u81ea\u8fdb\u5316\u5de5\u4f5c\u6d41\uff09\u4f18\u5316\u5668\u3002\u6211\u4eec\u5c06\u4f7f\u7528HumanEval\u57fa\u51c6\u4f5c\u4e3a\u793a\u4f8b\uff0c\u6f14\u793a\u5982\u4f55\u4f18\u5316\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u3002</p>"},{"location":"zh/tutorial/sew_optimizer/#1-\u6982\u8ff0","title":"1. \u6982\u8ff0","text":"<p>SEW\u4f18\u5316\u5668\u662fEvoAgentX\u4e2d\u7684\u5f3a\u5927\u5de5\u5177\uff0c\u5b83\u4f7f\u60a8\u80fd\u591f\uff1a</p> <ul> <li>\u81ea\u52a8\u4f18\u5316\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\uff08\u63d0\u793a\u8bcd\u548c\u5de5\u4f5c\u6d41\u7ed3\u6784\uff09</li> <li>\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4f18\u5316\u7ed3\u679c</li> <li>\u652f\u6301\u4e0d\u540c\u7684\u5de5\u4f5c\u6d41\u8868\u793a\u65b9\u6848\uff08Python\u3001Yaml\u3001BPMN\u7b49\uff09</li> </ul>"},{"location":"zh/tutorial/sew_optimizer/#2-\u8bbe\u7f6e\u73af\u5883","title":"2. \u8bbe\u7f6e\u73af\u5883","text":"<p>\u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5bfc\u5165\u8bbe\u7f6eSEW\u4f18\u5316\u5668\u6240\u9700\u7684\u5fc5\u8981\u6a21\u5757\uff1a</p> <pre><code>from evoagentx.config import Config\nfrom evoagentx.models import OpenAILLMConfig, OpenAILLM\nfrom evoagentx.workflow import SEWWorkFlowGraph \nfrom evoagentx.agents import AgentManager\nfrom evoagentx.benchmark import HumanEval \nfrom evoagentx.evaluators import Evaluator \nfrom evoagentx.optimizers import SEWOptimizer \nfrom evoagentx.core.callbacks import suppress_logger_info\n</code></pre>"},{"location":"zh/tutorial/sew_optimizer/#\u914d\u7f6ellm\u6a21\u578b","title":"\u914d\u7f6eLLM\u6a21\u578b","text":"<p>\u4e0eEvoAgentX\u4e2d\u7684\u5176\u4ed6\u7ec4\u4ef6\u7c7b\u4f3c\uff0c\u60a8\u9700\u8981\u4e00\u4e2a\u6709\u6548\u7684OpenAI API\u5bc6\u94a5\u6765\u521d\u59cb\u5316LLM\u3002</p> <pre><code>llm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=OPENAI_API_KEY)\nllm = OpenAILLM(config=llm_config)\n</code></pre>"},{"location":"zh/tutorial/sew_optimizer/#3-\u8bbe\u7f6e\u7ec4\u4ef6","title":"3. \u8bbe\u7f6e\u7ec4\u4ef6","text":""},{"location":"zh/tutorial/sew_optimizer/#\u6b65\u9aa41\u521d\u59cb\u5316sew\u5de5\u4f5c\u6d41","title":"\u6b65\u9aa41\uff1a\u521d\u59cb\u5316SEW\u5de5\u4f5c\u6d41","text":"<p>SEW\u5de5\u4f5c\u6d41\u662f\u5c06\u88ab\u4f18\u5316\u7684\u6838\u5fc3\u7ec4\u4ef6\u3002\u5b83\u4ee3\u8868\u4e00\u4e2a\u987a\u5e8f\u5de5\u4f5c\u6d41\uff0c\u65e8\u5728\u89e3\u51b3\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u3002</p> <pre><code>sew_graph = SEWWorkFlowGraph(llm_config=llm_config)\nagent_manager = AgentManager()\nagent_manager.add_agents_from_workflow(sew_graph)\n</code></pre>"},{"location":"zh/tutorial/sew_optimizer/#\u6b65\u9aa42\u51c6\u5907\u57fa\u51c6\u6d4b\u8bd5","title":"\u6b65\u9aa42\uff1a\u51c6\u5907\u57fa\u51c6\u6d4b\u8bd5","text":"<p>\u5bf9\u4e8e\u672c\u6559\u7a0b\uff0c\u6211\u4eec\u5c06\u4f7f\u7528HumanEval\u57fa\u51c6\u7684\u4fee\u6539\u7248\u672c\uff0c\u8be5\u7248\u672c\u5c06\u6d4b\u8bd5\u6570\u636e\u5206\u4e3a\u5f00\u53d1\u96c6\u548c\u6d4b\u8bd5\u96c6\uff1a</p> <pre><code>class HumanEvalSplits(HumanEval):\n    def _load_data(self):\n        # \u52a0\u8f7d\u539f\u59cb\u6d4b\u8bd5\u6570\u636e\n        super()._load_data()\n        # \u5c06\u6570\u636e\u5206\u4e3a\u5f00\u53d1\u96c6\u548c\u6d4b\u8bd5\u96c6\n        import numpy as np \n        np.random.seed(42)\n        num_dev_samples = int(len(self._test_data) * 0.2)\n        random_indices = np.random.permutation(len(self._test_data))\n        self._dev_data = [self._test_data[i] for i in random_indices[:num_dev_samples]]\n        self._test_data = [self._test_data[i] for i in random_indices[num_dev_samples:]]\n\n# \u521d\u59cb\u5316\u57fa\u51c6\nhumaneval = HumanEvalSplits()\n</code></pre> <p>SEWOptimizer\u9ed8\u8ba4\u4f1a\u5728\u5f00\u53d1\u96c6\u4e0a\u8bc4\u4f30\u6027\u80fd\u3002\u8bf7\u786e\u4fdd\u57fa\u51c6\u6d4b\u8bd5\u6b63\u786e\u8bbe\u7f6e\u4e86\u5f00\u53d1\u96c6\u3002\u60a8\u53ef\u4ee5\uff1a    - \u4f7f\u7528\u5df2\u7ecf\u63d0\u4f9b\u5f00\u53d1\u96c6\u7684\u57fa\u51c6\uff08\u5982HotPotQA\uff09    - \u5c06\u6570\u636e\u96c6\u5206\u4e3a\u5f00\u53d1\u96c6\u548c\u6d4b\u8bd5\u96c6\uff08\u5982\u4e0a\u9762HumanEvalSplits\u793a\u4f8b\u6240\u793a\uff09    - \u5b9e\u73b0\u5e26\u6709\u5f00\u53d1\u96c6\u652f\u6301\u7684\u81ea\u5b9a\u4e49\u57fa\u51c6</p>"},{"location":"zh/tutorial/sew_optimizer/#\u6b65\u9aa43\u8bbe\u7f6e\u8bc4\u4f30\u5668","title":"\u6b65\u9aa43\uff1a\u8bbe\u7f6e\u8bc4\u4f30\u5668","text":"<p>\u8bc4\u4f30\u5668\u8d1f\u8d23\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u8bc4\u4f30\u5de5\u4f5c\u6d41\u7684\u6027\u80fd\u3002\u6709\u5173\u5982\u4f55\u8bbe\u7f6e\u548c\u4f7f\u7528\u8bc4\u4f30\u5668\u7684\u66f4\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u57fa\u51c6\u548c\u8bc4\u4f30\u6559\u7a0b\u3002</p> <pre><code>def collate_func(example: dict) -&gt; dict:\n    # \u5c06\u539f\u59cb\u793a\u4f8b\u8f6c\u6362\u4e3aSEW\u5de5\u4f5c\u6d41\u7684\u9884\u671f\u8f93\u5165\n    return {\"question\": example[\"prompt\"]}\n\nevaluator = Evaluator(\n    llm=llm, \n    agent_manager=agent_manager, \n    collate_func=collate_func, \n    num_workers=5, \n    verbose=True\n)\n</code></pre>"},{"location":"zh/tutorial/sew_optimizer/#4-\u914d\u7f6e\u548c\u8fd0\u884csew\u4f18\u5316\u5668","title":"4. \u914d\u7f6e\u548c\u8fd0\u884cSEW\u4f18\u5316\u5668","text":"<p>SEW\u4f18\u5316\u5668\u53ef\u4ee5\u901a\u8fc7\u5404\u79cd\u53c2\u6570\u914d\u7f6e\uff0c\u4ee5\u63a7\u5236\u4f18\u5316\u8fc7\u7a0b\uff1a</p> <pre><code>optimizer = SEWOptimizer(\n    graph=sew_graph,           # \u8981\u4f18\u5316\u7684\u5de5\u4f5c\u6d41\u56fe\n    evaluator=evaluator,       # \u7528\u4e8e\u6027\u80fd\u8bc4\u4f30\u7684\u8bc4\u4f30\u5668\n    llm=llm,                   # \u8bed\u8a00\u6a21\u578b\n    max_steps=10,              # \u6700\u5927\u4f18\u5316\u6b65\u9aa4\u6570\n    eval_rounds=1,             # \u6bcf\u6b65\u8bc4\u4f30\u8f6e\u6570\n    repr_scheme=\"python\",      # \u5de5\u4f5c\u6d41\u7684\u8868\u793a\u65b9\u6848\n    optimize_mode=\"prompt\",    # \u8981\u4f18\u5316\u7684\u65b9\u9762\uff08\u63d0\u793a/\u7ed3\u6784/\u5168\u90e8\uff09\n    order=\"zero-order\"         # \u4f18\u5316\u7b97\u6cd5\u987a\u5e8f\uff08\u96f6\u9636/\u4e00\u9636\uff09\n)\n</code></pre>"},{"location":"zh/tutorial/sew_optimizer/#\u8fd0\u884c\u4f18\u5316","title":"\u8fd0\u884c\u4f18\u5316","text":"<p>\u8981\u542f\u52a8\u4f18\u5316\u8fc7\u7a0b\uff1a</p> <pre><code># \u4f18\u5316SEW\u5de5\u4f5c\u6d41\noptimizer.optimize(dataset=humaneval)\n\n# \u8bc4\u4f30\u4f18\u5316\u540e\u7684\u5de5\u4f5c\u6d41\nwith suppress_logger_info():\n    metrics = optimizer.evaluate(dataset=humaneval, eval_mode=\"test\")\nprint(\"Evaluation metrics: \", metrics)\n\n# \u4fdd\u5b58\u4f18\u5316\u540e\u7684SEW\u5de5\u4f5c\u6d41\noptimizer.save(\"debug/optimized_sew_workflow.json\")\n</code></pre> <p>\u6709\u5173\u5b8c\u6574\u7684\u5de5\u4f5c\u793a\u4f8b\uff0c\u8bf7\u53c2\u9605sew_optimizer.py\u3002</p>"}]}